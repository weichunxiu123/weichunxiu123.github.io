<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    
    <title>Spark-pyspark基础编码环境 | Hexo</title>
    
    
        <meta name="keywords" content="搭建,spark">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Spark（pyspark基础编码环境）（一）、pyspark环境配置安装。 PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。 1234前情提示：（1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。（2）将文件夹内bin内的Hadoop.dll">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-pyspark基础编码环境">
<meta property="og:url" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Spark（pyspark基础编码环境）（一）、pyspark环境配置安装。 PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。 1234前情提示：（1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。（2）将文件夹内bin内的Hadoop.dll">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps46.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps47.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps48.png">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps49.png">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps50.png">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps51.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps52.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps53.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps54.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps55.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps56.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps57.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps58.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps59.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps60.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps61.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps62.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps63.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps64.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps65.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps66.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps67.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps68.jpg">
<meta property="og:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps69.jpg">
<meta property="article:published_time" content="2023-06-08T03:05:04.757Z">
<meta property="article:modified_time" content="2023-06-09T00:29:10.456Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="搭建">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps46.jpg">
    

    
        <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Hexo</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/categories">分类</a>
                
                    <a class="main-nav-link" href="/tags">标签</a>
                
                    <a class="main-nav-link" href="/about">关于</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/tags">标签</a></td>
                
                    <td><a class="main-nav-link" href="/about">关于</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id="categories">
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id="allExpand" href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree"> 
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            工具
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/wiki/hello-world/">第一个hexo</a></li>  <li class="file"><a href="/wiki/hive%E5%AE%89%E8%A3%85/">Hive基础配置</a></li>  <li class="file"><a href="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">Git安装与配置</a></li>  <li class="file"><a href="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">Docker安装与基本操作</a></li>  <li class="file"><a href="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/">Spark-local模式配置</a></li>  <li class="file"><a href="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/">Spark-standalone环境部署</a></li>  <li class="file"><a href="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/">Spark-HA环境部署</a></li>  <li class="file"><a href="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/">Spark-YARN模式部署</a></li>  <li class="file active"><a href="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/">Spark-pyspark基础编码环境</a></li>  </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-Spark-pyspark基础编码环境" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/spark/" rel="tag">spark</a>, <a class="tag-link-link" href="/tags/%E6%90%AD%E5%BB%BA/" rel="tag">搭建</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/">
            <time datetime="2023-06-08T03:05:04.757Z" itemprop="datePublished">2023-06-08</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/weichunxiu123/git@github.com:weichunxiu123/weichunxiu123.github.io.git/raw/master/source/_posts/Spark-pyspark基础编码环境.md"> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/weichunxiu123/git@github.com:weichunxiu123/weichunxiu123.github.io.git/edit/master/source/_posts/Spark-pyspark基础编码环境.md"> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/weichunxiu123/git@github.com:weichunxiu123/weichunxiu123.github.io.git/commits/master/source/_posts/Spark-pyspark基础编码环境.md"> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Spark-pyspark基础编码环境
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <h2 id="Spark（pyspark基础编码环境）"><a href="#Spark（pyspark基础编码环境）" class="headerlink" title="Spark（pyspark基础编码环境）"></a><strong>Spark（pyspark基础编码环境）</strong></h2><p>（一）、pyspark环境配置安装。</p>
<p>PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">前情提示：</span><br><span class="line">（1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。</span><br><span class="line">（2）将文件夹内bin内的Hadoop.dll复制到C:\Windows\Systmctl32里面去。</span><br><span class="line">（3）在系统环境变量中配置HADOOP_HOME，指向hadoop-3.3.0文件夹的路径。</span><br></pre></td></tr></table></figure>

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps46.jpg" alt="img" style="zoom:110%;">

 

<p>（二）本机PySpark环境配置</p>
<p>在前面部署Spark的时候，已经在Linux系统上部署了acaconda的Python环境，详见Spark的Stand Alone模式部署章节。故本次在Windows上安装anaconda，并配置PySpark库。具体安装步骤如下：</p>
<p>（1）在课程资料中选择anaconda应用程序双击安装。</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps47.jpg" alt="img" style="zoom:110%;">

<p>（2）一直选择Next，进行安装。</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps48.png" alt="img" style="zoom:100%;">

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps49.png" alt="img" style="zoom:100%;"> 

<p>注意：选择第一个，将anaconda添加至我的环境变量中！</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps50.png" alt="img" style="zoom:100%;"> 

 

<p>（3）安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。</p>
<p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps51.jpg" alt="img"> </p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps52.jpg" alt="img" style="zoom:100%;"> 

 

<p>（4）配置国内源，加速网络下载。</p>
<p>1、在Anaconda Prompt(anaconda)中执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>

<p>2、将如下内容替换到C:\Users\用户名.condarc文件中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">channels: \- defaultsshow_channel_urls: truedefault_channels:</span><br><span class="line"> \- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line"> \- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line"> \- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line"> conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>



<p>（5）创建虚拟环境</p>
<p>1、创建虚拟环境 pyspark, 基于Python 3.8</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure>

<p>2、切换到虚拟环境内</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br></pre></td></tr></table></figure>

<p>3、在虚拟环境内安装包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i [https://pypi.tuna.tsinghua.edu.cn/](https://pypi.tuna.tsinghua.edu.cn/simple)simple</span><br></pre></td></tr></table></figure>

<p>安装成功示例：</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps53.jpg" alt="img" style="zoom:110%;"> 

 

<p>（三）PyCharm中配置Python解释器</p>
<p>（1）配置本地解释器：创建Python项目，选择conda虚拟环境PySpark中的Python.exe解释器。</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps54.jpg" alt="img" style="zoom:110%;"> 

 

<p>（2）配置远程SSH Linux解释器</p>
<p>1、远程SSH python pyspark环境</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps55.jpg" alt="img" style="zoom:110%;"> 

 

<p>2、添加新的远程连接</p>
<p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps56.jpg" alt="img" style="zoom:100%;"><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps57.jpg" alt="img" style="zoom:100%;"> </p>
<p>3、设置虚拟的python环境路径</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps58.jpg" alt="img" style="zoom:110%;"> 

 

<p>（四）WordCount应用实战</p>
<p>可以选择在本地的PySpark环境中执行spark代码，也可以选择在虚拟机环境PySpark中执行。选择本地的就是使用conda环境，应用其中的PySpark环境执行，来读取本地文件，完成单词计数的实例。选择远程虚拟机中的PySpark环境，需要SSH连接到服务器（这里需要安装Pycharm专业版），注意：无论是选择那种方案，都是在PyCharm软件中去执行，完成上述过程。</p>
<p>（1）WordCount代码本地执行</p>
<p>准备pyspark代码以及本地文件words.txt，在PyCharm中执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf8</span><br><span class="line">from pyspark import SparkConf, SparkContext\# import os</span><br><span class="line">import os</span><br><span class="line">os.environ[&#x27;PYSPARK_PYTHON&#x27;]=&#x27;D:\\anaconda3\\envs\\pyspark\\python.exe&#x27;</span><br><span class="line">os.environ [&#x27;JAVA_HOME&#x27;] = &#x27;D:\\Java\\jdk1.8.0_241&#x27;</span><br><span class="line">\#os.environ[&#x27;PYSPARK_PYTHON&#x27;]=&#x27;/export/server/anaconda3/envs/pyspark/bin/python3.8&#x27;</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">  conf = SparkConf().setAppName(&quot;WordCountHelloWorld&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">  # 通过SparkConf对象构建SparkContext对象</span><br><span class="line">  sc = SparkContext(conf=conf)</span><br><span class="line">  # 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量</span><br><span class="line">  # 读取文件</span><br><span class="line">  #file_rdd = sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)</span><br><span class="line">  #file_rdd = sc.textFile(&quot;file:///tmp/pycharm_project_621/data/words.txt&quot;)</span><br><span class="line">  file_rdd = sc.textFile(&quot;D:\\数据挖掘与分析实验报告合集\\pyspark\\data\\input\\words.txt&quot;)</span><br><span class="line">  # 将单词进行切割, 得到一个存储全部单词的集合对象</span><br><span class="line">  words_rdd = file_rdd.flatMap(lambda line: line.split(&quot; &quot;))</span><br><span class="line">  # 将单词转换为元组对象, key是单词, value是数字1</span><br><span class="line">  words_with_one_rdd = words_rdd.map(lambda x: (x, 1))</span><br><span class="line">  # 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加)</span><br><span class="line">  result_rdd = words_with_one_rdd.reduceByKey(lambda a, b: a + b)</span><br><span class="line">  # 通过collect方法收集RDD的数据打印输出结果</span><br><span class="line">print(result_rdd.collect())</span><br></pre></td></tr></table></figure>

<p>运行结果截图：</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps59.jpg" alt="img" style="zoom:110%;"> 

<p>（2）WordCount代码远程服务器上执行。</p>
<p>通过SSH连接到远程服务器上，详见上述操作。</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps60.jpg" alt="img" style="zoom:110%;"> 

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps61.jpg" alt="img" style="zoom:110%;"> 

 

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">完成与服务器连接后，会在服务器中的/tmp文件夹下新建了pycharm_project_xxx文件夹用于放置本地的同步代码。</span><br></pre></td></tr></table></figure>

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps62.jpg" alt="img" style="zoom:110%;"> 

 

<p>（3）读取HDFS上的文件</p>
<p>1、将读取文件路径改为hdfs上的&#x2F;input&#x2F;words.txt</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps63.jpg" alt="img" style="zoom:110%;">  

<p>2、在hdfs上新建&#x2F;input文件夹，使用命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input</span><br></pre></td></tr></table></figure>

<p>3、上传words.txt到hdfs中，使用命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put words.txt /input/</span><br></pre></td></tr></table></figure>

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps64.jpg" alt="img" style="zoom:100%;"> 

<p>4、在pycharm中执行spark代码</p>
<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps65.jpg" alt="img" style="zoom:110%;"> 

 

<p>（五）spark-submit作业提交</p>
<p>（1）local本地模式</p>
<p>首先将helloword.py程序放到&#x2F;root&#x2F;目录下，使用命令完成提交作业。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master local[*] /root/helloworld.py</span><br></pre></td></tr></table></figure>

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps66.jpg" alt="img" style="zoom:110%;"> 

 

<p>（2）spark on yarn模式</p>
<p>使用命令完成提交作业。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn /root/helloworld.py</span><br></pre></td></tr></table></figure>

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps67.jpg" alt="img" style="zoom:110%;"> 

 

<p>（3）使用历史服务器查看任务执行情况 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node1:18080</span><br></pre></td></tr></table></figure>

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps68.jpg" alt="img" style="zoom:110%;"> 

<img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps69.jpg" alt="img" style="zoom:110%;"> 





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<ul>
<li>[1]spark环境部署.md</li>
<li>[2]1-saprk基础入门.pdf</li>
<li>[3]spark1.pptx</li>
</ul>
</blockquote>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
    
        <a href="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">Spark-YARN模式部署</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            John Doe &copy; 2023 
            <a rel="external nofollow noopener noreferrer" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png"></a>
            <br> Powered by <a href="http://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a>. Theme - <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a>
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>