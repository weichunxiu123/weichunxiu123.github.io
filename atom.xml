<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="https://weichunxiu123.github.io/atom.xml" rel="self"/>
  
  <link href="https://weichunxiu123.github.io/"/>
  <updated>2023-06-09T00:29:10.456Z</updated>
  <id>https://weichunxiu123.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark-pyspark基础编码环境</title>
    <link href="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/"/>
    <id>https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/</id>
    <published>2023-06-08T03:05:04.757Z</published>
    <updated>2023-06-09T00:29:10.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark（pyspark基础编码环境）"><a href="#Spark（pyspark基础编码环境）" class="headerlink" title="Spark（pyspark基础编码环境）"></a><strong>Spark（pyspark基础编码环境）</strong></h2><p>（一）、pyspark环境配置安装。</p><p>PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">前情提示：</span><br><span class="line">（1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。</span><br><span class="line">（2）将文件夹内bin内的Hadoop.dll复制到C:\Windows\Systmctl32里面去。</span><br><span class="line">（3）在系统环境变量中配置HADOOP_HOME，指向hadoop-3.3.0文件夹的路径。</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps46.jpg" alt="img" style="zoom:110%;"> <p>（二）本机PySpark环境配置</p><p>在前面部署Spark的时候，已经在Linux系统上部署了acaconda的Python环境，详见Spark的Stand Alone模式部署章节。故本次在Windows上安装anaconda，并配置PySpark库。具体安装步骤如下：</p><p>（1）在课程资料中选择anaconda应用程序双击安装。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps47.jpg" alt="img" style="zoom:110%;"><p>（2）一直选择Next，进行安装。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps48.png" alt="img" style="zoom:100%;"><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps49.png" alt="img" style="zoom:100%;"> <p>注意：选择第一个，将anaconda添加至我的环境变量中！</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps50.png" alt="img" style="zoom:100%;">  <p>（3）安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。</p><p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps51.jpg" alt="img"> </p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps52.jpg" alt="img" style="zoom:100%;">  <p>（4）配置国内源，加速网络下载。</p><p>1、在Anaconda Prompt(anaconda)中执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><p>2、将如下内容替换到C:\Users\用户名.condarc文件中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">channels: \- defaultsshow_channel_urls: truedefault_channels:</span><br><span class="line"> \- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line"> \- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line"> \- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line"> conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line"> simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure><p>（5）创建虚拟环境</p><p>1、创建虚拟环境 pyspark, 基于Python 3.8</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure><p>2、切换到虚拟环境内</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br></pre></td></tr></table></figure><p>3、在虚拟环境内安装包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i [https://pypi.tuna.tsinghua.edu.cn/](https://pypi.tuna.tsinghua.edu.cn/simple)simple</span><br></pre></td></tr></table></figure><p>安装成功示例：</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps53.jpg" alt="img" style="zoom:110%;">  <p>（三）PyCharm中配置Python解释器</p><p>（1）配置本地解释器：创建Python项目，选择conda虚拟环境PySpark中的Python.exe解释器。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps54.jpg" alt="img" style="zoom:110%;">  <p>（2）配置远程SSH Linux解释器</p><p>1、远程SSH python pyspark环境</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps55.jpg" alt="img" style="zoom:110%;">  <p>2、添加新的远程连接</p><p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps56.jpg" alt="img" style="zoom:100%;"><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps57.jpg" alt="img" style="zoom:100%;"> </p><p>3、设置虚拟的python环境路径</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps58.jpg" alt="img" style="zoom:110%;">  <p>（四）WordCount应用实战</p><p>可以选择在本地的PySpark环境中执行spark代码，也可以选择在虚拟机环境PySpark中执行。选择本地的就是使用conda环境，应用其中的PySpark环境执行，来读取本地文件，完成单词计数的实例。选择远程虚拟机中的PySpark环境，需要SSH连接到服务器（这里需要安装Pycharm专业版），注意：无论是选择那种方案，都是在PyCharm软件中去执行，完成上述过程。</p><p>（1）WordCount代码本地执行</p><p>准备pyspark代码以及本地文件words.txt，在PyCharm中执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># coding:utf8</span><br><span class="line">from pyspark import SparkConf, SparkContext\# import os</span><br><span class="line">import os</span><br><span class="line">os.environ[&#x27;PYSPARK_PYTHON&#x27;]=&#x27;D:\\anaconda3\\envs\\pyspark\\python.exe&#x27;</span><br><span class="line">os.environ [&#x27;JAVA_HOME&#x27;] = &#x27;D:\\Java\\jdk1.8.0_241&#x27;</span><br><span class="line">\#os.environ[&#x27;PYSPARK_PYTHON&#x27;]=&#x27;/export/server/anaconda3/envs/pyspark/bin/python3.8&#x27;</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">  conf = SparkConf().setAppName(&quot;WordCountHelloWorld&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">  # 通过SparkConf对象构建SparkContext对象</span><br><span class="line">  sc = SparkContext(conf=conf)</span><br><span class="line">  # 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量</span><br><span class="line">  # 读取文件</span><br><span class="line">  #file_rdd = sc.textFile(&quot;hdfs://node1:8020/input/words.txt&quot;)</span><br><span class="line">  #file_rdd = sc.textFile(&quot;file:///tmp/pycharm_project_621/data/words.txt&quot;)</span><br><span class="line">  file_rdd = sc.textFile(&quot;D:\\数据挖掘与分析实验报告合集\\pyspark\\data\\input\\words.txt&quot;)</span><br><span class="line">  # 将单词进行切割, 得到一个存储全部单词的集合对象</span><br><span class="line">  words_rdd = file_rdd.flatMap(lambda line: line.split(&quot; &quot;))</span><br><span class="line">  # 将单词转换为元组对象, key是单词, value是数字1</span><br><span class="line">  words_with_one_rdd = words_rdd.map(lambda x: (x, 1))</span><br><span class="line">  # 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加)</span><br><span class="line">  result_rdd = words_with_one_rdd.reduceByKey(lambda a, b: a + b)</span><br><span class="line">  # 通过collect方法收集RDD的数据打印输出结果</span><br><span class="line">print(result_rdd.collect())</span><br></pre></td></tr></table></figure><p>运行结果截图：</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps59.jpg" alt="img" style="zoom:110%;"> <p>（2）WordCount代码远程服务器上执行。</p><p>通过SSH连接到远程服务器上，详见上述操作。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps60.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps61.jpg" alt="img" style="zoom:110%;">  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">完成与服务器连接后，会在服务器中的/tmp文件夹下新建了pycharm_project_xxx文件夹用于放置本地的同步代码。</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps62.jpg" alt="img" style="zoom:110%;">  <p>（3）读取HDFS上的文件</p><p>1、将读取文件路径改为hdfs上的&#x2F;input&#x2F;words.txt</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps63.jpg" alt="img" style="zoom:110%;">  <p>2、在hdfs上新建&#x2F;input文件夹，使用命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input</span><br></pre></td></tr></table></figure><p>3、上传words.txt到hdfs中，使用命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put words.txt /input/</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps64.jpg" alt="img" style="zoom:100%;"> <p>4、在pycharm中执行spark代码</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps65.jpg" alt="img" style="zoom:110%;">  <p>（五）spark-submit作业提交</p><p>（1）local本地模式</p><p>首先将helloword.py程序放到&#x2F;root&#x2F;目录下，使用命令完成提交作业。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master local[*] /root/helloworld.py</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps66.jpg" alt="img" style="zoom:110%;">  <p>（2）spark on yarn模式</p><p>使用命令完成提交作业。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn /root/helloworld.py</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps67.jpg" alt="img" style="zoom:110%;">  <p>（3）使用历史服务器查看任务执行情况 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node1:18080</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps68.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps69.jpg" alt="img" style="zoom:110%;"> <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark环境部署.md</li><li>[2]1-saprk基础入门.pdf</li><li>[3]spark1.pptx</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Spark（pyspark基础编码环境）&quot;&gt;&lt;a href=&quot;#Spark（pyspark基础编码环境）&quot; class=&quot;headerlink&quot; title=&quot;Spark（pyspark基础编码环境）&quot;&gt;&lt;/a&gt;&lt;strong&gt;Spark（pyspark基础编码环</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-YARN模式部署</title>
    <link href="https://weichunxiu123.github.io/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
    <id>https://weichunxiu123.github.io/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/</id>
    <published>2023-06-08T02:48:27.429Z</published>
    <updated>2023-06-09T00:30:12.987Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（YARN模式）"><a href="#spark（YARN模式）" class="headerlink" title="spark（YARN模式）"></a><strong>spark（YARN模式）</strong></h2><p>（1）Client模式中driver运行在客户端，在客户端显示输出结果，但是在spark历史服务器不显示logs信息。</p><p>（2）Cluster模式中driver运行在YARN容器内部，和ApplicationMaster在同一个容器内，在客户端不显示输出结果，所以在spark历史服务器中显示logs的信息。</p><img src="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/wps43.jpg" alt="img" style="zoom:110%;"> <p>（3）client模式测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 10</span><br></pre></td></tr></table></figure><img src="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/wps44.jpg" alt="img" style="zoom:110%;">  <p>（4） cluster模式测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m \--conf &quot;spark.pyspark.driver.python=/export/server/anaconda3</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/bin/python3&quot; \--conf &quot;spark.pyspark.python=/export/server/anaconda3</span><br><span class="line">/bin/python3&quot; $&#123;SPARK_HOME&#125;/examples/src/main/python/pi.py 10</span><br></pre></td></tr></table></figure><img src="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/wps45.jpg" alt="img" style="zoom:110%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark环境部署</li><li>[2]spark1.pptx</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（YARN模式）&quot;&gt;&lt;a href=&quot;#spark（YARN模式）&quot; class=&quot;headerlink&quot; title=&quot;spark（YARN模式）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（YARN模式）&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）Client</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-HA环境部署</title>
    <link href="https://weichunxiu123.github.io/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
    <id>https://weichunxiu123.github.io/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</id>
    <published>2023-06-08T02:34:22.238Z</published>
    <updated>2023-06-09T00:27:10.084Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（HA环境部署）"><a href="#spark（HA环境部署）" class="headerlink" title="spark（HA环境部署）"></a><strong>spark（HA环境部署）</strong></h2><p>（1）首先进入spark-env.sh中，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /export/server/spark/conf/spark-env.sh</span><br></pre></td></tr></table></figure><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps33.jpg" alt="img" style="zoom:100%;"> <p>（2）在spark-env.sh配置文件中删除</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_HOST=node1</span><br></pre></td></tr></table></figure><p>（目的是不让机器知道固定的master是谁，不然无法进行master切换）</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps34.jpg" alt="img" style="zoom:110%;"> <p>（3）在spark-env.sh配置文件中增加以下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span><br></pre></td></tr></table></figure><p># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</p><p># 指定Zookeeper的连接地址</p><p># 指定在Zookeeper中注册临时节点的路径</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps35.jpg" alt="img" style="zoom:110%;">  <p>（4）将spark-env.sh配置文件分发给node2、node3。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/spark/conf/spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line">scp -r /export/server/spark/conf/spark-env.sh node3:/export/server/spark/conf/</span><br></pre></td></tr></table></figure><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps36.jpg" alt="img" style="zoom:110%;">  <p>（5）启动StandAlone集群、zookeeper集群：</p><p>1）在node1上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure><p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps37.jpg" alt="img"> </p><p>2）在node2上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></figure><p>（目的是：备用master，当kill掉node1的master后，程序依然能进行）</p><p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps38.jpg" alt="img"> </p><p>（6）查看node1、node2的WEB UI</p><p>（如果8080端口被占用了，可以顺延到8081、8082端口，其中node1上的master是alive的，node2上的是standby） </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node1:8080--&gt;8081   </span><br><span class="line">node2:8080--&gt;8081\8082</span><br></pre></td></tr></table></figure><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps39.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps40.jpg" alt="img" style="zoom:100%;"> <p>（6）Master主备切换，在&#x2F;export&#x2F;server&#x2F;spark路径下提交一个任务到当前alive master上:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000</span><br></pre></td></tr></table></figure><p>（在提交成功后, 将alive master直接kill掉，系统不会中断，仍然能正常运行结果）</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps41.jpg" alt="img" style="zoom:100%;"> <p>（7）查看Master的WEB UI，只有node2是alive的，证明master切换成功</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps42.jpg" alt="img" style="zoom:100%;"> <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark环境部署.md</li><li>[2]1-spark基础环境配置.pdf</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（HA环境部署）&quot;&gt;&lt;a href=&quot;#spark（HA环境部署）&quot; class=&quot;headerlink&quot; title=&quot;spark（HA环境部署）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（HA环境部署）&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）首先进入sp</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-standalone环境部署</title>
    <link href="https://weichunxiu123.github.io/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
    <id>https://weichunxiu123.github.io/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</id>
    <published>2023-06-08T01:56:46.881Z</published>
    <updated>2023-06-09T00:29:40.028Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（StandAlone环境部署）"><a href="#spark（StandAlone环境部署）" class="headerlink" title="spark（StandAlone环境部署）"></a><strong>spark（StandAlone环境部署）</strong></h2><p>（一）、集群规划：选择三台机器分别为node1、node2、node3来组成集群环境。</p><p>其中node1上安装master和worker进程；node2上安装worker进程；node3上安装worker进程。</p><p>（二）、anaconda on linux安装过程：</p><p>（1）前提：在linux服务器node1、node2、node3上都安装python(anaconda)。并安装pyspark虚拟环境。具体安装步骤如下。</p><p>1、在&#x2F;export&#x2F;server&#x2F;目录下上传anaconda的安装包Anaconda3-2021.05-Linux-x86_64.sh。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps1.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps2.jpg" alt="img" style="zoom:100%;">  <p>2、安装anaconda 使用命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps3.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps4.jpg" alt="img" style="zoom:110%;"></p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps5.jpg" alt="img" style="zoom:100%;">  <p>3、安装完毕之后若没有出现base环境，进行如下配置。在&#x2F;root&#x2F;.condarc添加国内源</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps6.jpg" alt="img" style="zoom:110%;"> <p>安装完毕后，关闭服务器重新启动，出现base环境即安装成功。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps7.jpg" alt="img" style="zoom:110%;">  <p>（2）在anaconda中，安装pyspark虚拟环境。</p><p>1、基于python3.8安装pyspark环境。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps8.jpg" alt="img" style="zoom:110%;"> <p>2、切换到pyspark中，并安装所需要的安装包。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps9.jpg" alt="img" style="zoom:110%;"> <p>注：在node1、node2、node3三台服务器上都完成配置！</p><p>（三）、StandAlone模式部署</p><p>（1）安装spark压缩文件。</p><p>1、进入到&#x2F;export&#x2F;server&#x2F;中上传并解压spark-3.2.0-bin-hadoop3.2.tgz。并设置软链接，命令为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln-s/export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark。</span><br></pre></td></tr></table></figure><p>（2）在&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf，配置文件。</p><p>1、首先在配置workers文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br><span class="line">vim workers</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps10.jpg" alt="img" style="zoom:110%;"> <p>2.配置spark-env.sh文件。mv spark-env.sh.template spark-env.sh；</p><p>Vim spark-env.sh，添加如下内容。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">## 设置JAVA安装目录</span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line">\## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">\## 指定spark老大Master的IP和提交任务的通信端口</span><br><span class="line">\# 告知Spark的master运行在哪个机器上</span><br><span class="line">export SPARK_MASTER_HOST=node1</span><br><span class="line">\# 告知sparkmaster的通讯端口</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">\# 告知spark master的 webui端口</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">\# worker cpu可用核数</span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line">\# worker可用内存</span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line">\# worker的工作通讯地址</span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line">\# worker的 webui地址</span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">\## 设置历史服务器</span><br><span class="line">\# 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps11.jpg" alt="img" style="zoom:110%;">  <p>3、在HDFS上创建程序运行历史记录存放的文件夹。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog；hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure><p>4、配置spark-defaults.conf文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vim spark-defaults.conf</span><br></pre></td></tr></table></figure><p>添加如下内容。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps12.jpg" alt="img" style="zoom:110%;"><p>5、配置log4j.properties 文件[可选配置]。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure><p>修改配置，设置级别为WARN 只输出警告和错误日志。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps13.jpg" alt="img" style="zoom:110%;">  <p>（四）、将spark分发到node2和node3服务器上。注意同时要设置软链接。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/</span><br><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/</span><br><span class="line">ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><p>注意：配置&#x2F;etc&#x2F;profile，JAVA_HOME；SPARK_HOME；PYSPARK_PYTHON都指向正确的目录。</p><p>（五）、启动历史服务器，启动Spark的Master和Worker进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（1）启动历史服务器：sbin/start-history-server.sh</span><br><span class="line">（2）启动全部的master和worker：sbin/start-all.Sh</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps14.jpg" alt="img" style="zoom:110%;"> <p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps15.jpg" alt="img" style="zoom:100%;"><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps16.jpg" alt="img" style="zoom:100%;"> </p><p>（六）、查看Master的WEB UI 在浏览器中输入node1:8080</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps17.jpg" alt="img" style="zoom:100%;"> <p>（七）、连接到StandAlone集群</p><p>（1）通过master来连接到StandAlone集群。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master spark://node1:7077</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps18.jpg" alt="img" style="zoom:110%;"> <p>（2）使用spark-shell连接StandAlone集群。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell –master spark://node1:7077</span><br></pre></td></tr></table></figure><p>进行测试。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps19.jpg" alt="img" style="zoom:110%;"> <p>（3）使用spark-submit(PI)提交任务到集群上执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit –master spark://node1:7077/export/server/spark/examples/src/main/Python/pi.py 10</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps20.jpg" alt="img" style="zoom:110%;"><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps21.jpg" alt="img" style="zoom:110%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">查看历史服务器：在浏览器中输入node1：18080</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps22.jpg" alt="img" style="zoom:110%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark(standalone部署文档)</li><li>[2]1-spark基础入门.pdf</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（StandAlone环境部署）&quot;&gt;&lt;a href=&quot;#spark（StandAlone环境部署）&quot; class=&quot;headerlink&quot; title=&quot;spark（StandAlone环境部署）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（StandAlon</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-local模式配置</title>
    <link href="https://weichunxiu123.github.io/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/"/>
    <id>https://weichunxiu123.github.io/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/</id>
    <published>2023-06-07T11:38:25.725Z</published>
    <updated>2023-06-09T00:28:24.047Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（local环境部署）"><a href="#spark（local环境部署）" class="headerlink" title="spark（local环境部署）"></a><strong>spark（local<strong><strong>环境部署</strong></strong>）</strong></h2><p>（1）安装Anaconda</p><p>上传安装包 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps110.jpg" alt="img" style="zoom:110%;"> <p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps110.jpg" alt="img" style="zoom:110%;">出现（base)即为安装成功</p><p>（2）创建虚拟环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br><span class="line"></span><br><span class="line">conda activate pyspark</span><br><span class="line"></span><br><span class="line">pip install pyhive pyspark jieba -i [https://pypi.tuna.tsinghua.edu.cn](https://pypi.tuna.tsinghua.edu.cn/simple)/simple</span><br></pre></td></tr></table></figure><p>（1）修改环境变量配置Spark由如下5个环境变量需要设置</p><p>SPARK_HOME: 表示Spark安装路径在哪里 </p><p>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器。 </p><p>JAVA_HOME: 告知Spark Java在哪里 </p><p>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 </p><p>HADOOP_HOME: 告知Spark  Hadoop安装在哪里</p><p>这5个环境变量 都需要配置在: &#x2F;etc&#x2F;profile中！</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps111.jpg" alt="img" style="zoom:110%;"> <p>（4）解压</p><p>解压下载的Spark安装包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps113.jpg" alt="img" style="zoom:110%;"><p>设置软连接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><p>（2）测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()</span><br></pre></td></tr></table></figure><p>bin&#x2F;pyspark在这个环境可以运行spark代码，如图：</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps115.jpg" alt="img" style="zoom:110%;"><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps114.jpg" alt="img" style="zoom:110%;"><p>在所有机器安装Python(Anaconda)，并在所有机器配置环境变量。 </p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps116.jpg" alt="img" style="zoom:110%;">  <img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps117.jpg" alt="img" style="zoom:110%;"> <p>（3）配置配置文件</p><p>进入到spark的配置文件目录中, cd $SPARK_HOME&#x2F;conf&#96;</p><p>配置workers文件vi workers</p><p># 改名, 去掉后面的.template后缀</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br></pre></td></tr></table></figure><p># 编辑worker文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure><p># 将里面的localhost删除, 追加到workers文件内</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure><p>（4）配置spark-env.sh文件</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps118.jpg" alt="img" style="zoom:100%;"><p># 1. 改名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p># 2. 编辑spark-env.sh, 在底部追加如下内容</p><p>## 设置JAVA安装目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/export/server/jdk</span><br></pre></td></tr></table></figure><p>## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><p>## 指定spark老大Master的IP和提交任务的通信端口</p><p># 告知Spark的master运行在哪个机器上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_HOST=node1</span><br></pre></td></tr></table></figure><p># 告知sparkmaster的通讯端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure><p># 告知spark master的 webui端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br></pre></td></tr></table></figure><p># worker cpu可用核数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_CORES=1</span><br></pre></td></tr></table></figure><p># worker可用内存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_MEMORY=1g</span><br></pre></td></tr></table></figure><p># worker的工作通讯地址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_PORT=7078</span><br></pre></td></tr></table></figure><p># worker的 webui地址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br></pre></td></tr></table></figure><p>## 设置历史服务器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps119.jpg" alt="img" style="zoom:100%;"><p>在HDFS上创建程序运行历史记录存放的文件夹:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps120.jpg" alt="img" style="zoom:100%;"><p>（3）配置spark-defaults.conf文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 1. 改名</span><br><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 2. 修改内容, 追加如下内容</span><br><span class="line"># 开启spark的日期记录功能</span><br><span class="line">spark.eventLog.enabled true</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置spark日志记录的路径</span><br><span class="line">spark.eventLog.dir hdfs://node1:8020/sparklog/ </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 设置spark日志是否启动压缩</span><br><span class="line">spark.eventLog.compress true</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps121.jpg" alt="img" style="zoom:100%;"><p>（4)配置log4j.properties 文件 [可选配置]</p><p>mv log4j.properties.template log4j.properties 注意：将Spark安装文件夹  分发到其它的服务器</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps122.jpg" alt="img" style="zoom:100%;"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1）scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/</span><br><span class="line"></span><br><span class="line">2）scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/</span><br><span class="line"></span><br><span class="line">3）在node2和node3上 给spark安装目录增加软链接</span><br><span class="line"></span><br><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps123.jpg" alt="img" style="zoom:100%;"> <p>（5）启动历史服务器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure> <img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/image-20230607195152782.png" alt="image-20230607195152782" style="zoom:100%;"><p>（6）启动Spark的Master和Worker进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-master.sh</span><br><span class="line">sbin/start-worker.sh</span><br><span class="line">sbin/stop-all.sh</span><br><span class="line">sbin/stop-master.sh</span><br><span class="line">sbin/stop-worker.sh</span><br></pre></td></tr></table></figure><p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps126.jpg" alt="img"> </p><p>查看Master的WEB UI</p><p>连接到StandAlone集群</p><p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps128.jpg" alt="img"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark部署.md</li><li>[2]spark 1.pptx</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（local环境部署）&quot;&gt;&lt;a href=&quot;#spark（local环境部署）&quot; class=&quot;headerlink&quot; title=&quot;spark（local环境部署）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（local&lt;strong&gt;&lt;strong&gt;环境部</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Docker安装与基本操作</title>
    <link href="https://weichunxiu123.github.io/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>https://weichunxiu123.github.io/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</id>
    <published>2023-06-07T10:25:11.000Z</published>
    <updated>2023-06-07T12:13:23.159Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、安装docker"><a href="#一、安装docker" class="headerlink" title="一、安装docker"></a>一、<strong>安装docker</strong></h2><p>（1）卸载（可选）</p><p>如果之前安装过旧版本的Docker，可以使用下面命令卸载：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum remove docker \</span><br><span class="line">docker-client \</span><br><span class="line">docker-client-latest \</span><br><span class="line">docker-common \</span><br><span class="line">docker-latest \</span><br><span class="line">docker-latest-logrotate \</span><br><span class="line">docker-logrotate \</span><br><span class="line">docker-selinux \</span><br><span class="line">docker-engine-selinux \</span><br><span class="line">docker-engine \</span><br><span class="line">docker-ce</span><br></pre></td></tr></table></figure><p>（2）yum源配置</p><p>1.备份配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps45.jpg" alt="img" style="zoom:110%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo [http://mirrors.aliyun](http://mirrors.aliyun.com/repo/Centos-7.repo).com/repo/Centos-7.repo</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps46.jpg" alt="img" style="zoom:110%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -O /etc/yum.repos.d/epel.repo [http://mirrors.aliyun.com/rep](http://mirrors.aliyun.com/repo/epel-7.repo)o/epel-7.repo</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps47.jpg" alt="img" style="zoom:110%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps48.jpg" alt="img" style="zoom:90%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps49.jpg" style="zoom:120%;"><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y bash-completion vim lrzsz wget expect net-tools nc nmap treedos2unix htop iftop iotop unzip telnet sl psmisc nethogs glances bc ntpdate openldap-devel</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps50.jpg" alt="img" style="zoom:115%;"><p><em><strong>*安装docker*</strong></em></p><p>（1）受限需要虚拟机联网，安装yum工具</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps51.jpg" alt="img" style="zoom:110%;"> <p>（2）配置网卡转发</p><p>1）docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能</p><p>#写入</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps52.jpg" alt="img" style="zoom:110%;"> <p>2）重新加载内核参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">modprobe br_netfilter</span><br><span class="line"></span><br><span class="line">sysctl -p /etc/sysctl.d/docker.conf</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps53.jpg" alt="img" style="zoom:110%;"> <p>（3）利用yum进行docker安装</p><p>提前配置好yum仓库</p><p>1）阿里云自带仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o /etc/yum.repos.d/Centos-7.repo [http://mirrors.aliyun.com/repo/](http://mirrors.aliyun.com/repo/Centos-7.repo)Centos-7.repo</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps54.jpg" alt="img" style="zoom:110%;"> <p> 2）阿里云提供的docker专属repo仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl-o/etc/yum.repos.d/docker-ce.repohttp://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps55.jpg" alt="img" style="zoom:110%;"> <p>3）更新yum缓存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum clean all &amp;&amp; yum makecache</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps56.jpg" alt="img" style="zoom:110%;"> <p>4）查看源中可用版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum list docker-ce --showduplicates | sort -r</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps57.jpg" alt="img" style="zoom:110%;"> <p>5）yum安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install docker-ce -y</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps58.jpg" alt="img" style="zoom:110%;"><p>docker -v</p><p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps59.jpg" alt="img"> </p><p>卸载</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum remove -y docker-ce-xxx</span><br></pre></td></tr></table></figure><p>（4）配置镜像加速器</p><p>用于加速镜像文件下载,选用阿里云镜像站</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps60.jpg" alt="img" style="zoom:110%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch /etc/docker/daemon.json</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps61.jpg" alt="img" style="zoom:110%;"> <p>1）进入文件vim &#x2F;etc&#x2F;docker&#x2F;daemon.json编写以下内容：</p><p>{“registry-mirrors” : [“<a href="https://8xpk5wnt.mirror.aliyuncs.com" ]}">https://8xpk5wnt.mirror.aliyuncs.com&quot;]}</a></p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps62.jpg" alt="img" style="zoom:100%;"> <p>（5）启动docker</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1）关闭防火墙：systemctl stop firewalld</span><br><span class="line">2）禁止开机启动防火墙：systemctl disable firewalld</span><br><span class="line">3）查看防火墙状态：systemctl status firewalld</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps63.jpg" alt="img" style="zoom:110%;"> <p>通过命令启动docker：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker  启动docker服务</span><br><span class="line"></span><br><span class="line">systemctl stop docker  停止docker服务</span><br><span class="line"></span><br><span class="line">systemctl restart docker  重启docker服务</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps64.jpg" alt="img" style="zoom:100%;"> <p>docker配置文件重新加载：systemctl daemon-reload</p><p>设置开启自启动：systemctl enable docker</p><p>（6）查看docker信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker info</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps65.jpg" alt="img" style="zoom:110%;"> <p>（7）显示当前正在运行的容器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps66.jpg" alt="img" style="zoom:100%;"><p>（8）docker镜像：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps67.jpg" alt="img" style="zoom:100%;"> <p>（9）docker版本：docker version</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker-client</span><br><span class="line">which docker</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps68.jpg" alt="img" style="zoom:100%;"> <p>docker daemon，运行在docker host上，负责创建、运行、监控容器、构建、存储镜像</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux |grep docker</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps69.jpg" alt="img" style="zoom:100%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">containerd</span><br><span class="line">ps aux|grep containerd</span><br><span class="line">systemctl status containerd</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps70.jpg" alt="img" style="zoom:110%;"> <p><em><strong>*docker的基本操作*</strong></em></p><p>\1. 启动第一个docker容器</p><p>Nginx web服务器，运行一个80端口的网站</p><p>在宿主机上，运行Nginx</p><p>开启服务器</p><p>2.在服务器上安装好运行nginx所需的依赖关系</p><p>3.安装nginx yum install nginx -y</p><p>4.修改nginx配置文件</p><p>5.启动nginx</p><p>6.客户端去访问nginx</p><p>（1）查看本地的docker镜像有哪些：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image ls 或 docker images</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps71.jpg" alt="img" style="zoom:100%;"> <p>（2）可选择删除旧版本：docker rmi 镜像id</p><p>（3）搜索一下远程仓库中的镜像文件是否存在：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search nginx</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps72.jpg" alt="img" style="zoom:100%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nginx</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps73.jpg" alt="img" style="zoom:110%;"> <p>（4）再次查看镜像：docker images</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps74.jpg" alt="img" style="zoom:100%;"> <p>（5）运行镜像，运行出具体内容，在容器中就跑着一个nginx服务，docker run 参数 镜像的名字&#x2F;id</p><p>#-d 后台运行容器</p><p>#-p 80:80 端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口，会返回一个容器的id</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 80:80 nginx</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps75.jpg" alt="img" style="zoom:100%;"> <p>（6）查看容器是否在运行：docker ps</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps76.jpg" alt="img" style="zoom:100%;"> <p>（7）访问网站192.168.88.163:80</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps77.jpg" alt="img" style="zoom:110%;"> <p>（8）停止容器：docker stop 容器id</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps78.jpg" alt="img" style="zoom:90%;"> <p>（9）恢复容器：docker start 容器id</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps79.jpg" alt="img" style="zoom:90%;">  <p><em><strong>*docker镜像原理*</strong></em></p><p>（1）查看发行版： </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/redhat-release </span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps80.jpg" alt="img" style="zoom:100%;"> <p>（2）查看内核：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -r</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps81.jpg" alt="img" style="zoom:100%;"><p>（3）利用docker获取不同的发行版镜像（例如centos：7.8.2003）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull centos:7.8.2003</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps82.jpg" alt="img" style="zoom:110%;"> <p>（4）确认当前宿主机的发行版：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/redhat-release</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps83.jpg" alt="img" style="zoom:100%;"> <p>（5）运行centos:7.8.2003发行版本</p><p>#运行容器，且进入容器内部</p><p>#参数解释，-i 交互式命令操作 -t 开启一个终端 bash 进入容器后执行的命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it afb6fca791e0 bash</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps84.jpg" alt="img" style="zoom:100%;"> <p>（6）退出容器空间：exit</p><p><em><strong>*获取镜像*</strong></em></p><p>（1）docker search 镜像名:tag tag就是具体的标签版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search centos</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps85.jpg" alt="img" style="zoom:100%;"> <p>（2）查看docker镜像的存储路径：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker info |grep Root</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps86.jpg" alt="img" style="zoom:100%;"> <p>（3）具体位置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /var/lib/docker/image/overlay2/imagedb/content/sha256</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps87.jpg" alt="img" style="zoom:110%;"> <p>（4）使用不同的镜像，生成容器# -it 开启一个交互式的终端–rm 容器退出时删除该容器</p><p>#再运行一个7.8centos</p><p>docker run -it –rm centos bash</p><p><em><strong>*查看镜像*</strong></em></p><p>（1）查看所有镜像：docker images</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps88.jpg" alt="img" style="zoom:100%;"> <p>（2）指定tag查看：docker images centos:7.8.2003</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps89.jpg" alt="img" style="zoom:100%;"> <p>（3）只列出镜像id  #-q –quiet 只列出id：docker images -q</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps90.jpg" alt="img" style="zoom:100%;"> <p>（4）格式化显示镜像</p><p># 这是docker的模板语言，–format</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images --format &quot;&#123;&#123;.ID&#125;&#125;--&#123;&#123;.Repository&#125;&#125;&quot;</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps91.jpg" alt="img" style="zoom:100%;"> <p><em><strong>*删除镜像*</strong></em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">（1）删除容器记录：docker rm 容器id</span><br><span class="line">（2）指定id的前三位即可：docker rmi 镜像id</span><br></pre></td></tr></table></figure><p><em><strong>*镜像管理*</strong></em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">（1）批量删除镜像，慎用：docker rmi &#x27;docker images -aq&#x27;</span><br><span class="line">（2）批量删除容器：docker rm &#x27;docker ps -aq&#x27;</span><br><span class="line">（3）导出镜像：docker save -o nginx.tgz nginx:latest#打包tar包</span><br><span class="line"></span><br><span class="line">（4）导入镜像</span><br><span class="line">先删除本地的nginx镜像：docker rmi centos:7.8.2003</span><br><span class="line">docker image load -i /export/software/centos1.8.2003.tgz#重新加载nginx-tar包</span><br><span class="line">查看cocker服务的信息：docker info</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps92.jpg" alt="img" style="zoom:100%;"> <p>查看镜像详细信息：docker image inspact 镜像id</p><p><em><strong>*docker镜像管理练习*</strong></em></p><p>（1）去DockerHub搜索Redies</p><p>（2）利用docker pull命令拉去镜像：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull redis</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps93.jpg" alt="img" style="zoom:110%;"> <p>（3）查看Redies镜像的名称和版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search redis</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps94.jpg" alt="img" style="zoom:110%;"> <p>（4）利用docker save命令将redies:latest打包为一个redies.tar包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker save -o redis.tar redis:latest</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps95.jpg" alt="img" style="zoom:100%;"><p>（5）利用docker rmi删除本地的redis:latest </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi redis:latest</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps96.jpg" alt="img" style="zoom:100%;"> <p>（6）利用docker load重新加载Redis.tar文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load -i redis.tar</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps97.jpg" alt="img" style="zoom:100%;"> <p><em><strong>*容器操作*</strong></em></p><p>1）创建并运行mn容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mn -p 80:80 -d nginx</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps98.jpg" alt="img" style="zoom:100%;"> <p>2）运行刚刚创建的nginx容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mn bash</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps99.jpg" alt="img" style="zoom:90%;"><p>3） 进入nginx的HTML所在目录 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/share/nginx/html</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps100.jpg" alt="img" style="zoom:100%;"> <p>4） 修改index.html的内容</p><p>sed -i -e ‘s#Welcome to nginx#人工智能学院欢迎您#g’ -e ‘s#<head>#<head><meta charset="utf-8">#g’ index.html</head></head></p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps101.jpg" alt="img" style="zoom:110%;">  <p><em><strong>*创建和查看数据卷*</strong></em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.创建数据卷</span><br><span class="line">docker volume create html</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps102.jpg" alt="img" style="zoom:100%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.查看所有数据</span><br><span class="line">docker volume ls</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps103.jpg" alt="img" style="zoom:100%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3.查看数据卷详细信息卷</span><br><span class="line">docker volume inspect html</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps104.jpg" alt="img" style="zoom:110%;"> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">4.挂载数据卷</span><br><span class="line"></span><br><span class="line">（1）创建容器并挂载数据卷到容器内的HTML目录，把/export/data/docker-data/nginx-html/数据卷挂载到容器内的/user/share/nginx/html目录中：</span><br><span class="line">docker run --name mn -v /export/data/docker-data/nginx-html/:/usr/share/nginx/html -p 80:80 -d nginx</span><br><span class="line"></span><br><span class="line">（2）进入html数据卷所在位置，并修改HTML内容,查看html数据卷的位置：</span><br><span class="line">docker volume inspect /export/data/docker-data/nginx-html/</span><br><span class="line"></span><br><span class="line">（3）进入该目录</span><br><span class="line">cd /export/data/docker-data/nginx-html/_data</span><br><span class="line"></span><br><span class="line">（4）修改文件vi index.html</span><br></pre></td></tr></table></figure><p><em><strong>*Docker应用部署*</strong></em></p><ol><li><p>搜索mysql镜像：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search mysql</span><br></pre></td></tr></table></figure></li></ol><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps105.jpg" alt="img" style="zoom:100%;"> <ol start="2"><li>拉取mysql镜像</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull mysql:5.6</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps106.jpg" alt="img" style="zoom:100%;">  <ol start="3"><li>创建容器，设置端口映射、目录映射</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/docker-data/mysql</span><br><span class="line">cd /export/data/docker-data/mysql</span><br><span class="line">docker run -id \</span><br><span class="line">-p 3306:3306 \</span><br><span class="line">--name=bigdata_mysql \</span><br><span class="line">-v $PWD/conf:/etc/mysql/conf.d \</span><br><span class="line">-v $PWD/logs:/logs \</span><br><span class="line">-v $PWD/data:/var/lib/mysql \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=hadoop \</span><br><span class="line">mysql:5.7.29</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps107.jpg" alt="img" style="zoom:100%;"> <ol start="4"><li>进入容器，操作mysql</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec –it bigdata_mysql /bin/bash</span><br></pre></td></tr></table></figure><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps108.jpg" alt="img" style="zoom:100%;"> <ol start="5"><li>使用外部机器连接容器中的mysql</li></ol><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps109.jpg" alt="img" style="zoom:100%;">  <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><p>docker应用部署</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、安装docker&quot;&gt;&lt;a href=&quot;#一、安装docker&quot; class=&quot;headerlink&quot; title=&quot;一、安装docker&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;安装docker&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）卸载（可选）&lt;/p&gt;
&lt;p&gt;如果之前</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Git安装与配置</title>
    <link href="https://weichunxiu123.github.io/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>https://weichunxiu123.github.io/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</id>
    <published>2023-06-07T09:39:32.518Z</published>
    <updated>2023-06-09T00:27:50.079Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Git安装"><a href="#一、Git安装" class="headerlink" title="一、Git安装"></a>一、<strong>Git安装</strong></h2><p>（1）Git下载</p><p>Git下载程序</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps21.jpg" alt="img" style="zoom:110%;"> <p>（2）可视化客户端</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps22.jpg" alt="img" style="zoom:100%;"><p>中文语言包</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps23.jpg" alt="img" style="zoom:110%;"> <p>（3）初始化仓库</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps24.jpg" alt="img" style="zoom:110%;">  <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps25.jpg" alt="img" style="zoom:100%;"> <p>（4）添加文件，提交文件至本地仓库</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps26.jpg" alt="img" style="zoom:100%;"> <p>（5）本地删除与恢复</p><p>文件选中删除，可用以下方式还原</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps27.jpg" alt="img" style="zoom:110%;">  <p>（6）创建分支</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps28.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps29.jpg" alt="img" style="zoom:100%;"> <p>（7）分支的查看切换</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps30.jpg" alt="img" style="zoom:110%;">  <p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps31.jpg" alt="img" style="zoom:110%;"><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps32.jpg" alt="img" style="zoom:110%;"> </p><p>（8）标签的创建</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps33.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps34.jpg" alt="img" style="zoom:100%;"> <p>（9）切换与删除</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps35.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps36.jpg" alt="img" style="zoom:110%;"> <p>通过右键选中删除</p><p><em><strong>*远程仓库*</strong></em></p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps37.jpg" alt="img" style="zoom:100%;">  <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps38.jpg" alt="img" style="zoom:100%;"> <p>（1）码云账号注册</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps39.jpg" alt="img" style="zoom:100%;"> <p>填写邮箱发送验证码,然后可以注册账号,主页如下</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps40.jpg" alt="img" style="zoom:100%;"> <p>（2）创建远程仓库</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps41.jpg" alt="img" style="zoom:100%;"> <p>（3）把本地代码推送到远端</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps42.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps43.jpg" alt="img" style="zoom:90%;"> <p>生成公钥私钥</p><p>ssh-keygen -t rsa</p><p>一直回车</p><p>会默认用户目录 .ssh 目录生成一个默认的id_rsa文件 和id_rsa.pub</p><p>密钥配置</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps44.jpg" alt="img" style="zoom:100%;">  <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li><u><em><a href="%E8%AF%BE%E4%B8%8AGit%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3">1</a></em></u></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、Git安装&quot;&gt;&lt;a href=&quot;#一、Git安装&quot; class=&quot;headerlink&quot; title=&quot;一、Git安装&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;Git安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）Git下载&lt;/p&gt;
&lt;p&gt;Git下载程序&lt;/p&gt;
&lt;img </summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础配置</title>
    <link href="https://weichunxiu123.github.io/wiki/hive%E5%AE%89%E8%A3%85/"/>
    <id>https://weichunxiu123.github.io/wiki/hive%E5%AE%89%E8%A3%85/</id>
    <published>2023-06-07T08:56:02.680Z</published>
    <updated>2023-06-09T00:23:47.164Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Hive安装"><a href="#一、Hive安装" class="headerlink" title="一、Hive安装"></a>一、<strong>Hive安装</strong></h2><p><em><strong>*（1）*</strong></em><em><strong>*Mysql安装*</strong></em></p><p>1）卸载Centos7自带的mariadb</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps1.jpg" alt="img" style="zoom:115%;"><p>如果出现了mariadb-libs-5.5.64-1.el7.x86_64，输入rpm -e mariadb- libs-5.5.64-1.el7.x86_64 –nodeps,在输入rpm -qa|grep mariadb，即可</p><p>2）安装mysql</p><p>新建文件夹</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/server/mysql</span><br></pre></td></tr></table></figure><p>上传mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar到上述文件夹下，解压</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar</span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps2.jpg" alt="img" style="zoom:130%;"><p>3）执行安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install libaio</span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps3.jpg" alt="img" style="zoom:120%;"><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps4.jpg" alt="img" style="zoom:130%;"> <p>4）mysql初始化设置</p><p>初始化：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqld --initialize</span><br></pre></td></tr></table></figure><p>更改所属组：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown mysql:mysql /var/lib/mysql -R</span><br></pre></td></tr></table></figure><p>启动mysql：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mysqld.service</span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps5.jpg" alt="img" style="zoom:110%;"> <p>查看临时生成的root密码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat  /var/log/mysqld.log</span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps6.jpg" alt="img" style="zoom:130%;"> <p>5）修改root密码 授权远程访问 设置开机自启动</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps7.jpg" alt="img" style="zoom:120%;"> <p>修改root密码 设置为hadoop</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps8.jpg" alt="img" style="zoom:120%;"> <p>授权</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">use mysql;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;hadoop&#x27; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps9.jpg" alt="img" style="zoom:100%;"> <p>mysql的启动和关闭 状态查看 （这几个命令必须记住）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop mysqld</span><br><span class="line"></span><br><span class="line">systemctl status mysqld</span><br><span class="line"></span><br><span class="line">systemctl start mysqld</span><br><span class="line"></span><br><span class="line">设置开机自动启动：systemctl enable  mysqld </span><br></pre></td></tr></table></figure><p>查看是否设置自动启动成功</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps10.jpg" alt="img" style="zoom:110%;"> <p><em><strong>*（2）*</strong></em><em><strong>*H*</strong></em><em><strong>*ive*</strong></em><em><strong>*的安装*</strong></em></p><p>1）上传安装包 解压</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf apache-hive-3.1.2-bin.tar.gz</span><br><span class="line"></span><br><span class="line">ln -s apache-hive-3.1.2-bin hive</span><br></pre></td></tr></table></figure><p>2）解决Hive与Hadoop之间guava版本差异</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hive/</span><br><span class="line"></span><br><span class="line">rm -rf lib/guava-19.0.jar</span><br><span class="line"></span><br><span class="line">cp /export/server/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar./lib/</span><br></pre></td></tr></table></figure><p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps11.jpg" alt="img"> </p><p>3）修改配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive-env.sh</span><br><span class="line"></span><br><span class="line">cd /export/server/hive/conf</span><br><span class="line"></span><br><span class="line">mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim hive-env.sh</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/export/server/hadoop</span><br><span class="line"></span><br><span class="line">export HIVE_CONF_DIR=/export/server/hive/conf</span><br><span class="line"></span><br><span class="line">export HIVE_AUX_JARS_PATH=/export/server/hive/lib</span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps12.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/hive%E5%AE%89%E8%A3%85/wps13.jpg" alt="img" style="zoom:100%;"> <p>hive-site.xml</p><p>vim hive-site.xml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 存储元数据mysql相关配置 --&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;jdbc:mysql://node1:3306/hive3?createDatabaseIfNotExist=true&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">&lt;value&gt;hadoop&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- H2S运行绑定host --&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">  &lt;value&gt;node1&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 远程模式部署metastore metastore地址 --&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">  &lt;value&gt;thrift://node1:9083&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 关闭元数据存储授权  --&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>4）上传mysql jdbc驱动到hive安装包lib下</p><p>mysql-connector-java-5.1.32.jar</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps14.jpg" alt="img" style="zoom:110%;"> <p>5）初始化元数据</p><p>cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/schematool -initSchema -dbType mysql -verbos</span><br></pre></td></tr></table></figure><p>初始化成功之后会在MySQL中创建74张表</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps15.jpg" alt="img" style="zoom:115%;"> <p>6）在hdfs创建hive存储目录（如存在则不用操作）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /tmp</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod g+w /tmp</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><p>7）启动hive</p><p><em><strong>*（*</strong></em><em><strong>*3）*</strong></em><em><strong>*启动metastore服务*</strong></em> 前台启动  关闭ctrl+c</p><p>&#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive –service metastore</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps16.jpg" alt="img" style="zoom:110%;"> <p>前台启动开启debug日志</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/hive/bin/hive --service metastore --hiveconf hive.root.logger=DEBUG,console  </span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps17.jpg" alt="img" style="zoom:100%;"> <p>后台启动 进程挂起  关闭使用jps+ kill -9</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup /export/server/hive/bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps18.jpg" alt="img" style="zoom:100%;"> <p><em><strong>*（*</strong></em><em><strong>*4）*</strong></em><em><strong>*启动hiveserver2服务*</strong></em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup /export/server/hive/bin/hive --service hiveserver2 &amp;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps19.jpg" alt="img" style="zoom:120%;"> <img src="/wiki/hive%E5%AE%89%E8%A3%85/wps20.jpg" alt="img" style="zoom:110%;"> <p>beeline客户端连接</p><p>拷贝node1安装包到beeline客户端机器上（node3）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/apache-hive-3.1.2-bin/ root@node3:/export/server/</span><br></pre></td></tr></table></figure><p><em><strong>*（5）*</strong></em><em><strong>*hive注释信息中文乱码解决*</strong></em></p><p>以下sql语句均在mysql数据库中执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">use hivenode2;</span><br><span class="line"></span><br><span class="line">show tables;</span><br><span class="line"></span><br><span class="line">alter table hivenode2.COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;</span><br><span class="line"></span><br><span class="line">alter table hivenode2.TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line"></span><br><span class="line">alter table hivenode2.PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;</span><br><span class="line"></span><br><span class="line">alter table hivenode2.PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</span><br><span class="line"></span><br><span class="line">alter table hivenode2.INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、Hive安装&quot;&gt;&lt;a href=&quot;#一、Hive安装&quot; class=&quot;headerlink&quot; title=&quot;一、Hive安装&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;Hive安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;*（1）*&lt;/strong&gt;&lt;/e</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://weichunxiu123.github.io/wiki/hello-world/"/>
    <id>https://weichunxiu123.github.io/wiki/hello-world/</id>
    <published>2023-06-07T00:16:18.933Z</published>
    <updated>2023-06-09T00:30:25.185Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
