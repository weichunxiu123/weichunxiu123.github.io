<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>weichunxiu123</title>
  
  
  <link href="https://weichunxiu123.github.io/atom.xml" rel="self"/>
  
  <link href="https://weichunxiu123.github.io/"/>
  <updated>2023-06-14T11:55:24.718Z</updated>
  <id>https://weichunxiu123.github.io/</id>
  
  <author>
    <name>Bigdata_weichunxiu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>安装kafka以及kafka命令行操作</title>
    <link href="https://weichunxiu123.github.io/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
    <id>https://weichunxiu123.github.io/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/</id>
    <published>2023-06-14T11:33:38.632Z</published>
    <updated>2023-06-14T11:55:24.718Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kafka安装"><a href="#kafka安装" class="headerlink" title="kafka安装"></a><strong>kafka安装</strong></h2><p>安装Kafka集群</p><p>（1）上传安装包</p><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps53.jpg" alt="img" style="zoom:100%;"> <p>\1. 修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">修改文件内容#进入配置文件目录：cd &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1&#x2F;config#编辑配置文件vi server.properties#为依次增长的:0、1、2、3、4,集群中唯一 id -- 从0开始，每台不能重复，第一块要改的。broker.id&#x3D;0 #数据存储的目录，第二块要改的：log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs #指定 zk 集群地址，第三块要改的zookeeper.connect&#x3D;node1:2181，node2:2181<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps54.jpg" alt="img" style="zoom:100%;"><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps55.jpg" alt="img" style="zoom:100%;"><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps56.jpg" alt="img" style="zoom:100%;"><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps57.jpg" alt="img" style="zoom:100%;"> <p>（2）分发kafla</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;syncfile &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（3）配置环境变量</p><pre class="line-numbers language-none"><code class="language-none">export KAFKA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1 export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意:还需要分发环境变量！</p><pre class="line-numbers language-none"><code class="language-none">syncfile &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）在node2上修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;server.propertie<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>更改broker.id&#x3D;1</p><pre class="line-numbers language-none"><code class="language-none">log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）启动集群（各个节点）</p><p>启动：</p><pre class="line-numbers language-none"><code class="language-none">kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;server.properties<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps58.jpg" alt="img" style="zoom:100%;"><p>关闭：</p><pre class="line-numbers language-none"><code class="language-none">kafka-server-stop.sh stop<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps59.jpg" alt="img" style="zoom:100%;"><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps60.jpg" alt="img" style="zoom:100%;"><p>（6）kafka命令行操作</p><p>1）创建topic</p><p>基本方式</p><pre class="line-numbers language-none"><code class="language-none">.&#x2F;kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps61.jpg" alt="img" style="zoom:100%;"><p>2）手动指定副本的存储位置</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps62.jpg" alt="img" style="zoom:100%;"><p>3）查看目前Kafka中的主题</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --list --bootstrap-server node1:9092<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps63.jpg" alt="img" style="zoom:100%;"> <p>4）查看topic</p><pre class="line-numbers language-none"><code class="language-none">列出当前系统中的所有 topickafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps64.jpg" alt="img" style="zoom:100%;"> <p>5）查看 topic 详细信息</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps65.jpg" alt="img" style="zoom:100%;"><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --describe --topic tpc_1 --zookeeper node1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps66.jpg" alt="img"> </p><p>6）增加分区数</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps67.jpg" alt="img"></p><p>7）动态配置topic 参数</p><p>添加参数</p><pre class="line-numbers language-none"><code class="language-none">kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type&#x3D;gzip <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps68.jpg" alt="img" style="zoom:105%;"> <p>8）删除参数</p><pre class="line-numbers language-none"><code class="language-none">kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps69.jpg" alt="img" style="zoom:110%;"> <p>9）生产消息到Kafka并进行消费</p><pre class="line-numbers language-none"><code class="language-none">kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps70.jpg" alt="img" style="zoom:110%;"> <p>10）消费信息（从头开始）</p><pre class="line-numbers language-none"><code class="language-none">kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps71.jpg" alt="img" style="zoom:110%;"><p>11）指定要消费的分区,和要消费的起始 offset</p><pre class="line-numbers language-none"><code class="language-none">kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps72.jpg" alt="img" style="zoom:110%;"> <p>12）配置管理kafka-configs</p><pre class="line-numbers language-none"><code class="language-none">kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps73.jpg" alt="img" style="zoom:120%;"> <p>Kafka基准测试</p><p>1.三分区，两副本</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --create --topic tpc_3 --partitions 2 --replication-factor 1 --zookeeper node1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps74.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">kafka-producer-perf-test.sh --topic tpc_3 --num-records 1000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers&#x3D;node1:9092 acks&#x3D;1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps75.jpg" alt="img" style="zoom:110%;"> <p>\2. 四分区，两副本</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --create --topic tpc_4 --partitions 2 --replication-factor 2 --zookeeper node1:2181kafka-producer-perf-test.sh --topic tpc_4 --num-records 1000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers&#x3D;node1:9092 acks&#x3D;1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps76.jpg" alt="img" style="zoom:110%;"> <p>\3. 七分区，十二副本</p><pre class="line-numbers language-none"><code class="language-none">kafka-topics.sh --create --topic tpc_7 --partitions 12 --replication-factor 1 --zookeeper node1:2181kafka-producer-perf-test.sh --topic tpc_7 --num-records 1000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers&#x3D;node1:9092 acks&#x3D;1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85kafka%E4%BB%A5%E5%8F%8Akafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/wps77.jpg" alt="img" style="zoom:110%;"> <p>由此可知：在一定限度内：副本增多，吞吐量变大。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]kafka安装配置文档.md</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;kafka安装&quot;&gt;&lt;a href=&quot;#kafka安装&quot; class=&quot;headerlink&quot; title=&quot;kafka安装&quot;&gt;&lt;/a&gt;&lt;strong&gt;kafka安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;安装Kafka集群&lt;/p&gt;
&lt;p&gt;（1）上传安装包&lt;/p&gt;
&lt;im</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="kafka" scheme="https://weichunxiu123.github.io/tags/kafka/"/>
    
    <category term="kafka命令行操作" scheme="https://weichunxiu123.github.io/tags/kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper安装以及zookeeperJavaAPI操作命令</title>
    <link href="https://weichunxiu123.github.io/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
    <id>https://weichunxiu123.github.io/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/</id>
    <published>2023-06-14T11:02:56.070Z</published>
    <updated>2023-06-14T11:32:13.666Z</updated>
    
    <content type="html"><![CDATA[<h2 id="zookeeper安装"><a href="#zookeeper安装" class="headerlink" title="zookeeper安装"></a><strong>zookeeper安装</strong></h2><p>安装前需要安装好jdk，检测集群时间是否同步，检测防火墙是否关闭，检测主机 ip映射有没有配置</p><p>（1）在node1上切换到 &#x2F;export&#x2F;server 目录下，上传zookeeper压缩包并解压，设置一个软连接。</p><p>切换到server目录下：</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>解压zookeeper压缩包：</p><pre class="line-numbers language-none"><code class="language-none">tar -zxvf zookeeper.tar.gz -C &#x2F;export&#x2F;server&#x2F;  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>设置软连接：</p><pre class="line-numbers language-none"><code class="language-none">ln -s  &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; zookeeper  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps23.jpg" alt="img" style="zoom:110%;"> <p>\1. 修改环境变量（三台都修改）</p><pre class="line-numbers language-none"><code class="language-none">vi &#x2F;etc&#x2F;profileexport ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeperexport PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;binsource &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps24.jpg" alt="img" style="zoom:110%;"> <p>\2. 修改zookeeper配置文件 </p><p>切换到conf&#x2F;目录下</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>复制zoo_sample.cfg文件，文件名为zoo.cfg</p><pre class="line-numbers language-none"><code class="language-none">cp zoo_sample.cfg zoo.cfg <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps25.jpg" alt="img" style="zoom:100%;"> <p>创建文件：</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;   <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps26.jpg" alt="img" style="zoom:100%;"><pre class="line-numbers language-none"><code class="language-none">vim zoo.cfg <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>填充以下内容：</p><pre class="line-numbers language-none"><code class="language-none">#Zookeeper的数据存放目录dataDir &#x3D; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;# 保留多少个快照autopurge.snapRetainCount &#x3D; 3# 日志多少小时清理一次autopurge.purgeInterval &#x3D; 1# 集群中服务器地址server.1 &#x3D; node1:2888:3888server.2 &#x3D; node2:2888:3888server.3 &#x3D; node3:2888:3888<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps27.jpg" alt="img" style="zoom:100%;"><p>\3. 添加myid配置</p><p>在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1</p><pre class="line-numbers language-none"><code class="language-none">echo 1 &gt; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps28.jpg" alt="img" style="zoom:100%;"><p>\4. 安装包分发并修改myid的值</p><p>在node1主机上，将安装包分发到其他机器</p><p>第一台机器上面执行以下两个命令</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; root@node2:&#x2F;export&#x2F;server&#x2F;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>分发node2</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; root@node3:&#x2F;export&#x2F;server&#x2F;  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>分发node3</p><pre class="line-numbers language-none"><code class="language-none">ln -s zookeeper-3.4.6&#x2F; zookeeper  建立软连接（node2 node3）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps29.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps30.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">echo 2 &gt; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid  （node2上执行）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps31.jpg" alt="img" style="zoom:100%;"><pre class="line-numbers language-none"><code class="language-none">echo 3 &gt; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid  （node3上执行）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps32.jpg" alt="img" style="zoom:110%;">  <p>\5. 三台机器启动zookeeper服务</p><pre class="line-numbers language-none"><code class="language-none">&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps33.jpg" alt="img"> </p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps34.jpg" alt="img" style="zoom:100%;"><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps35.jpg" alt="img" style="zoom:100%;"> <p>（2） 三台主机分别查看启动状态</p><pre class="line-numbers language-none"><code class="language-none">&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps36.jpg" alt="img"><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps37.jpg" alt="img" style="zoom:100%;"> </p><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps38.jpg" alt="img"></p><p>\6. 编写一个脚本批量启动node1，2，3的zookeeper</p><p>（1）创建shell目录</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;shell<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps39.jpg" alt="img" style="zoom:100%;"> <p>（2）再此目录下建立一个zkall.sh文件</p><pre class="line-numbers language-none"><code class="language-none">vim zkall.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加以下内容：</p><pre class="line-numbers language-none"><code class="language-none">#!&#x2F;bin&#x2F;bashcase $1 in&quot;start&quot;)&#123;for i in node1 node2 node3doecho ---------- zookeeper $i 启动 ------------ssh $i &quot;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start&quot;done&#125;;;&quot;stop&quot;)&#123;for i in node1 node2 node3doecho ---------- zookeeper $i 停止 ------------ ssh $i &quot;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop&quot;done&#125;;;&quot;status&quot;)&#123;for i in node1 node2 node3doecho ---------- zookeeper $i 状态 ------------ ssh $i &quot;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status&quot;done&#125;;;Esac<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）配置zk脚本环境变量、zookeeper的环境变量</p><pre class="line-numbers language-none"><code class="language-none">vi &#x2F;etc&#x2F;profile#ZOOKEEPER_SHELL_HOMEexport ZKS_HOME&#x3D;&#x2F;export&#x2F;shell&#x2F;export PATH&#x3D;$PATH:$ZKS_HOMEexport ZK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeperexport PATH&#x3D;$&#123;ZK_HOME&#125;&#x2F;bin:$PATH<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps40.jpg" alt="img"> </p><p>让环境变量生效</p><pre class="line-numbers language-none"><code class="language-none">source &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4） 设置环境路径</p><pre class="line-numbers language-none"><code class="language-none">vim .bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps41.jpg" alt="img"> </p><p>（5） 增加可执行权限</p><pre class="line-numbers language-none"><code class="language-none">chmod +x zkall.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps42.jpg" alt="img"></p><p>（6）启动测试</p><pre class="line-numbers language-none"><code class="language-none">zkall.sh start<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps43.jpg" alt="img"> </p><pre class="line-numbers language-none"><code class="language-none">查看状态zkall.sh status<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps44.jpg" alt="img"> </p><p>（7）zookeeper服务器常用命令</p><pre class="line-numbers language-none"><code class="language-none">启动 ZooKeeper 服务 .&#x2F;zkServer.sh start查看 ZooKeeper 服务状态 .&#x2F;zkServer.sh status停止 ZooKeeper 服务 .&#x2F;zkServer.sh stop 重启 ZooKeeper 服务 .&#x2F;zkServer.sh restart <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>\7. Zookeerper命令操作</p><p>（1）连接ZooKeeper服务端</p><pre class="line-numbers language-none"><code class="language-none">.&#x2F;zkCli.sh -server node1:2181<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps45.jpg" alt="img"> </p><p>\8. ZooKeeper JavaAPI 操作</p><p>（1）建立连接</p><pre class="line-numbers language-none"><code class="language-none">client &#x3D; CuratorFrameworkFactory.builder()        .connectString(&quot;192.168.88.161:2181&quot;)        .sessionTimeoutMs(60 * 1000)        .connectionTimeoutMs(15 * 1000        .retryPolicy(retryPolicy)        .namespace(&quot;bigdata&quot;)        .build();    client.start();<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps46.jpg" alt="img" style="zoom:105%;"> <p>（2）创建节点（带有数据）</p><pre class="line-numbers language-none"><code class="language-none">String path &#x3D; client.create().forPath(&quot;&#x2F;app2&quot;, &quot;hehe&quot;.getBytes())System.out.println(path);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps47.jpg" alt="img" style="zoom:105%;"> <p>（3）查询结点</p><pre class="line-numbers language-none"><code class="language-none">byte[] data &#x3D; client.getData().forPath(&quot;&#x2F;app2&quot;)System.out.println(new String(data));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps48.jpg" alt="img" style="zoom:105%;"> <p>（4）修改数据</p><pre class="line-numbers language-none"><code class="language-none">client.setData().forPath(&quot;&#x2F;app2&quot;, &quot;bigdata&quot;.getBytes())<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps49.jpg" alt="img" style="zoom:100%;"> <p>（5）删除节点</p><pre class="line-numbers language-none"><code class="language-none">client.delete().forPath(&quot;&#x2F;app1&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps50.jpg" alt="img" style="zoom:110%;"><p>（6）Watch事件监听</p><pre class="line-numbers language-none"><code class="language-none">TreeCache treeCache &#x3D; new TreeCache(client,&quot;&#x2F;app2&quot;);    treeCache.getListenable().addListener(new TreeCacheListener() &#123;      @Override      public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception &#123;        System.out.println(&quot;节点变化了&quot;);        System.out.println(event);     &#125;    &#125;);     treeCache.start();<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps51.jpg" alt="img" style="zoom:110%;"><p>（7）分布式锁实现</p><pre class="line-numbers language-none"><code class="language-none">public static void main(String[] args) &#123;    Ticket12306_old ticket12306_old &#x3D; new Ticket12306_old();    Thread t1 &#x3D; new Thread(ticket12306_old,&quot;携程&quot;);    Thread t2 &#x3D; new Thread(ticket12306_old,&quot;飞猪&quot;);    t1.start();    t2.start();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/%E5%AE%89%E8%A3%85zookeeper%E4%BB%A5%E5%8F%8AzookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/wps52.jpg" alt="img" style="zoom:110%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]zookeeper安装文档.md</li><li>[2]zookeeper.ppt</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;zookeeper安装&quot;&gt;&lt;a href=&quot;#zookeeper安装&quot; class=&quot;headerlink&quot; title=&quot;zookeeper安装&quot;&gt;&lt;/a&gt;&lt;strong&gt;zookeeper安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;安装前需要安装好jdk，检测集群时</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="zookeeper" scheme="https://weichunxiu123.github.io/tags/zookeeper/"/>
    
    <category term="zookeeperJavaAPI操作命令" scheme="https://weichunxiu123.github.io/tags/zookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>sqoop安装以及导入和导出</title>
    <link href="https://weichunxiu123.github.io/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/"/>
    <id>https://weichunxiu123.github.io/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/</id>
    <published>2023-06-14T10:14:47.696Z</published>
    <updated>2023-06-14T10:54:00.573Z</updated>
    
    <content type="html"><![CDATA[<h2 id="sqoop安装"><a href="#sqoop安装" class="headerlink" title="sqoop安装"></a><strong>sqoop安装</strong></h2><p>（1）首先需要有java、mysql、hadoop和hive环境</p><p>（2）将以下下载到CentOS系统中，对其解压</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps1.jpg" alt="img" style="zoom:100%;"> <p>（3）解压在&#x2F;export&#x2F;server&#x2F;  </p><pre class="line-numbers language-none"><code class="language-none">tar xzf apache-flume-1.9.0-bin.tar.gz -C &#x2F;export&#x2F;server&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）添加软连接：</p><pre class="line-numbers language-none"><code class="language-none">ln -s apache-flume-1.9.0-bin flume<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）配置文件修改：</p><p> vim &#x2F;etc&#x2F;profile</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps2.jpg" alt="img" style="zoom:100%;"> <p>vi sqoop-env.sh</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps3.jpg" alt="img" style="zoom:100%;"> <p>（5）加入mysql的jdbc驱动包 cp &#x2F;export&#x2F;server&#x2F;hive&#x2F;lib&#x2F;mysql-connector-java-5.1.32.jar $SQOOP_HOME&#x2F;lib&#x2F;</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps4.jpg" alt="img" style="zoom:100%;"> <p>（7）验证启动</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;sqoop list-databases --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F; --username root --password hadoop <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>本命令会列出所有mysql的数据库。 到这里，整个Sqoop安装工作完成。</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps5.jpg" alt="img" style="zoom:100%;"> <h2 id="sqoop导入"><a href="#sqoop导入" class="headerlink" title="sqoop导入"></a><strong>sqoop导入</strong></h2><p>（1）Sqoop测试表数据 在mysql中创建数据库userdb ，然后执行并参考资料中的sql脚本。</p><p>创建三张表: emp 雇员表、 emp_add 雇员地址表、 emp_conn 雇员联系表。</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps6.jpg" alt="img" style="zoom:100%;">  <p><em><strong>（2）全量导入mysql表数据到HDFS</strong></em></p><p>下面的命令用于从MySQL数据库服务器中的emp表导入HDFS</p><pre class="line-numbers language-none"><code class="language-none">#example1-mysql-hdfs-start bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --delete-target-dir \ --target-dir &#x2F;sqoop&#x2F;sqoopresult \ --table emp --m 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps7.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">#example2-mysql-hdfs-terminated bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --target-dir &#x2F;sqoop&#x2F;sqoopresult2 \ --fields-terminated-by &#39;\t&#39; \ --table emp --m 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps8.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">#example3-mysql-hdfs-splitbin&#x2F;sqoop import \--connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \--username root \--password hadoop \--target-dir &#x2F;sqoop&#x2F;sqoopresult3 \--fields-terminated-by &#39;\t&#39; \--split-by id \--table emp --m 2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps9.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（3）全量导入mysql表数据到HIVE</strong></em></p><p>先复制表结构到hive中再导入数据，在hive中新建数据库sqoop_test用于测试。</p><pre class="line-numbers language-none"><code class="language-none">create database if not exists sqoop_test comment &quot;this is sqoop db&quot; with dbproperties(&#39;createdBy&#39;&#x3D;&#39;yzl&#39;); use sqoop_test; show tables; desc formatted emp_add_sp;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>将关系型数据的表结构复制到hive中</p><pre class="line-numbers language-none"><code class="language-none">#example4-1-mysql-hive-structure bin&#x2F;sqoop create-hive-table \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --table emp_add \ --username root \ --password hadoop \ --hive-table sqoop_test.emp_add_sp<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从关系数据库导入文件到hive中</p><pre class="line-numbers language-none"><code class="language-none">#example4-2-mysql-hive-data bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --table emp_add \ --hive-table sqoop_test.emp_add_sp \ --hive-import \ --m 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps10.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（4）导入表数据子集(where过滤)</strong></em></p><pre class="line-numbers language-none"><code class="language-none">#example6-mysql-hdfs-where bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --where &quot;city &#x3D;&#39;sec-bad&#39;&quot; \ --target-dir &#x2F;sqoop&#x2F;wherequery \ --table emp_add --m 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps11.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（5）导入表数据子集(query查询</strong></em></p><pre class="line-numbers language-none"><code class="language-none">#example7-mysql-hdfs-query bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --target-dir &#x2F;sqoop&#x2F;wherequery2 \ --query &#39;select id,name,deg from emp WHERE id&gt;1203 and $CONDITIONS&#39; \ --split-by id \ --fields-terminated-by &#39;\001&#39; \ --m 2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps12.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（6）增量导入</strong></em></p><p>Append模式增量导入，执行以下指令先将我们之前的数据导入。</p><pre class="line-numbers language-none"><code class="language-none">#example8-1-mysql-hdfs-append bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --target-dir &#x2F;sqoop&#x2F;appendresult \ --table emp --m 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用hdfs dfs -cat查看生成的数据文件，发现数据已经导入到hdfs中 然后在mysql的emp表中插入2条数据:</p><pre class="line-numbers language-none"><code class="language-none">insert into &#96;userdb&#96;.&#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;deg&#96;, &#96;salary&#96;, &#96;dept&#96;) values (&#39;1206&#39;, &#39;allen&#39;, &#39;admin&#39;, &#39;30000&#39;, &#39;tp&#39;); insert into &#96;userdb&#96;.&#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;deg&#96;, &#96;salary&#96;, &#96;dept&#96;) values (&#39;1207&#39;, &#39;woon&#39;, &#39;admin&#39;, &#39;40000&#39;, &#39;tp&#39;);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>执行如下的指令，实现增量的导入:</p><pre class="line-numbers language-none"><code class="language-none">#example8-2-mysql-hdfs-append bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --table emp --m 1 \ --target-dir &#x2F;sqoop&#x2F;appendresult \ --incremental append \ --check-column id \ --last-value 1205<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps13.jpg" alt="img" style="zoom:100%;"> <p><em><strong>Lastmodified模式增量导入</strong></em></p><p>（1）首先创建一个customer表，指定一个时间戳字段：</p><pre class="line-numbers language-none"><code class="language-none">create table customertest(id int,name varchar(20),last_mod timestamp default current_timestamp on update current_timestamp);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此处的时间戳设置为在数据的产生和更新时都会发生改变.</p><p>（2）插入如下记录:</p><pre class="line-numbers language-none"><code class="language-none">insert into customertest(id,name) values(1,&#39;neil&#39;); insert into customertest(id,name) values(2,&#39;jack&#39;); insert into customertest(id,name) values(3,&#39;martin&#39;); insert into customertest(id,name) values(4,&#39;tony&#39;); insert into customertest(id,name) values(5,&#39;eric&#39;);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）此时执行sqoop指令将数据导入hdfs:</p><pre class="line-numbers language-none"><code class="language-none">#example9-1-mysql-hdfs-Lastmodified bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --target-dir &#x2F;sqoop&#x2F;lastmodifiedresult \ --table customertest --m 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（4）再次插入一条数据进入customertest表</p><pre class="line-numbers language-none"><code class="language-none">insert into customertest(id,name) values(6,&#39;james&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（5）使用incremental的方式进行增量的导入:</p><pre class="line-numbers language-none"><code class="language-none">#example9-2-mysql-hdfs-Lastmodified bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --table customertest \ --target-dir &#x2F;sqoop&#x2F;lastmodifiedresult \ --check-column last_mod \ --incremental lastmodified \ --last-value &quot;2019-05-28 18:42:06&quot; \ --m 1 \ --append<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps14.jpg" alt="img" style="zoom:100%;"> <p><em><strong>Lastmodified模式:append、merge-key</strong></em></p><p>使用lastmodified模式进行增量处理要指定增量数据是以append模式(附加) 和merge-key(合并)模式添加 。</p><p>下面演示使用merge-by的模式进行增量更新 ：</p><p>（1）我们去更新 id为1的name字段。</p><pre class="line-numbers language-none"><code class="language-none">update customertest set name &#x3D; &#39;Neil&#39; where id &#x3D; 1;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>更新之后，这条数据的时间戳会更新为更新数据时的系统时间.</p><p>（2）执行如下指令，把id字段作为merge-key:</p><pre class="line-numbers language-none"><code class="language-none">#example10-mysql-hdfs-merge-key bin&#x2F;sqoop import \ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \ --username root \ --password hadoop \ --table customertest \ --target-dir &#x2F;sqoop&#x2F;lastmodifiedresult \ --check-column last_mod \ --incremental lastmodified \ --last-value &quot;2019-05-28 18:42:06&quot; \ --m 1 \ --merge-key id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps15.jpg" alt="img" style="zoom:100%;"><h2 id="sqoop导出"><a href="#sqoop导出" class="headerlink" title="sqoop导出"></a><strong>sqoop导出</strong></h2><p>（1）准备HDFS数据</p><p>在HDFS文件系统中“&#x2F;emp&#x2F;”目录的下创建一个文件emp_data.txt：mkdir &#x2F;export&#x2F;data&#x2F;sqoop-data&#x2F;emp&#x2F;</p><p>vim emp_data.txt 添加如下内容：</p><pre class="line-numbers language-none"><code class="language-none">1201,gopal,manager,50000,TP1202,manisha,preader,50000,TP1203,kalil,php dev,30000,AC1204,prasanth,php dev,30000,AC1205,kranthi,admin,20000,TP1206,satishp,grpdes,20000,GR<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps16.jpg" alt="img" style="zoom:100%;"> <p>上传至hdfs：</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;sqoop&#x2F;emp_datahadoop fs -put emp_data.txt &#x2F;sqoop&#x2F;emp_data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>（2）手动创建mysql中的目标表</p><pre class="line-numbers language-none"><code class="language-none">mysql&gt; use userdb;mysql&gt; create table employee ( id int not null primary key, name varchar(20), deg varchar(20),salary int,dept varchar(10));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps17.jpg" alt="img" style="zoom:100%;"> <p>（3）执行导出命令：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;sqoop export --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb --username root --password hadoop --table employee1 --columns id,name,deg,salary,dept --export-dir &#x2F;sqoop&#x2F;emp_data&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em><strong>更新导出（updateonly模式）</strong></em></p><p>（1）准备HDFS数据</p><p>在HDFS文件系统中&#x2F;sqoop&#x2F;updateonly_1&#x2F;目录的下创建一个文件updateonly_1.txt：</p><pre class="line-numbers language-none"><code class="language-none">1201,gopal,manager,500001202,manisha,preader,500001203,kalil,php dev,30000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps18.jpg" alt="img" style="zoom:100%;"><p>（2）手动创建mysql中的目标表</p><pre class="line-numbers language-none"><code class="language-none">mysql&gt; USE userdb;mysql&gt; CREATE TABLE updateonly ( id INT NOT NULL PRIMARY KEY,  name VARCHAR(20), deg VARCHAR(20),salary INT);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps19.jpg" alt="img" style="zoom:100%;"> <p>（3）先执行全部导出操作</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;sqoop export --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb --username root --password hadoop --table allowinsert --export-dir &#x2F;sqoop&#x2F;allowinsert_1&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看此时的数据</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps20.jpg" alt="img" style="zoom:100%;"><p>（4）新增文件</p><p>创建文件allowinsert_2.txt。修改前三条数据并且新增了一条记录。上传至 &#x2F;sqoop&#x2F;allowinsert_2&#x2F;目录下：</p><pre class="line-numbers language-none"><code class="language-none">1201,gopal,manager,12121202,manisha,preader,13131203,kalil,php dev,14141204,allen,java,1515<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps21.jpg" alt="img" style="zoom:100%;"> <p>2.6 执行更新导出</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;sqoop export \--connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \--username root --password hadoop \--table allowinsert \--export-dir &#x2F;sqoop&#x2F;allowinsert_2&#x2F; \--update-key id \--update-mode allowinsert<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看最终结果</p><img src="/wiki/sqoop%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA/wps22.png" alt="img" style="zoom:100%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]sqoop配置文档.md</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;sqoop安装&quot;&gt;&lt;a href=&quot;#sqoop安装&quot; class=&quot;headerlink&quot; title=&quot;sqoop安装&quot;&gt;&lt;/a&gt;&lt;strong&gt;sqoop安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）首先需要有java、mysql、hadoop和hive环境&lt;</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="sqoop" scheme="https://weichunxiu123.github.io/tags/sqoop/"/>
    
    <category term="导入" scheme="https://weichunxiu123.github.io/tags/%E5%AF%BC%E5%85%A5/"/>
    
    <category term="导出" scheme="https://weichunxiu123.github.io/tags/%E5%AF%BC%E5%87%BA/"/>
    
  </entry>
  
  <entry>
    <title>Fume测试以及综合案例</title>
    <link href="https://weichunxiu123.github.io/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/"/>
    <id>https://weichunxiu123.github.io/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/</id>
    <published>2023-06-14T08:03:25.939Z</published>
    <updated>2023-06-14T08:56:55.580Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Flume测试以及综合案例"><a href="#Flume测试以及综合案例" class="headerlink" title="Flume测试以及综合案例"></a><strong>Flume测试以及综合案例</strong></h2><h6 id="（1）拦截器"><a href="#（1）拦截器" class="headerlink" title="（1）拦截器"></a><strong>（1）拦截器</strong></h6><p>Host Interceptor拦截器将运行agent的hostname 或者 IP地址写入到事件的headers中。</p><p>1）在 &#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf 中添加example8-interceptor.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none"># 定义agent名称为a1# 设置3个组件的名称a1.sources &#x3D; r1a1.sinks &#x3D; k1a1.channels &#x3D; c1# 配置source类型为NetCat,监听地址为本机，端口为44444a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444# 配置拦截器为hosta1.sources.r1.interceptors &#x3D; i1 a1.sources.r1.interceptors.i1.type &#x3D; host# 配置sink类型为Loggera1.sinks.k1.type &#x3D; logger# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100# 将source和sink绑定到channel上a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps1.jpg" alt="img" style="zoom:100%;"> <p>2）、在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume（复制链接进入相同路径下，输入nc node1 44444输入内容进行测试，结果显示IP地址，测试成功）</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps2.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（2）Timestamp Interceptor拦截器</strong></em></p><p>将当前时间写入到事件的headers中。</p><p>1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-inteptor.conf中：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources.r1.interceptors &#x3D; i1 i2a1.sources.r1.interceptors.i1.type &#x3D; hosta1.sources.r1.interceptors.i2.type &#x3D; timestamp<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps3.jpg" alt="img" style="zoom:100%;"> <p>2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume</p><p>（与上一测试使用相同方法进行测试，结果显示timestanp，测试成功）</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps4.jpg" alt="img" style="zoom:100%;">  <p><em><strong>（2）Static Interceptor</strong></em></p><p>运行用户对所有的事件添加固定的header</p><p>1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-interceptor.conf中：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources.r1.interceptors &#x3D; i1 i2 i3a1.sources.r1.interceptors.i1.type &#x3D; hosta1.sources.r1.interceptors.i2.type &#x3D; timestampa1.sources.r1.interceptors.i3.type &#x3D; statica1.sources.r1.interceptors.i3.key &#x3D; datacentera1.sources.r1.interceptors.i3.value &#x3D; NEW_YORK<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps5.jpg" alt="img" style="zoom:100%;"> <p>2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：（结果显示datacenter，则测试成功）</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps6.jpg" alt="img" style="zoom:110%;"> <p><em><strong>（4）UUID Interceptor</strong></em></p><p>用于每个events header中生成一个UUID字符串，可以在sink中读取并使用</p><p>1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-interceptor.conf中：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources.r1.interceptors &#x3D; i1 i2 i3 i4a1.sources.r1.interceptors.i1.type &#x3D; hosta1.sources.r1.interceptors.i2.type &#x3D; timestampa1.sources.r1.interceptors.i3.type &#x3D; statica1.sources.r1.interceptors.i3.key &#x3D; datacentera1.sources.r1.interceptors.i3.value &#x3D; NEW_YORKa1.sources.r1.interceptors.i4.type &#x3D; org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps7.jpg" alt="img" style="zoom:100%;">  <p>2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：（显示字符串，则测试成功）</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps8.jpg" alt="img" style="zoom:100%;"><p><em><strong>（5）Search and Replace Interceptor</strong></em></p><p>用于将events中的正则匹配到的内容做相应的替换。</p><p>1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-interceptor.conf中：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources.r1.interceptors &#x3D; i1 i2 i3 i4 i5a1.sources.r1.interceptors.i1.type &#x3D; hosta1.sources.r1.interceptors.i2.type &#x3D; timestampa1.sources.r1.interceptors.i3.type &#x3D; statica1.sources.r1.interceptors.i3.key &#x3D; datacentera1.sources.r1.interceptors.i3.value &#x3D; NEW_YORKa1.sources.r1.interceptors.i4.type &#x3D; org.apache.flume.sink.solr.morphline.UUIDInterceptor$Buildera1.sources.r1.interceptors.i5.type &#x3D; search_replacea1.sources.r1.interceptors.i5.searchPattern &#x3D; \\d&#123;6&#125;a1.sources.r1.interceptors.i5.replaceString &#x3D; ******1234<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps9.jpg" alt="img" style="zoom:100%;"> <p>2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps10.jpg" alt="img" style="zoom:100%;"><p>3）复制node1链接，进入&#x2F;export&#x2F;server&#x2F;flume路径下，输入</p><pre class="line-numbers language-none"><code class="language-none">nc node1 44444<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps11.jpg" alt="img" style="zoom:90%;"> <p><em><strong>（6）自定义拦截器</strong></em></p><p>1）在ldea中添加自定义拦截器</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps12.jpg" alt="img" style="zoom:100%;"> <p>2）将项目打包复制到node1的&#x2F;export&#x2F;server&#x2F;flume&#x2F;lib路径下</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps13.jpg" alt="img" style="zoom:100%;"> <p>3）在node1、node2中添加上游服务器设置example9-1-taildir-f-avro-interceptor.conf</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; TAILDIRa1.sources.r1.channels &#x3D; c1a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.jsona1.sources.r1.filegroups &#x3D; g1a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;app&#x2F;event.*#提高吞吐量a1.sources.r1.batchSize &#x3D; 1000#动态的header-keys eg：filepath&#x3D;&#x2F;..&#x2F;..&#x2F;..&#x2F;a1.sources.r1.fileHeaderKey &#x3D; filepath#拦截器配置,添加自定义拦截器a1.sources.r1.interceptors &#x3D; i1a1.sources.r1.interceptors.i1.type &#x3D; ccjz.rgzn.flume.EventTimestampInterceptor$EventTimestampInterceptorBuildera1.sources.r1.interceptors.i1.tsFiledName &#x3D; timeStampa1.sources.r1.interceptors.i1.keyName &#x3D; timestampa1.sources.r1.interceptors.i1.toEncryFieldName &#x3D; accounta1.channels.c1.type &#x3D; file#本机数据汇集检查点、event存储目录a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpointa1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;dataa1.channels.c1.transactionCapacity &#x3D; 2000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; avroa1.sinks.k1.batch-size &#x3D; 1000#下游目标主机、端口a1.sinks.k1.hostname &#x3D; node3a1.sinks.k1.port &#x3D; 44444<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps14.jpg" alt="img" style="zoom:100%;"><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps15.jpg" alt="img" style="zoom:100%;"> <p>4）在node3中添加下游服务器配置example9-2-avro-f-hdfs-interceptor.conf</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1#下游数据汇集avro sourcea1.sources.r1.type &#x3D; avroa1.sources.r1.channels &#x3D; c1a1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444a1.sources.r1.threads &#x3D; 10a1.sources.r1.batchSize &#x3D; 1000a1.channels.c1.type &#x3D; filea1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpointa1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;dataa1.channels.c1.transactionCapacity &#x3D; 2000#hdfs sinka1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata-interceptor&#x2F;%Y-%m-%d&#x2F;%H&#x2F;#eg：文件名 logdata_34438hxfd.log，在滚动时，logdata_34438hxfd.log.tmpa1.sinks.k1.hdfs.filePrefix &#x3D; logdata_a1.sinks.k1.hdfs.fileSuffix &#x3D; .log#三个条件没有优先级，谁先达到就进行滚动#按时间间隔滚动a1.sinks.k1.hdfs.rollInterval &#x3D; 0#按文件大小滚动 256MBa1.sinks.k1.hdfs.rollSize &#x3D; 268435456#按event条数滚动a1.sinks.k1.hdfs.rollCount &#x3D; 100000a1.sinks.k1.hdfs.batchSize &#x3D; 1000a1.sinks.k1.hdfs.codeC &#x3D; gzipa1.sinks.k1.hdfs.fileType &#x3D; CompressedStream<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps16.jpg" alt="img" style="zoom:100%;"><p>5）在node1和node2中创建日志目录来生成模拟日志数据</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;app&#x2F;mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;app&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>6）在node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example9-2-avro-f-hdfs-interceptor.conf -Dflume.root.logger&#x3D;DEBUG,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps17.jpg" alt="img" style="zoom:110%;">  <p>7）启动node1和node2上的flume agent</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example9-1-taildir-f-avro-interceptor.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps18.jpg" alt="img" style="zoom:110%;"> <p>8）在hdfs上查看是否采集到数据</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps19.jpg" alt="img" style="zoom:110%;"> <p><em><strong>（7）Channel选择器</strong></em></p><p>Replicating Channel Selector中c2是一个可选的channel，写入c2失败的话会被忽略，c1没有标记为可选，如果写入c1失败会导致事务的失败</p><p> 1）在 &#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf 中添加example10-channel-replicating.conf，</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.sinks &#x3D; k1 k2a1.channels &#x3D; c1 c2a1.sources.r1.type &#x3D; execa1.sources.r1.channels &#x3D; c1 c2a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;logdata&#x2F;access.loga1.sources.r1.batchSize &#x3D; 1000a1.sources.r1.selector.type &#x3D; replicatinga1.sources.r1.selector.optional &#x3D; c2a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 1000a1.channels.c2.type &#x3D; memorya1.channels.c2.capacity &#x3D; 1000a1.channels.c2.transactionCapacity &#x3D; 1000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata_c1&#x2F;%Y-%m-%d&#x2F;%H&#x2F;a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_a1.sinks.k1.hdfs.fileSuffix &#x3D; .loga1.sinks.k1.hdfs.rollInterval &#x3D; 0a1.sinks.k1.hdfs.rollSize &#x3D; 268435456a1.sinks.k1.hdfs.rollCount &#x3D; 0a1.sinks.k1.hdfs.batchSize &#x3D; 1000a1.sinks.k1.hdfs.fileType &#x3D; DataStreama1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; truea1.sinks.k2.channel &#x3D; c2a1.sinks.k2.type &#x3D; hdfsa1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata_c2&#x2F;%Y-%m-%d&#x2F;%H&#x2F;a1.sinks.k2.hdfs.filePrefix &#x3D; logdata_a1.sinks.k2.hdfs.fileSuffix &#x3D; .loga1.sinks.k2.hdfs.rollInterval &#x3D; 0a1.sinks.k2.hdfs.rollSize &#x3D; 268435456a1.sinks.k2.hdfs.rollCount &#x3D; 0a1.sinks.k2.hdfs.batchSize &#x3D; 1000a1.sinks.k2.hdfs.fileType &#x3D; DataStreama1.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps20.jpg" alt="img" style="zoom:100%;"> <p>2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><p>bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example10-channel-replica</p><p>ting.conf -Dflume.root.logger&#x3D;INFO,console</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps21.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（8）Multiplexing Channel Selector</strong></em></p><p>可以根据event中的一个指定key的value来决定这条消息会写入哪个channel。</p><p>1）在 &#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf 中添加example11-channel-Multiplexing.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1 c2a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; TAILDIRa1.sources.r1.channels &#x3D; c1 c2a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.jsona1.sources.r1.filegroups &#x3D; g1 g2a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web.*a1.sources.r1.filegroups.g2 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx.*a1.sources.r1.headers.g1.logtype &#x3D; weba1.sources.r1.headers.g2.logtype &#x3D; wxa1.sources.r1.selector.type &#x3D; multiplexinga1.sources.r1.selector.header &#x3D; logtypea1.sources.r1.selector.mapping.web &#x3D; c1a1.sources.r1.selector.mapping.wx &#x3D; c2a1.sources.r1.selector.default &#x3D; c2a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 1000a1.channels.c2.type &#x3D; memorya1.channels.c2.capacity &#x3D; 1000a1.channels.c2.transactionCapacity &#x3D; 1000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;%&#123;logtype&#125;&#x2F;%Y-%m-%d&#x2F;%H&#x2F;a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_a1.sinks.k1.hdfs.fileSuffix &#x3D; .loga1.sinks.k1.hdfs.rollInterval &#x3D; 0a1.sinks.k1.hdfs.rollSize &#x3D; 268435456a1.sinks.k1.hdfs.rollCount &#x3D; 0a1.sinks.k1.hdfs.batchSize &#x3D; 1000a1.sinks.k1.hdfs.fileType &#x3D; DataStreama1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; truea1.sinks.k2.channel &#x3D; c2a1.sinks.k2.type &#x3D; hdfsa1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;%&#123;logtype&#125;&#x2F;%Y-%m-%d&#x2F;%H&#x2F;a1.sinks.k2.hdfs.filePrefix &#x3D; logdata_a1.sinks.k2.hdfs.fileSuffix &#x3D; .loga1.sinks.k2.hdfs.rollInterval &#x3D; 0a1.sinks.k2.hdfs.rollSize &#x3D; 268435456a1.sinks.k2.hdfs.rollCount &#x3D; 0a1.sinks.k2.hdfs.batchSize &#x3D; 1000a1.sinks.k2.hdfs.fileType &#x3D; DataStreama1.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps22.jpg" alt="img" style="zoom:110%;"> <p>2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example11-channel-Multiplexing.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps23.jpg" alt="img" style="zoom:100%;"> <p>3）在hdfs上查看</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps24.jpg" alt="img" style="zoom:110%;"><p><em><strong>（9）Sink处理器实例</strong></em></p><p>Failover Sink Processor是一组中只有优先级高的那个sink在工作，另一个是等待中，如果高优先级的sink发送数据失败，则专用低优先级的sink去工作，并且，在配置时间penalty之后，还会尝试用高优先级的去发送数据</p><p>1）在node1上配置上游服务器example12-1-sink-failover.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; execa1.sources.r1.channels &#x3D; c1a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;logdata&#x2F;access.loga1.sources.r1.batchSize &#x3D; 1000a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 1000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; node2a1.sinks.k1.port &#x3D; 44444a1.sinks.k1.batch-size &#x3D; 1000a1.sinks.k2.channel &#x3D; c1a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; node3a1.sinks.k2.port &#x3D; 44444a1.sinks.k2.batch-size &#x3D; 1000a1.sinkgroups &#x3D; g1a1.sinkgroups.g1.sinks &#x3D; k1 k2a1.sinkgroups.g1.processor.type &#x3D; failovera1.sinkgroups.g1.processor.priority.k1 &#x3D; 200a1.sinkgroups.g1.processor.priority.k2 &#x3D; 100a1.sinkgroups.g1.processor.maxpenalty &#x3D; 5000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps25.jpg" alt="img" style="zoom:100%;">  <p>2）在node2、node3上配置下游服务器example12-2-sink-failover.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.channels &#x3D; c1a1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444a1.sources.r1.threads &#x3D; 10a1.sources.r1.batchSize &#x3D; 1000a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps26.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps27.jpg" alt="img" style="zoom:100%;"> <p>3）在node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example12-2-sink-failover.conf -Dflume.root.logger&#x3D;DEBUG,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps28.jpg" alt="img" style="zoom:100%;"> <p>4）启动node1和node2上的flume agent</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example12-1-sink-failover.Conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em><strong>（10）Load balancing Sink Processor</strong></em></p><p>允许channel中的数据在一组sink中的多个sink之间进行交替</p><p>1）在node1上配置上游服务器example13-1-sink-loadbalance.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; execa1.sources.r1.channels &#x3D; c1a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;logdata&#x2F;access.loga1.sources.r1.batchSize &#x3D; 1000a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 1000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; node2a1.sinks.k1.port &#x3D; 44444a1.sinks.k1.batch-size &#x3D; 1000a1.sinks.k2.channel &#x3D; c1a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; node3a1.sinks.k2.port &#x3D; 44444a1.sinks.k2.batch-size &#x3D; 1000a1.sinkgroups &#x3D; g1a1.sinkgroups.g1.sinks &#x3D; k1 k2a1.sinkgroups.g1.processor.type &#x3D; load_balancea1.sinkgroups.g1.processor.backoff &#x3D; true a1.sinkgroups.g1.processor.selector &#x3D; round_robin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps29.jpg" alt="img" style="zoom:100%;"> <p>2）在node2、node3上配置下游服务器example13-2-sink-loadbalance.conf内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.channels &#x3D; c1a1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444a1.sources.r1.threads &#x3D; 10a1.sources.r1.batchSize &#x3D; 1000a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps30.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps31.jpg" alt="img" style="zoom:100%;"> <p>3）在node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example13-2-sink-loadbalance.conf -Dflume.root.logger&#x3D;DEBUG,console<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps32.jpg" alt="img" style="zoom:100%;"> <p>4）启动node1和node2上的flume agent</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example13-1-sink-loadbalance.conf<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps33.jpg" alt="img" style="zoom:100%;"> <h6 id="（11）Flume综合实战案例"><a href="#（11）Flume综合实战案例" class="headerlink" title="（11）Flume综合实战案例"></a><em><strong>（11）Flume综合实战案例</strong></em></h6><p>1）准备一个MySQL服务器，创建一个库：realtimedw，字符集选择utf8，防止中文乱码：</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps34.jpg" alt="img" style="zoom:90%;"> <p>2）将事先准备好的realtimedw.sql、t_md_areas.sql两个sql文件导入到realtimedw库中：</p><p>3）在node1中创建一个目录，用来存放配置文件：</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps35.jpg" alt="img" style="zoom:100%;"> <p>4）修改log_gen_app.jar、log_gen_wx.jar的配置文件中的other.properties,内容如下：</p><pre class="line-numbers language-none"><code class="language-none">#logger,kafkasink.type&#x3D;logger#roll console dayrolllogger.type&#x3D;dayrollinitdata.releasechannel&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;releasechannel.txtinitdata.phoneinfo&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;phoneinfo.txtinitdata.eventIds&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;eventIds.txtinit.user.area&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;area.txtdb.url&#x3D;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;realtimedw?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf8&amp;useSSL&#x3D;falsedb.user&#x3D;rootdb.password&#x3D;hadoop# max concurrent accessor amountonline.max.num&#x3D;1000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5）log_gen_app.jar的配置文件log4j.properties，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">log4j.rootLogger&#x3D;INFO,tracelog4j.appender.trace&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.trace.Threshold&#x3D;DEBUGlog4j.appender.trace.ImmediateFlush&#x3D;truelog4j.appender.trace.Target&#x3D;System.outlog4j.appender.trace.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.trace.layout.ConversionPattern&#x3D;[%-5p] %d(%r) --&gt; [%t] %l: %m %x %nlog4j.logger.console &#x3D; INFO,consolelog4j.additivity.console&#x3D;falselog4j.appender.console&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.console.Threshold&#x3D;DEBUGlog4j.appender.console.ImmediateFlush&#x3D;truelog4j.appender.console.Target&#x3D;System.outlog4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern&#x3D;%m%n# log4j.logger.roll &#x3D; INFO,rollingFile# log4j.additivity.roll&#x3D;false# log4j.appender.rollingFile&#x3D;org.apache.log4j.RollingFileAppender# log4j.appender.rollingFile.Threshold&#x3D;DEBUG# log4j.appender.rollingFile.ImmediateFlush&#x3D;true# log4j.appender.rollingFile.Append&#x3D;true# log4j.appender.rollingFile.File&#x3D;&#x2F;loggen&#x2F;logdata&#x2F;wx&#x2F;event.log# log4j.appender.rollingFile.MaxFileSize&#x3D;120MB# log4j.appender.rollingFile.MaxBackupIndex&#x3D;50# log4j.appender.rollingFile.layout&#x3D;org.apache.log4j.PatternLayout# log4j.appender.rollingFile.layout.ConversionPattern&#x3D;%m%nlog4j.logger.dayroll &#x3D; INFO,DailyRollinglog4j.additivity.dayroll&#x3D;falselog4j.appender.DailyRolling&#x3D;org.apache.log4j.DailyRollingFileAppenderlog4j.appender.DailyRolling.File&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;gen_logdata&#x2F;event_log_applog4j.appender.DailyRolling.DatePattern&#x3D;yyyy-MM-dd&#39;.log&#39;log4j.appender.DailyRolling.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.DailyRolling.layout.ConversionPattern&#x3D;%m%n<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps36.jpg" alt="img" style="zoom:100%;"> <p>6）修改log_gen_wx.jar的配置文件log4j.properties</p><pre class="line-numbers language-none"><code class="language-none">log4j.rootLogger&#x3D;INFO,tracelog4j.appender.trace&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.trace.Threshold&#x3D;DEBUGlog4j.appender.trace.ImmediateFlush&#x3D;truelog4j.appender.trace.Target&#x3D;System.outlog4j.appender.trace.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.trace.layout.ConversionPattern&#x3D;[%-5p] %d(%r) --&gt; [%t] %l: %m %x %nlog4j.logger.console &#x3D; INFO,consolelog4j.additivity.console&#x3D;falselog4j.appender.console&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.console.Threshold&#x3D;DEBUGlog4j.appender.console.ImmediateFlush&#x3D;truelog4j.appender.console.Target&#x3D;System.outlog4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern&#x3D;%m%n# log4j.logger.roll &#x3D; INFO,rollingFile# log4j.additivity.roll&#x3D;false# log4j.appender.rollingFile&#x3D;org.apache.log4j.RollingFileAppender# log4j.appender.rollingFile.Threshold&#x3D;DEBUG\# log4j.appender.rollingFile.ImmediateFlush&#x3D;true# log4j.appender.rollingFile.Append&#x3D;true# log4j.appender.rollingFile.File&#x3D;&#x2F;loggen&#x2F;logdata&#x2F;wx&#x2F;event.log# log4j.appender.rollingFile.MaxFileSize&#x3D;120MB# log4j.appender.rollingFile.MaxBackupIndex&#x3D;50# log4j.appender.rollingFile.layout&#x3D;org.apache.log4j.PatternLayout# log4j.appender.rollingFile.layout.ConversionPattern&#x3D;%m%nlog4j.logger.dayroll &#x3D; INFO,DailyRollinglog4j.additivity.dayroll&#x3D;falselog4j.appender.DailyRolling&#x3D;org.apache.log4j.DailyRollingFileAppenderlog4j.appender.DailyRolling.File&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;gen_logdata&#x2F;event_log_wxlog4j.appender.DailyRolling.DatePattern&#x3D;yyyy-MM-dd&#39;.log&#39;log4j.appender.DailyRolling.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.DailyRolling.layout.ConversionPattern&#x3D;%m%n<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）将以下文件上传到&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit路径下：</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps37.jpg" alt="img" style="zoom:110%;"> <p>8）给shell文件授权，并启动日志生成器：</p><pre class="line-numbers language-none"><code class="language-none">chmod +x genapplog.shchmod +x genwxlog.shsh genapplog.shsh genwxlog.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>9）查看jps</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps38.jpg" alt="img" style="zoom:100%;"> <p>10）进入&#x2F;export&#x2F;data&#x2F;flume-example&#x2F;gen_logdata下查看日志文件生成效果：</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps39.jpg" alt="img" style="zoom:100%;"> <p>11）在node1、node2中配置上游服务器example14-1-Comprehensive-practical.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; TAILDIRa1.sources.r1.channels &#x3D; c1a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.jsona1.sources.r1.filegroups &#x3D; g1a1.sources.r1.filegroups.g1 &#x3D;  &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;gen_logdata&#x2F;event_.*a1.sources.r1.batchSize &#x3D; 1000a1.sources.r1.interceptors &#x3D; i1 i2 i3a1.sources.r1.interceptors.i1.type &#x3D; ccjz.rgzn.flume.EncryptSpecifiedFieldInterceptor$EncryptInterceptorBuildera1.sources.r1.interceptors.i1.toEncryFieldName &#x3D; accounta1.sources.r1.interceptors.i2.type &#x3D; ccjz.rgzn.flume.EventTimeStampExtractInterceptor$EventTimestampInterceptorBuildera1.sources.r1.interceptors.i2.tsFiledName &#x3D; timeStampa1.sources.r1.interceptors.i2.keyName &#x3D; timestamp\#拥有openid的是wx小程序用户日志a1.sources.r1.interceptors.i3.type &#x3D; ccjz.rgzn.flume.LogTypeInterceptor$LogTypeInterceptorBuildera1.sources.r1.interceptors.i3.flag.fieldname &#x3D; openida1.sources.r1.interceptors.i3.headerKey &#x3D; logtypea1.channels.c1.type &#x3D; filea1.channels.c1.checkpointDir &#x3D;  &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpointa1.channels.c1.dataDirs &#x3D;  &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;dataa1.channels.c1.transactionCapacity &#x3D; 2000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; node2a1.sinks.k1.port &#x3D; 44444a1.sinks.k1.batch-size &#x3D; 1000a1.sinks.k2.channel &#x3D; c1a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; node3a1.sinks.k2.port &#x3D; 44444a1.sinks.k2.batch-size &#x3D; 1000a1.sinkgroups &#x3D; g1a1.sinkgroups.g1.sinks &#x3D; k1 k2a1.sinkgroups.g1.processor.type &#x3D; failovera1.sinkgroups.g1.processor.priority.k1 &#x3D; 200a1.sinkgroups.g1.processor.priority.k2 &#x3D; 100a1.sinkgroups.g1.processor.maxpenalty &#x3D; 5000<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps40.jpg" alt="img" style="zoom:110%;"> <p>12）在node2、node3中配置下游服务器example14-2-Comprehensive-practical.conf，内容如下：</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.channels &#x3D; c1a1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444a1.sources.r1.threads &#x3D; 10a1.sources.r1.batchSize &#x3D; 1000a1.channels.c1.type &#x3D; filea1.channels.c1.checkpointDir &#x3D;  &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata_2&#x2F;checkpointa1.channels.c1.dataDirs &#x3D;  &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata_2&#x2F;dataa1.channels.c1.transactionCapacity &#x3D; 2000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;gen_logdata&#x2F;%&#123;logtype&#125;&#x2F;%Y-%m-%d&#x2F;a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_a1.sinks.k1.hdfs.fileSuffix &#x3D; .loga1.sinks.k1.hdfs.rollInterval &#x3D; 300a1.sinks.k1.hdfs.rollSize &#x3D; 268435456a1.sinks.k1.hdfs.rollCount &#x3D; 0a1.sinks.k1.hdfs.batchSize &#x3D; 1000a1.sinks.k1.hdfs.codeC &#x3D; gzipa1.sinks.k1.hdfs.fileType &#x3D; CompressedStream<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps41.jpg" alt="img" style="zoom:110%;"> <p>13）将node1、node2、node3机器上之前的一些flumedata目录清除；</p><p>14）在node1、node2中创建日志数据目录，在此目录下创建log1、log2、log3文件夹。</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;data&#x2F;flume-example&#x2F;weblog<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>15）在node2、node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example14-2-Comprehensive-practical.conf -Dflume.root.logger&#x3D;DEBUG,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>16）在&#x2F;export&#x2F;data&#x2F;flume-example&#x2F;weblog路径下，</p><p>输入for i in {i..10000}模拟往3类日志中写入日志：</p><pre class="line-numbers language-none"><code class="language-none">&gt;do&gt;echo $&#123;i&#125;-access,1389999,asdb,ccc,ddd,eee,fff &gt;&gt; log1&#x2F;access.log&gt;echo $&#123;i&#125;-nginx,1389999,asdb,ccc,ddd,eee,fff &gt;&gt; log2&#x2F;nginx.log&gt;echo $&#123;i&#125;-weblog,1389999,asdb,ccc,ddd,eee,fff &gt;&gt; log3&#x2F;weblog.log&gt;sleep 0.2&gt;done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps42.jpg" alt="img"> </p><p>17）在node1中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example14-1-Comprehensive-practical.conf -Dflume.root.logger&#x3D;DEBUG,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>18）在hdfs上查看结果，数据上传成功</p><img src="/wiki/flume%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/wps43.jpg" alt="img" style="zoom:110%;"> <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]Flume安装测试文档.md</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Flume测试以及综合案例&quot;&gt;&lt;a href=&quot;#Flume测试以及综合案例&quot; class=&quot;headerlink&quot; title=&quot;Flume测试以及综合案例&quot;&gt;&lt;/a&gt;&lt;strong&gt;Flume测试以及综合案例&lt;/strong&gt;&lt;/h2&gt;&lt;h6 id=&quot;（1）拦</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="Flume" scheme="https://weichunxiu123.github.io/tags/Flume/"/>
    
    <category term="测试" scheme="https://weichunxiu123.github.io/tags/%E6%B5%8B%E8%AF%95/"/>
    
    <category term="案例" scheme="https://weichunxiu123.github.io/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>Fume安装及测试</title>
    <link href="https://weichunxiu123.github.io/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/"/>
    <id>https://weichunxiu123.github.io/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/</id>
    <published>2023-06-14T06:19:03.519Z</published>
    <updated>2023-06-14T07:46:23.835Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Flume安装"><a href="#一、Flume安装" class="headerlink" title="一、Flume安装"></a>一、<strong>Flume安装</strong></h2><p>1、Flume安装</p><p>（1）上传flume的压缩包到&#x2F;export&#x2F;software下。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps1.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps2.jpg" alt="img" style="zoom:100%;"> <p>（2）解压到&#x2F;export&#x2F;server目录下。</p><pre class="line-numbers language-none"><code class="language-none">tar -zxvf apache-flume-1.9.0-bin.tar.gz -C &#x2F;export&#x2F;server&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>同时设置软链接：</p><pre class="line-numbers language-none"><code class="language-none">ln -s &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin&#x2F; flume<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如下图所示：</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps3.jpg" alt="img" style="zoom:100%;"> <p>（3）编辑&#x2F;etc&#x2F;profile，配置FLUME_HOME指向正确的路径</p><pre class="line-numbers language-none"><code class="language-none">#FLUME_HOMEexport FLUME_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;flumeexport PATH&#x3D;$PATH:$FLUME_HOME&#x2F;binsource &#x2F;etc&#x2F;profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps4.jpg" alt="img" style="zoom:100%;"> <p>（4）通过scp命令发送给其余机器，并设置软链接。</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin root@node1:&#x2F;export&#x2F;server&#x2F;scp -r &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin root@node3:&#x2F;export&#x2F;server&#x2F;ln -s &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin &#x2F;export&#x2F;server&#x2F;flume<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（5）添加配置文件</p><p>在flume&#x2F;myconf目录下添加配置文件netcat-logger.conf，添加如下内容：</p><pre class="line-numbers language-none"><code class="language-none"># example1-netcat-logger.conf: 单节点Flume配置\# 定义agent名称为a1\# 设置3个组件的名称a1.sources &#x3D; r1a1.sinks &#x3D; k1a1.channels &#x3D; c1\# 配置source类型为NetCat,监听地址为本机，端口为44444a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444\#source和channel关联a1.sources.r1.channels &#x3D; c1\# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100\# 配置sink类型为Loggera1.sinks.k1.type &#x3D; logger\# 将sink绑定到channel上a1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（6）启动flume</p><p>在&#x2F;export&#x2F;server&#x2F;flume目录下：</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example1-netcat-logger.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps5.jpg" alt="img" style="zoom:100%;"> <p>使用Netcat测试，复制node1标签，启动netcat连接到44444端口</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps6.jpg" alt="img" style="zoom:100%;"> <p>可以看到agent控制台接收到信息。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps7.jpg" alt="img" style="zoom:100%;"> <h2 id="二、Flume测试"><a href="#二、Flume测试" class="headerlink" title="二、Flume测试"></a><strong>二、Flume测试</strong></h2><p><em><strong>（1）exec_source测试</strong></em></p><p>使用Flume从日志文件中将日志收集到日志中间，以便于查找和分析。启动测试流程为：</p><p>1）准备日志文件 </p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）写一个脚本模拟向日志文件中持续写入数据，</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;shell；vim access_data.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps8.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">for i in &#123;1..10000&#125;；do echo $&#123;i&#125; “bigdata log 5&#x2F;11” &gt;&gt; access.log;sleep 0.5;done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>3）创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps9.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none"># example2-exec-source-logger.confa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; execa1.sources.r1.channels &#x3D; c1a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;shell&#x2F;access.log a1.sources.r1.batchSize &#x3D; 100a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100a1.sinks.k1.type &#x3D; loggera1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）启动flume</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example2-exec-source-logger.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps10.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（2）spooldir_source测试</strong></em></p><p>监听一个指定的文件夹，如果文件夹下有没有采集过的新文件，则会采集新文件中的数据。监听测试流程为：</p><p>1）创建spooldir监听目录。</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;spooldir<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2）在spooldir下新建文件。</p><pre class="line-numbers language-none"><code class="language-none">mkdir flume511.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps11.jpg" alt="img" style="zoom:100%;"> <p>3）创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps12.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">#example3-spooldir-source.confa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.channels &#x3D; c1a1.sources.r1.type &#x3D; spooldira1.sources.r1.spoolDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;spooldira1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100a1.sinks.k1.type &#x3D; loggera1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）启动flume</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example3-spooldir-source.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps13.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（3）taildir_source测试</strong></em></p><p>监听指定目录下的文件，只要文件中有新写入的行，就会被tail到，它会记录每一个文件所tail到的位置，记录到一个指定的positionfile保存目录中，格式为json。保证数据不会漏采（丢失）。 测试流程如下：</p><p>1）新建监听目录  </p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web-access.logmkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web-access.log.1mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx-access.log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>2）生成测试数据</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps14.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">for i in &#123;1..1000&#125;doecho &quot;webweb111 $RANDOM&quot; &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;&#x2F;web-access.logecho &quot;webweb222 $RANDOM&quot; &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;log&#x2F;web-access.log.1echo &quot;wxwxwx333 $RANDOM&quot; &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx-access.logsleep 0.1done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>3）创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps15.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">#example4-taildir-source.confa1.sources &#x3D; r1a1.sinks &#x3D; k1a1.channels &#x3D; c1a1.sources.r1.type &#x3D; TAILDIRa1.sources.r1.channels &#x3D; c1a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.jsona1.sources.r1.filegroups &#x3D; g1 g2a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web.*a1.sources.r1.filegroups.g2 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx.*a1.sources.r1.fileHeader &#x3D; true\#动态的header-keys eg：filepath&#x3D;&#x2F;..&#x2F;..&#x2F;..&#x2F;a1.sources.r1.fileHeaderKey &#x3D; filepath\#写死的header-keys（静态的） eg:a1 &#x3D; aa1a1.sources.r1.headers.g1.a1 &#x3D; aa1a1.sources.r1.headers.g1.b1 &#x3D; bb1a1.sources.r1.headers.g2.a2 &#x3D; aa2a1.sources.r1.headers.g2.b2 &#x3D; bb2a1.sources.r1.maxBatchCount &#x3D; 1000a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 10000a1.channels.c1.transactionCapacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）启动flume</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f  myconf&#x2F;example4-taildir-source.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps16.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（4）avro source</strong>测试</em></p><p>avro source 是通过监听一个网络端口来接收数据，被接收数据必须是使用avro序列化框架序列化后的数据。</p><p>工作机制：启动一个网络服务，监听一个端口，收集端口上收到的avro序列化数据流。</p><p>测试流程如下：</p><p>1）自定义创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps17.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">#example5-avro-source.confa1.sources &#x3D; r1a1.sources.r1.type &#x3D; avroa1.sources.r1.channels &#x3D; c1a1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 4141a1.channels &#x3D; c1a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 200a1.channels.c1.transactionCapacity &#x3D; 100a1.sinks &#x3D; k1a1.sinks.k1.type &#x3D; loggera1.sinks.k1.channel &#x3D; c1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）启动agent</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -c conf -f  myconf&#x2F;example5-avro-source.conf -n a1 -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  注意：在node1的&#x2F;export&#x2F;server&#x2F;flume目录下执行。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps18.jpg" alt="img" style="zoom:100%;"> <p>3）新建avro-log.txt，用一个客户端去给启动好的source发送avro序列化数据</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;avro-log.txt  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps19.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng avro-client --host node1  --port 4141  -f &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;avro-log.txt  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意：复制标签在新窗口执行！！！ </p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps20.jpg" alt="img" style="zoom:100%;"> <p>最后在agent控制台可以看到avro-log.txt内的内容。</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps21.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（5）使用File Channel实现数据持久化</strong></em></p><p>测试流程如下：</p><p>1）自定义flume配置文件，并放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf中</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps22.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">#example6-file-channel.conf\# 定义agent名称为a1\# 设置3个组件的名称a1.sources &#x3D; r1a1.sinks &#x3D; k1\# 多个channel使用空格分隔a1.channels &#x3D; c1 c2\# 配置source类型为NetCat,监听地址为本机，端口为44444a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444\# 配置sink类型为Loggera1.sinks.k1.type &#x3D; logger\# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c1.transactionCapacity &#x3D; 100\# 配置FileChannel,checkpointDir为检查点文件存储目录，dataDirs为日志数据存储目录，a1.channels.c2.type &#x3D; filea1.channels.c2.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint_filechannela1.channels.c2.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data_filechannel\# 将source和sink绑定到channel上\# source同时绑定到c1和c2上a1.sources.r1.channels &#x3D; c1 c2a1.sinks.k1.channel &#x3D; c1 | c2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2）为了方便日志打印，可以将-Dflume.root.logger&#x3D;INFO,console添加在conf的环境配置中。</p><pre class="line-numbers language-none"><code class="language-none">cp flume-env.sh.template flume-env.shvi flume-env.sh# 添加JAVA_OPTSexport JAVA_OPTS&#x3D;&quot;-Dflume.root.logger&#x3D;INFO,console&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps23.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps24.jpg" alt="img" style="zoom:100%;"> <p>2）启动flume</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example6-file-channel.conf -Dflume.root.logger&#x3D;INFO,console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3）通过Netcat发送数据，发送到c2的数据被消费</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps25.jpg" alt="img" style="zoom:100%;"><p>agent控制台接收数据成功</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps26.jpg" alt="img" style="zoom:100%;"> <p><em><strong>（6）利用avro source和avro sink实现agent级联</strong></em></p><p>（1）基本介绍：可以将多个Flume agent 程序连接在一起，其中一个agent的sink将数据发送到另一个agent的source。从多个Web服务器收集日志，发送到一个或多个集中处理的agent，之后再发往日志存储中心。</p><p>（2）测试流程如下：</p><p>\1) 启动hdfs，并检查工作状态。node1上有DataNode,NodeManager,Namenode,ResourceManager。node2上有NodeManager,SecondaryNameNode,DataNode。node3上有NodeManager,DataNode。</p><p>\2) 配置上游配置文件，保存到node1,node2上。</p><p>\3) 配置下游配置文件，保存到node3上。</p><p>\4) 启动下游node3上的flume agent。</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example7-2-avro-f-hdfs.conf -Dflume.root.logger&#x3D;DEBUG，console<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>\5) 在node1和node2上准备两个日志目录来生成模拟日志数据。</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>\6) 在node1和node2上利用shell脚本生成日志数据。</p><pre class="line-numbers language-none"><code class="language-none">vim avro-hdfs.shwhile truedoecho webwebwebwebweb &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web-access.logecho wxwxwxwxwxwxwx &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx-access.logsleep 0.01Done<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）启动node1和node2上的flume agent</p><pre class="line-numbers language-none"><code class="language-none">nohup bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example7-1-taildir-f-avro.conf -Dflume.root.logger&#x3D;INFO,console 1&gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>8）上游配置文件：</p><p>#上游服务器配置 example7-1-taildir-f-avro.conf</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; TAILDIRa1.sources.r1.channels &#x3D; c1a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.jsona1.sources.r1.filegroups &#x3D; g1 g2a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web.*a1.sources.r1.filegroups.g2 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx.*\#提高吞吐量a1.sources.r1.batchSize &#x3D; 1000\#动态的header-keys eg：filepath&#x3D;&#x2F;..&#x2F;..&#x2F;..&#x2F;a1.sources.r1.fileHeaderKey &#x3D; filepath\#拦截器配置，添加header&#x3D;timestampa1.sources.r1.interceptors &#x3D; i1a1.sources.r1.interceptors.i1.type &#x3D; timestampa1.sources.r1.interceptors.i1.headerName &#x3D; timestampa1.channels.c1.type &#x3D; file\#本机数据汇集检查点、event存储目录a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpointa1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;dataa1.channels.c1.transactionCapacity &#x3D; 2000a1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; avroa1.sinks.k1.batch-size &#x3D; 1000\#下游目标主机、端口a1.sinks.k1.hostname &#x3D; node3a1.sinks.k1.port &#x3D; 44444<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>9）下游配置文件：</p><p>#下游服务器配置 example7-2-avro-f-hdfs.conf</p><pre class="line-numbers language-none"><code class="language-none">a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1#下游数据汇集avro sourcea1.sources.r1.type &#x3D; avroa1.sources.r1.channels &#x3D; c1a1.sources.r1.bind &#x3D; 0.0.0.0a1.sources.r1.port &#x3D; 44444a1.sources.r1.threads &#x3D; 10a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; filea1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpointa1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;dataa1.channels.c1.transactionCapacity &#x3D; 2000 #hdfs sinka1.sinks.k1.channel &#x3D; c1a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata&#x2F;%Y-%m-%d&#x2F;%H&#x2F;#eg：文件名 logdata_34438hxfd.log，在滚动时，logdata_34438hxfd.log.tmpa1.sinks.k1.hdfs.filePrefix &#x3D; logdata_a1.sinks.k1.hdfs.fileSuffix &#x3D; .log#三个条件没有优先级，谁先达到就进行滚动#按时间间隔滚动a1.sinks.k1.hdfs.rollInterval &#x3D; 0#按文件大小滚动 256MBa1.sinks.k1.hdfs.rollSize &#x3D; 268435456#按event条数滚动a1.sinks.k1.hdfs.rollCount &#x3D; 100000a1.sinks.k1.hdfs.batchSize &#x3D; 1000a1.sinks.k1.hdfs.codeC &#x3D; gzipa1.sinks.k1.hdfs.fileType &#x3D; CompressedStream<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>先在下游启动flume agent 。下游的agent的sink将数据发送到上游agent的source。</p><p>启动下游的flume agent</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps27.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps28.jpg" alt="img" style="zoom:90%;"> <img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps29.jpg" alt="img" style="zoom:100%;"><p>到hdfs上查看结果</p><img src="/wiki/flume%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/wps30.jpg" alt="img" style="zoom:95%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]Flume安装配置文档.md</li><li>[2]<a href="https://skykip.github.io/2022/04/10/hello-world/">https://skykip.github.io/2022/04/10/hello-world/</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、Flume安装&quot;&gt;&lt;a href=&quot;#一、Flume安装&quot; class=&quot;headerlink&quot; title=&quot;一、Flume安装&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;Flume安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;1、Flume安装&lt;/p&gt;
&lt;p&gt;（1）上传flu</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="Flume" scheme="https://weichunxiu123.github.io/tags/Flume/"/>
    
    <category term="安装及测试" scheme="https://weichunxiu123.github.io/tags/%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Spark-pyspark基础编码环境</title>
    <link href="https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/"/>
    <id>https://weichunxiu123.github.io/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/</id>
    <published>2023-06-08T03:05:04.757Z</published>
    <updated>2023-06-09T00:29:10.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark（pyspark基础编码环境）"><a href="#Spark（pyspark基础编码环境）" class="headerlink" title="Spark（pyspark基础编码环境）"></a><strong>Spark（pyspark基础编码环境）</strong></h2><p>（一）、pyspark环境配置安装。</p><p>PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。</p><pre class="line-numbers language-none"><code class="language-none">前情提示：（1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。（2）将文件夹内bin内的Hadoop.dll复制到C:\Windows\Systmctl32里面去。（3）在系统环境变量中配置HADOOP_HOME，指向hadoop-3.3.0文件夹的路径。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps46.jpg" alt="img" style="zoom:110%;"> <p>（二）本机PySpark环境配置</p><p>在前面部署Spark的时候，已经在Linux系统上部署了acaconda的Python环境，详见Spark的Stand Alone模式部署章节。故本次在Windows上安装anaconda，并配置PySpark库。具体安装步骤如下：</p><p>（1）在课程资料中选择anaconda应用程序双击安装。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps47.jpg" alt="img" style="zoom:110%;"><p>（2）一直选择Next，进行安装。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps48.png" alt="img" style="zoom:100%;"><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps49.png" alt="img" style="zoom:100%;"> <p>注意：选择第一个，将anaconda添加至我的环境变量中！</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps50.png" alt="img" style="zoom:100%;">  <p>（3）安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。</p><p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps51.jpg" alt="img"> </p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps52.jpg" alt="img" style="zoom:100%;">  <p>（4）配置国内源，加速网络下载。</p><p>1、在Anaconda Prompt(anaconda)中执行</p><pre class="line-numbers language-none"><code class="language-none">conda config --set show_channel_urls yes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2、将如下内容替换到C:\Users\用户名.condarc文件中。</p><pre class="line-numbers language-none"><code class="language-none">channels: \- defaultsshow_channel_urls: truedefault_channels: \- https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main \- https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r \- https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;msys2custom_channels: conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud simpleitk: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（5）创建虚拟环境</p><p>1、创建虚拟环境 pyspark, 基于Python 3.8</p><pre class="line-numbers language-none"><code class="language-none">conda create -n pyspark python&#x3D;3.8<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2、切换到虚拟环境内</p><pre class="line-numbers language-none"><code class="language-none">conda activate pyspark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3、在虚拟环境内安装包</p><pre class="line-numbers language-none"><code class="language-none">pip install pyhive pyspark jieba -i [https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;](https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple)simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装成功示例：</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps53.jpg" alt="img" style="zoom:110%;">  <p>（三）PyCharm中配置Python解释器</p><p>（1）配置本地解释器：创建Python项目，选择conda虚拟环境PySpark中的Python.exe解释器。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps54.jpg" alt="img" style="zoom:110%;">  <p>（2）配置远程SSH Linux解释器</p><p>1、远程SSH python pyspark环境</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps55.jpg" alt="img" style="zoom:110%;">  <p>2、添加新的远程连接</p><p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps56.jpg" alt="img" style="zoom:100%;"><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps57.jpg" alt="img" style="zoom:100%;"> </p><p>3、设置虚拟的python环境路径</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps58.jpg" alt="img" style="zoom:110%;">  <p>（四）WordCount应用实战</p><p>可以选择在本地的PySpark环境中执行spark代码，也可以选择在虚拟机环境PySpark中执行。选择本地的就是使用conda环境，应用其中的PySpark环境执行，来读取本地文件，完成单词计数的实例。选择远程虚拟机中的PySpark环境，需要SSH连接到服务器（这里需要安装Pycharm专业版），注意：无论是选择那种方案，都是在PyCharm软件中去执行，完成上述过程。</p><p>（1）WordCount代码本地执行</p><p>准备pyspark代码以及本地文件words.txt，在PyCharm中执行。</p><pre class="line-numbers language-none"><code class="language-none"># coding:utf8from pyspark import SparkConf, SparkContext\# import osimport osos.environ[&#39;PYSPARK_PYTHON&#39;]&#x3D;&#39;D:\\anaconda3\\envs\\pyspark\\python.exe&#39;os.environ [&#39;JAVA_HOME&#39;] &#x3D; &#39;D:\\Java\\jdk1.8.0_241&#39;\#os.environ[&#39;PYSPARK_PYTHON&#39;]&#x3D;&#39;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python3.8&#39;if __name__ &#x3D;&#x3D; &#39;__main__&#39;:  conf &#x3D; SparkConf().setAppName(&quot;WordCountHelloWorld&quot;).setMaster(&quot;local[*]&quot;)  # 通过SparkConf对象构建SparkContext对象  sc &#x3D; SparkContext(conf&#x3D;conf)  # 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量  # 读取文件  #file_rdd &#x3D; sc.textFile(&quot;hdfs:&#x2F;&#x2F;node1:8020&#x2F;input&#x2F;words.txt&quot;)  #file_rdd &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;tmp&#x2F;pycharm_project_621&#x2F;data&#x2F;words.txt&quot;)  file_rdd &#x3D; sc.textFile(&quot;D:\\数据挖掘与分析实验报告合集\\pyspark\\data\\input\\words.txt&quot;)  # 将单词进行切割, 得到一个存储全部单词的集合对象  words_rdd &#x3D; file_rdd.flatMap(lambda line: line.split(&quot; &quot;))  # 将单词转换为元组对象, key是单词, value是数字1  words_with_one_rdd &#x3D; words_rdd.map(lambda x: (x, 1))  # 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加)  result_rdd &#x3D; words_with_one_rdd.reduceByKey(lambda a, b: a + b)  # 通过collect方法收集RDD的数据打印输出结果print(result_rdd.collect())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>运行结果截图：</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps59.jpg" alt="img" style="zoom:110%;"> <p>（2）WordCount代码远程服务器上执行。</p><p>通过SSH连接到远程服务器上，详见上述操作。</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps60.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps61.jpg" alt="img" style="zoom:110%;">  <pre class="line-numbers language-none"><code class="language-none">完成与服务器连接后，会在服务器中的&#x2F;tmp文件夹下新建了pycharm_project_xxx文件夹用于放置本地的同步代码。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps62.jpg" alt="img" style="zoom:110%;">  <p>（3）读取HDFS上的文件</p><p>1、将读取文件路径改为hdfs上的&#x2F;input&#x2F;words.txt</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps63.jpg" alt="img" style="zoom:110%;">  <p>2、在hdfs上新建&#x2F;input文件夹，使用命令</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;input<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3、上传words.txt到hdfs中，使用命令</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -put words.txt &#x2F;input&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps64.jpg" alt="img" style="zoom:100%;"> <p>4、在pycharm中执行spark代码</p><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps65.jpg" alt="img" style="zoom:110%;">  <p>（五）spark-submit作业提交</p><p>（1）local本地模式</p><p>首先将helloword.py程序放到&#x2F;root&#x2F;目录下，使用命令完成提交作业。</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-submit --master local[*] &#x2F;root&#x2F;helloworld.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps66.jpg" alt="img" style="zoom:110%;">  <p>（2）spark on yarn模式</p><p>使用命令完成提交作业。</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-submit --master yarn &#x2F;root&#x2F;helloworld.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps67.jpg" alt="img" style="zoom:110%;">  <p>（3）使用历史服务器查看任务执行情况 </p><pre class="line-numbers language-none"><code class="language-none">node1:18080<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps68.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Spark-pyspark%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A0%81%E7%8E%AF%E5%A2%83/wps69.jpg" alt="img" style="zoom:110%;"> <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark环境部署.md</li><li>[2]1-saprk基础入门.pdf</li><li>[3]spark1.pptx</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Spark（pyspark基础编码环境）&quot;&gt;&lt;a href=&quot;#Spark（pyspark基础编码环境）&quot; class=&quot;headerlink&quot; title=&quot;Spark（pyspark基础编码环境）&quot;&gt;&lt;/a&gt;&lt;strong&gt;Spark（pyspark基础编码环</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-YARN模式部署</title>
    <link href="https://weichunxiu123.github.io/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/"/>
    <id>https://weichunxiu123.github.io/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/</id>
    <published>2023-06-08T02:48:27.429Z</published>
    <updated>2023-06-09T00:30:12.987Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（YARN模式）"><a href="#spark（YARN模式）" class="headerlink" title="spark（YARN模式）"></a><strong>spark（YARN模式）</strong></h2><p>（1）Client模式中driver运行在客户端，在客户端显示输出结果，但是在spark历史服务器不显示logs信息。</p><p>（2）Cluster模式中driver运行在YARN容器内部，和ApplicationMaster在同一个容器内，在客户端不显示输出结果，所以在spark历史服务器中显示logs的信息。</p><img src="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/wps43.jpg" alt="img" style="zoom:110%;"> <p>（3）client模式测试</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-submit --master yarn --deploy-mode client --driver-memory 512m $&#123;SPARK_HOME&#125;&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/wps44.jpg" alt="img" style="zoom:110%;">  <p>（4） cluster模式测试</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-submit --master yarn --deploy-mode cluster --driver-memory 512m \--conf &quot;spark.pyspark.driver.python&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">&#x2F;bin&#x2F;python3&quot; \--conf &quot;spark.pyspark.python&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;bin&#x2F;python3&quot; $&#123;SPARK_HOME&#125;&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/spark-YARN%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2/wps45.jpg" alt="img" style="zoom:110%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark环境部署</li><li>[2]spark1.pptx</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（YARN模式）&quot;&gt;&lt;a href=&quot;#spark（YARN模式）&quot; class=&quot;headerlink&quot; title=&quot;spark（YARN模式）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（YARN模式）&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）Client</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-HA环境部署</title>
    <link href="https://weichunxiu123.github.io/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
    <id>https://weichunxiu123.github.io/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</id>
    <published>2023-06-08T02:34:22.238Z</published>
    <updated>2023-06-09T00:27:10.084Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（HA环境部署）"><a href="#spark（HA环境部署）" class="headerlink" title="spark（HA环境部署）"></a><strong>spark（HA环境部署）</strong></h2><p>（1）首先进入spark-env.sh中，</p><pre class="line-numbers language-none"><code class="language-none">vim &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps33.jpg" alt="img" style="zoom:100%;"> <p>（2）在spark-env.sh配置文件中删除</p><pre class="line-numbers language-none"><code class="language-none">export SPARK_MASTER_HOST&#x3D;node1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（目的是不让机器知道固定的master是谁，不然无法进行master切换）</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps34.jpg" alt="img" style="zoom:110%;"> <p>（3）在spark-env.sh配置文件中增加以下内容：</p><pre class="line-numbers language-none"><code class="language-none">SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -Dspark.deploy.zookeeper.url&#x3D;node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</p><p># 指定Zookeeper的连接地址</p><p># 指定在Zookeeper中注册临时节点的路径</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps35.jpg" alt="img" style="zoom:110%;">  <p>（4）将spark-env.sh配置文件分发给node2、node3。</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;scp -r &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps36.jpg" alt="img" style="zoom:110%;">  <p>（5）启动StandAlone集群、zookeeper集群：</p><p>1）在node1上：</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps37.jpg" alt="img"> </p><p>2）在node2上：</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-master.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（目的是：备用master，当kill掉node1的master后，程序依然能进行）</p><p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps38.jpg" alt="img"> </p><p>（6）查看node1、node2的WEB UI</p><p>（如果8080端口被占用了，可以顺延到8081、8082端口，其中node1上的master是alive的，node2上的是standby） </p><pre class="line-numbers language-none"><code class="language-none">node1:8080--&gt;8081   node2:8080--&gt;8081\8082<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps39.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps40.jpg" alt="img" style="zoom:100%;"> <p>（6）Master主备切换，在&#x2F;export&#x2F;server&#x2F;spark路径下提交一个任务到当前alive master上:</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-submit --master spark:&#x2F;&#x2F;node1:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（在提交成功后, 将alive master直接kill掉，系统不会中断，仍然能正常运行结果）</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps41.jpg" alt="img" style="zoom:100%;"> <p>（7）查看Master的WEB UI，只有node2是alive的，证明master切换成功</p><img src="/wiki/spark-HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps42.jpg" alt="img" style="zoom:100%;"> <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark环境部署.md</li><li>[2]1-spark基础环境配置.pdf</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（HA环境部署）&quot;&gt;&lt;a href=&quot;#spark（HA环境部署）&quot; class=&quot;headerlink&quot; title=&quot;spark（HA环境部署）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（HA环境部署）&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）首先进入sp</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-standalone环境部署</title>
    <link href="https://weichunxiu123.github.io/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
    <id>https://weichunxiu123.github.io/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</id>
    <published>2023-06-08T01:56:46.881Z</published>
    <updated>2023-06-09T00:29:40.028Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（StandAlone环境部署）"><a href="#spark（StandAlone环境部署）" class="headerlink" title="spark（StandAlone环境部署）"></a><strong>spark（StandAlone环境部署）</strong></h2><p>（一）、集群规划：选择三台机器分别为node1、node2、node3来组成集群环境。</p><p>其中node1上安装master和worker进程；node2上安装worker进程；node3上安装worker进程。</p><p>（二）、anaconda on linux安装过程：</p><p>（1）前提：在linux服务器node1、node2、node3上都安装python(anaconda)。并安装pyspark虚拟环境。具体安装步骤如下。</p><p>1、在&#x2F;export&#x2F;server&#x2F;目录下上传anaconda的安装包Anaconda3-2021.05-Linux-x86_64.sh。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps1.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps2.jpg" alt="img" style="zoom:100%;">  <p>2、安装anaconda 使用命令：</p><pre class="line-numbers language-none"><code class="language-none">sh .&#x2F;Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps3.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps4.jpg" alt="img" style="zoom:110%;"></p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps5.jpg" alt="img" style="zoom:100%;">  <p>3、安装完毕之后若没有出现base环境，进行如下配置。在&#x2F;root&#x2F;.condarc添加国内源</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps6.jpg" alt="img" style="zoom:110%;"> <p>安装完毕后，关闭服务器重新启动，出现base环境即安装成功。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps7.jpg" alt="img" style="zoom:110%;">  <p>（2）在anaconda中，安装pyspark虚拟环境。</p><p>1、基于python3.8安装pyspark环境。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps8.jpg" alt="img" style="zoom:110%;"> <p>2、切换到pyspark中，并安装所需要的安装包。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps9.jpg" alt="img" style="zoom:110%;"> <p>注：在node1、node2、node3三台服务器上都完成配置！</p><p>（三）、StandAlone模式部署</p><p>（1）安装spark压缩文件。</p><p>1、进入到&#x2F;export&#x2F;server&#x2F;中上传并解压spark-3.2.0-bin-hadoop3.2.tgz。并设置软链接，命令为</p><pre class="line-numbers language-none"><code class="language-none">ln-s&#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）在&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf，配置文件。</p><p>1、首先在配置workers文件。</p><pre class="line-numbers language-none"><code class="language-none">mv workers.template workersvim workers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps10.jpg" alt="img" style="zoom:110%;"> <p>2.配置spark-env.sh文件。mv spark-env.sh.template spark-env.sh；</p><p>Vim spark-env.sh，添加如下内容。</p><pre class="line-numbers language-none"><code class="language-none">## 设置JAVA安装目录JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk\## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoopYARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop\## 指定spark老大Master的IP和提交任务的通信端口\# 告知Spark的master运行在哪个机器上export SPARK_MASTER_HOST&#x3D;node1\# 告知sparkmaster的通讯端口export SPARK_MASTER_PORT&#x3D;7077\# 告知spark master的 webui端口SPARK_MASTER_WEBUI_PORT&#x3D;8080\# worker cpu可用核数SPARK_WORKER_CORES&#x3D;1\# worker可用内存SPARK_WORKER_MEMORY&#x3D;1g\# worker的工作通讯地址SPARK_WORKER_PORT&#x3D;7078\# worker的 webui地址SPARK_WORKER_WEBUI_PORT&#x3D;8081\## 设置历史服务器\# 配置的意思是  将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中SPARK_HISTORY_OPTS&#x3D;&quot;-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; -Dspark.history.fs.cleaner.enabled&#x3D;true&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps11.jpg" alt="img" style="zoom:110%;">  <p>3、在HDFS上创建程序运行历史记录存放的文件夹。</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;sparklog；hadoop fs -chmod 777 &#x2F;sparklog<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4、配置spark-defaults.conf文件。</p><pre class="line-numbers language-none"><code class="language-none">mv spark-defaults.conf.template spark-defaults.confvim spark-defaults.conf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>添加如下内容。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps12.jpg" alt="img" style="zoom:110%;"><p>5、配置log4j.properties 文件[可选配置]。</p><pre class="line-numbers language-none"><code class="language-none">mv log4j.properties.template log4j.properties<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>修改配置，设置级别为WARN 只输出警告和错误日志。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps13.jpg" alt="img" style="zoom:110%;">  <p>（四）、将spark分发到node2和node3服务器上。注意同时要设置软链接。</p><pre class="line-numbers language-none"><code class="language-none">scp -r spark-3.1.2-bin-hadoop3.2 node2:&#x2F;export&#x2F;server&#x2F;scp -r spark-3.1.2-bin-hadoop3.2 node3:&#x2F;export&#x2F;server&#x2F;ln -s &#x2F;export&#x2F;server&#x2F;spark-3.1.2-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注意：配置&#x2F;etc&#x2F;profile，JAVA_HOME；SPARK_HOME；PYSPARK_PYTHON都指向正确的目录。</p><p>（五）、启动历史服务器，启动Spark的Master和Worker进程</p><pre class="line-numbers language-none"><code class="language-none">（1）启动历史服务器：sbin&#x2F;start-history-server.sh（2）启动全部的master和worker：sbin&#x2F;start-all.Sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps14.jpg" alt="img" style="zoom:110%;"> <p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps15.jpg" alt="img" style="zoom:100%;"><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps16.jpg" alt="img" style="zoom:100%;"> </p><p>（六）、查看Master的WEB UI 在浏览器中输入node1:8080</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps17.jpg" alt="img" style="zoom:100%;"> <p>（七）、连接到StandAlone集群</p><p>（1）通过master来连接到StandAlone集群。</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;pyspark --master spark:&#x2F;&#x2F;node1:7077<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps18.jpg" alt="img" style="zoom:110%;"> <p>（2）使用spark-shell连接StandAlone集群。</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-shell –master spark:&#x2F;&#x2F;node1:7077<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>进行测试。</p><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps19.jpg" alt="img" style="zoom:110%;"> <p>（3）使用spark-submit(PI)提交任务到集群上执行。</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;node1:7077&#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;Python&#x2F;pi.py 10<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps20.jpg" alt="img" style="zoom:110%;"><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps21.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">查看历史服务器：在浏览器中输入node1：18080<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-standalone%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/wps22.jpg" alt="img" style="zoom:110%;"><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark(standalone部署文档)</li><li>[2]1-spark基础入门.pdf</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（StandAlone环境部署）&quot;&gt;&lt;a href=&quot;#spark（StandAlone环境部署）&quot; class=&quot;headerlink&quot; title=&quot;spark（StandAlone环境部署）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（StandAlon</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark-local模式配置</title>
    <link href="https://weichunxiu123.github.io/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/"/>
    <id>https://weichunxiu123.github.io/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/</id>
    <published>2023-06-07T11:38:25.725Z</published>
    <updated>2023-06-09T00:28:24.047Z</updated>
    
    <content type="html"><![CDATA[<h2 id="spark（local环境部署）"><a href="#spark（local环境部署）" class="headerlink" title="spark（local环境部署）"></a><strong>spark（local<strong><strong>环境部署</strong></strong>）</strong></h2><p>（1）安装Anaconda</p><p>上传安装包 </p><pre class="line-numbers language-none"><code class="language-none">sh .&#x2F;Anaconda3-2021.05-Linux-x86_64.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps110.jpg" alt="img" style="zoom:110%;"> <p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps110.jpg" alt="img" style="zoom:110%;">出现（base)即为安装成功</p><p>（2）创建虚拟环境</p><pre class="line-numbers language-none"><code class="language-none">conda create -n pyspark python&#x3D;3.8conda activate pysparkpip install pyhive pyspark jieba -i [https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn](https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple)&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（1）修改环境变量配置Spark由如下5个环境变量需要设置</p><p>SPARK_HOME: 表示Spark安装路径在哪里 </p><p>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器。 </p><p>JAVA_HOME: 告知Spark Java在哪里 </p><p>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 </p><p>HADOOP_HOME: 告知Spark  Hadoop安装在哪里</p><p>这5个环境变量 都需要配置在: &#x2F;etc&#x2F;profile中！</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps111.jpg" alt="img" style="zoom:110%;"> <p>（4）解压</p><p>解压下载的Spark安装包</p><pre class="line-numbers language-none"><code class="language-none">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps113.jpg" alt="img" style="zoom:110%;"><p>设置软连接</p><pre class="line-numbers language-none"><code class="language-none">ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）测试</p><pre class="line-numbers language-none"><code class="language-none">sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>bin&#x2F;pyspark在这个环境可以运行spark代码，如图：</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps115.jpg" alt="img" style="zoom:110%;"><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps114.jpg" alt="img" style="zoom:110%;"><p>在所有机器安装Python(Anaconda)，并在所有机器配置环境变量。 </p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps116.jpg" alt="img" style="zoom:110%;">  <img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps117.jpg" alt="img" style="zoom:110%;"> <p>（3）配置配置文件</p><p>进入到spark的配置文件目录中, cd $SPARK_HOME&#x2F;conf&#96;</p><p>配置workers文件vi workers</p><p># 改名, 去掉后面的.template后缀</p><pre class="line-numbers language-none"><code class="language-none">mv workers.template workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 编辑worker文件</p><pre class="line-numbers language-none"><code class="language-none">vim workers<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 将里面的localhost删除, 追加到workers文件内</p><pre class="line-numbers language-none"><code class="language-none">node1node2node3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（4）配置spark-env.sh文件</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps118.jpg" alt="img" style="zoom:100%;"><p># 1. 改名</p><pre class="line-numbers language-none"><code class="language-none">mv spark-env.sh.template spark-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 2. 编辑spark-env.sh, 在底部追加如下内容</p><p>## 设置JAVA安装目录</p><pre class="line-numbers language-none"><code class="language-none">JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</p><pre class="line-numbers language-none"><code class="language-none">HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoopYARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>## 指定spark老大Master的IP和提交任务的通信端口</p><p># 告知Spark的master运行在哪个机器上</p><pre class="line-numbers language-none"><code class="language-none">export SPARK_MASTER_HOST&#x3D;node1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 告知sparkmaster的通讯端口</p><pre class="line-numbers language-none"><code class="language-none">export SPARK_MASTER_PORT&#x3D;7077<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># 告知spark master的 webui端口</p><pre class="line-numbers language-none"><code class="language-none">SPARK_MASTER_WEBUI_PORT&#x3D;8080<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># worker cpu可用核数</p><pre class="line-numbers language-none"><code class="language-none">SPARK_WORKER_CORES&#x3D;1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># worker可用内存</p><pre class="line-numbers language-none"><code class="language-none">SPARK_WORKER_MEMORY&#x3D;1g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># worker的工作通讯地址</p><pre class="line-numbers language-none"><code class="language-none">SPARK_WORKER_PORT&#x3D;7078<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p># worker的 webui地址</p><pre class="line-numbers language-none"><code class="language-none">SPARK_WORKER_WEBUI_PORT&#x3D;8081<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>## 设置历史服务器</p><pre class="line-numbers language-none"><code class="language-none"># 配置的意思是  将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中SPARK_HISTORY_OPTS&#x3D;&quot;-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; -Dspark.history.fs.cleaner.enabled&#x3D;true&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps119.jpg" alt="img" style="zoom:100%;"><p>在HDFS上创建程序运行历史记录存放的文件夹:</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;sparkloghadoop fs -chmod 777 &#x2F;sparklog<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps120.jpg" alt="img" style="zoom:100%;"><p>（3）配置spark-defaults.conf文件</p><pre class="line-numbers language-none"><code class="language-none"># 1. 改名mv spark-defaults.conf.template spark-defaults.conf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 2. 修改内容, 追加如下内容# 开启spark的日期记录功能spark.eventLog.enabled true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 设置spark日志记录的路径spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none"># 设置spark日志是否启动压缩spark.eventLog.compress true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps121.jpg" alt="img" style="zoom:100%;"><p>（4)配置log4j.properties 文件 [可选配置]</p><p>mv log4j.properties.template log4j.properties 注意：将Spark安装文件夹  分发到其它的服务器</p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps122.jpg" alt="img" style="zoom:100%;"><pre class="line-numbers language-none"><code class="language-none">1）scp -r spark-3.1.2-bin-hadoop3.2 node2:&#x2F;export&#x2F;server&#x2F;2）scp -r spark-3.1.2-bin-hadoop3.2 node3:&#x2F;export&#x2F;server&#x2F;3）在node2和node3上 给spark安装目录增加软链接ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps123.jpg" alt="img" style="zoom:100%;"> <p>（5）启动历史服务器</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-history-server.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre> <img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/image-20230607195152782.png" alt="image-20230607195152782" style="zoom:100%;"><p>（6）启动Spark的Master和Worker进程</p><pre class="line-numbers language-none"><code class="language-none">sbin&#x2F;start-all.shsbin&#x2F;start-master.shsbin&#x2F;start-worker.shsbin&#x2F;stop-all.shsbin&#x2F;stop-master.shsbin&#x2F;stop-worker.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps126.jpg" alt="img"> </p><p>查看Master的WEB UI</p><p>连接到StandAlone集群</p><p><img src="/wiki/Spark-local%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE/wps128.jpg" alt="img"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li>[1]spark部署.md</li><li>[2]spark 1.pptx</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;spark（local环境部署）&quot;&gt;&lt;a href=&quot;#spark（local环境部署）&quot; class=&quot;headerlink&quot; title=&quot;spark（local环境部署）&quot;&gt;&lt;/a&gt;&lt;strong&gt;spark（local&lt;strong&gt;&lt;strong&gt;环境部</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="spark" scheme="https://weichunxiu123.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Docker安装与基本操作</title>
    <link href="https://weichunxiu123.github.io/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>https://weichunxiu123.github.io/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</id>
    <published>2023-06-07T10:25:11.923Z</published>
    <updated>2023-06-09T06:11:50.827Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、安装docker"><a href="#一、安装docker" class="headerlink" title="一、安装docker"></a>一、<strong>安装docker</strong></h2><p>（1）卸载（可选）</p><p>如果之前安装过旧版本的Docker，可以使用下面命令卸载：</p><pre class="line-numbers language-none"><code class="language-none">yum remove docker \docker-client \docker-client-latest \docker-common \docker-latest \docker-latest-logrotate \docker-logrotate \docker-selinux \docker-engine-selinux \docker-engine \docker-ce<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（2）yum源配置</p><p>1.备份配置文件</p><pre class="line-numbers language-./" data-language="./"><code class="language-./">&#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.backup<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps45.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo [http:&#x2F;&#x2F;mirrors.aliyun](http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo).com&#x2F;repo&#x2F;Centos-7.repo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps46.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo [http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;rep](http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo)o&#x2F;epel-7.repo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps47.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">yum clean all<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps48.jpg" alt="img" style="zoom:90%;"> <pre class="line-numbers language-none"><code class="language-none">yum makecache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps49.jpg" style="zoom:120%;"><pre class="line-numbers language-none"><code class="language-none">yum install -y bash-completion vim lrzsz wget expect net-tools nc nmap treedos2unix htop iftop iotop unzip telnet sl psmisc nethogs glances bc ntpdate openldap-devel<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps50.jpg" alt="img" style="zoom:115%;"><p><em><strong>*安装docker*</strong></em></p><p>（1）受限需要虚拟机联网，安装yum工具</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps51.jpg" alt="img" style="zoom:110%;"> <p>（2）配置网卡转发</p><p>1）docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能</p><p>#写入</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps52.jpg" alt="img" style="zoom:110%;"> <p>2）重新加载内核参数</p><pre class="line-numbers language-none"><code class="language-none">modprobe br_netfiltersysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;docker.conf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps53.jpg" alt="img" style="zoom:110%;"> <p>（3）利用yum进行docker安装</p><p>提前配置好yum仓库</p><p>1）阿里云自带仓库</p><pre class="line-numbers language-none"><code class="language-none">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;Centos-7.repo [http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;](http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo)Centos-7.repo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps54.jpg" alt="img" style="zoom:110%;"> <p> 2）阿里云提供的docker专属repo仓库</p><pre class="line-numbers language-none"><code class="language-none">curl-o&#x2F;etc&#x2F;yum.repos.d&#x2F;docker-ce.repohttp:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps55.jpg" alt="img" style="zoom:110%;"> <p>3）更新yum缓存</p><pre class="line-numbers language-none"><code class="language-none">yum clean all &amp;&amp; yum makecache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps56.jpg" alt="img" style="zoom:110%;"> <p>4）查看源中可用版本</p><pre class="line-numbers language-none"><code class="language-none">yum list docker-ce --showduplicates | sort -r<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps57.jpg" alt="img" style="zoom:110%;"> <p>5）yum安装</p><pre class="line-numbers language-none"><code class="language-none">yum install docker-ce -y<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps58.jpg" alt="img" style="zoom:110%;"><p>docker -v</p><p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps59.jpg" alt="img"> </p><p>卸载</p><pre class="line-numbers language-none"><code class="language-none">yum remove -y docker-ce-xxx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（4）配置镜像加速器</p><p>用于加速镜像文件下载,选用阿里云镜像站</p><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;etc&#x2F;docker<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps60.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">touch &#x2F;etc&#x2F;docker&#x2F;daemon.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps61.jpg" alt="img" style="zoom:110%;"> <p>1）进入文件vim &#x2F;etc&#x2F;docker&#x2F;daemon.json编写以下内容：</p><p>{“registry-mirrors” : [“<a href="https://8xpk5wnt.mirror.aliyuncs.com" ]}">https://8xpk5wnt.mirror.aliyuncs.com&quot;]}</a></p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps62.jpg" alt="img" style="zoom:100%;"> <p>（5）启动docker</p><pre class="line-numbers language-none"><code class="language-none">1）关闭防火墙：systemctl stop firewalld2）禁止开机启动防火墙：systemctl disable firewalld3）查看防火墙状态：systemctl status firewalld<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps63.jpg" alt="img" style="zoom:110%;"> <p>通过命令启动docker：</p><pre class="line-numbers language-none"><code class="language-none">systemctl start docker  启动docker服务systemctl stop docker  停止docker服务systemctl restart docker  重启docker服务<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps64.jpg" alt="img" style="zoom:100%;"> <p>docker配置文件重新加载：systemctl daemon-reload</p><p>设置开启自启动：systemctl enable docker</p><p>（6）查看docker信息：</p><pre class="line-numbers language-none"><code class="language-none">docker info<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps65.jpg" alt="img" style="zoom:110%;"> <p>（7）显示当前正在运行的容器：</p><pre class="line-numbers language-none"><code class="language-none">docker ps<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps66.jpg" alt="img" style="zoom:100%;"><p>（8）docker镜像：</p><pre class="line-numbers language-none"><code class="language-none">docker images<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps67.jpg" alt="img" style="zoom:100%;"> <p>（9）docker版本：docker version</p><pre class="line-numbers language-none"><code class="language-none">docker-clientwhich docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps68.jpg" alt="img" style="zoom:100%;"> <p>docker daemon，运行在docker host上，负责创建、运行、监控容器、构建、存储镜像</p><pre class="line-numbers language-none"><code class="language-none">ps aux |grep docker<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps69.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">containerdps aux|grep containerdsystemctl status containerd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps70.jpg" alt="img" style="zoom:110%;"> <p><em><strong>*docker的基本操作*</strong></em></p><p>\1. 启动第一个docker容器</p><p>Nginx web服务器，运行一个80端口的网站</p><p>在宿主机上，运行Nginx</p><p>开启服务器</p><p>2.在服务器上安装好运行nginx所需的依赖关系</p><p>3.安装nginx yum install nginx -y</p><p>4.修改nginx配置文件</p><p>5.启动nginx</p><p>6.客户端去访问nginx</p><p>（1）查看本地的docker镜像有哪些：</p><pre class="line-numbers language-none"><code class="language-none">docker image ls 或 docker images<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps71.jpg" alt="img" style="zoom:100%;"> <p>（2）可选择删除旧版本：docker rmi 镜像id</p><p>（3）搜索一下远程仓库中的镜像文件是否存在：</p><pre class="line-numbers language-none"><code class="language-none">docker search nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps72.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">docker pull nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps73.jpg" alt="img" style="zoom:110%;"> <p>（4）再次查看镜像：docker images</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps74.jpg" alt="img" style="zoom:100%;"> <p>（5）运行镜像，运行出具体内容，在容器中就跑着一个nginx服务，docker run 参数 镜像的名字&#x2F;id</p><p>#-d 后台运行容器</p><p>#-p 80:80 端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口，会返回一个容器的id</p><pre class="line-numbers language-none"><code class="language-none">docker run -d -p 80:80 nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps75.jpg" alt="img" style="zoom:100%;"> <p>（6）查看容器是否在运行：docker ps</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps76.jpg" alt="img" style="zoom:100%;"> <p>（7）访问网站192.168.88.163:80</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps77.jpg" alt="img" style="zoom:110%;"> <p>（8）停止容器：docker stop 容器id</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps78.jpg" alt="img" style="zoom:90%;"> <p>（9）恢复容器：docker start 容器id</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps79.jpg" alt="img" style="zoom:90%;">  <p><em><strong>*docker镜像原理*</strong></em></p><p>（1）查看发行版： </p><pre class="line-numbers language-none"><code class="language-none">cat &#x2F;etc&#x2F;redhat-release <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps80.jpg" alt="img" style="zoom:100%;"> <p>（2）查看内核：</p><pre class="line-numbers language-none"><code class="language-none">uname -r<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps81.jpg" alt="img" style="zoom:100%;"><p>（3）利用docker获取不同的发行版镜像（例如centos：7.8.2003）：</p><pre class="line-numbers language-none"><code class="language-none">docker pull centos:7.8.2003<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps82.jpg" alt="img" style="zoom:110%;"> <p>（4）确认当前宿主机的发行版：</p><pre class="line-numbers language-none"><code class="language-none">cat &#x2F;etc&#x2F;redhat-release<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps83.jpg" alt="img" style="zoom:100%;"> <p>（5）运行centos:7.8.2003发行版本</p><p>#运行容器，且进入容器内部</p><p>#参数解释，-i 交互式命令操作 -t 开启一个终端 bash 进入容器后执行的命令</p><pre class="line-numbers language-none"><code class="language-none">docker run -it afb6fca791e0 bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps84.jpg" alt="img" style="zoom:100%;"> <p>（6）退出容器空间：exit</p><p><em><strong>*获取镜像*</strong></em></p><p>（1）docker search 镜像名:tag tag就是具体的标签版本：</p><pre class="line-numbers language-none"><code class="language-none">docker search centos<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps85.jpg" alt="img" style="zoom:100%;"> <p>（2）查看docker镜像的存储路径：</p><pre class="line-numbers language-none"><code class="language-none">docker info |grep Root<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps86.jpg" alt="img" style="zoom:100%;"> <p>（3）具体位置：</p><pre class="line-numbers language-none"><code class="language-none">ls &#x2F;var&#x2F;lib&#x2F;docker&#x2F;image&#x2F;overlay2&#x2F;imagedb&#x2F;content&#x2F;sha256<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps87.jpg" alt="img" style="zoom:110%;"> <p>（4）使用不同的镜像，生成容器# -it 开启一个交互式的终端–rm 容器退出时删除该容器</p><p>#再运行一个7.8centos</p><p>docker run -it –rm centos bash</p><p><em><strong>*查看镜像*</strong></em></p><p>（1）查看所有镜像：docker images</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps88.jpg" alt="img" style="zoom:100%;"> <p>（2）指定tag查看：docker images centos:7.8.2003</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps89.jpg" alt="img" style="zoom:100%;"> <p>（3）只列出镜像id  #-q –quiet 只列出id：docker images -q</p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps90.jpg" alt="img" style="zoom:100%;"> <p>（4）格式化显示镜像</p><p># 这是docker的模板语言，–format</p><pre class="line-numbers language-none"><code class="language-none">docker images --format &quot;&#123;&#123;.ID&#125;&#125;--&#123;&#123;.Repository&#125;&#125;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps91.jpg" alt="img" style="zoom:100%;"> <p><em><strong>*删除镜像*</strong></em></p><pre class="line-numbers language-none"><code class="language-none">（1）删除容器记录：docker rm 容器id（2）指定id的前三位即可：docker rmi 镜像id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><em><strong>*镜像管理*</strong></em></p><pre class="line-numbers language-none"><code class="language-none">（1）批量删除镜像，慎用：docker rmi &#39;docker images -aq&#39;（2）批量删除容器：docker rm &#39;docker ps -aq&#39;（3）导出镜像：docker save -o nginx.tgz nginx:latest#打包tar包（4）导入镜像先删除本地的nginx镜像：docker rmi centos:7.8.2003docker image load -i &#x2F;export&#x2F;software&#x2F;centos1.8.2003.tgz#重新加载nginx-tar包查看cocker服务的信息：docker info<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps92.jpg" alt="img" style="zoom:100%;"> <p>查看镜像详细信息：docker image inspact 镜像id</p><p><em><strong>*docker镜像管理练习*</strong></em></p><p>（1）去DockerHub搜索Redies</p><p>（2）利用docker pull命令拉去镜像：</p><pre class="line-numbers language-none"><code class="language-none">docker pull redis<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps93.jpg" alt="img" style="zoom:110%;"> <p>（3）查看Redies镜像的名称和版本：</p><pre class="line-numbers language-none"><code class="language-none">docker search redis<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps94.jpg" alt="img" style="zoom:110%;"> <p>（4）利用docker save命令将redies:latest打包为一个redies.tar包</p><pre class="line-numbers language-none"><code class="language-none">docker save -o redis.tar redis:latest<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps95.jpg" alt="img" style="zoom:100%;"><p>（5）利用docker rmi删除本地的redis:latest </p><pre class="line-numbers language-none"><code class="language-none">docker rmi redis:latest<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps96.jpg" alt="img" style="zoom:100%;"> <p>（6）利用docker load重新加载Redis.tar文件</p><pre class="line-numbers language-none"><code class="language-none">docker load -i redis.tar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps97.jpg" alt="img" style="zoom:100%;"> <p><em><strong>*容器操作*</strong></em></p><p>1）创建并运行mn容器</p><pre class="line-numbers language-none"><code class="language-none">docker run --name mn -p 80:80 -d nginx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps98.jpg" alt="img" style="zoom:100%;"> <p>2）运行刚刚创建的nginx容器</p><pre class="line-numbers language-none"><code class="language-none">docker exec -it mn bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps99.jpg" alt="img" style="zoom:90%;"><p>3） 进入nginx的HTML所在目录 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps100.jpg" alt="img" style="zoom:100%;"> <p>4） 修改index.html的内容</p><p>sed -i -e ‘s#Welcome to nginx#人工智能学院欢迎您#g’ -e ‘s#<head>#<head><meta charset="utf-8">#g’ index.html</head></head></p><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps101.jpg" alt="img" style="zoom:110%;">  <p><em><strong>*创建和查看数据卷*</strong></em></p><pre class="line-numbers language-none"><code class="language-none">1.创建数据卷docker volume create html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps102.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">2.查看所有数据docker volume ls<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps103.jpg" alt="img" style="zoom:100%;"> <pre class="line-numbers language-none"><code class="language-none">3.查看数据卷详细信息卷docker volume inspect html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps104.jpg" alt="img" style="zoom:110%;"> <pre class="line-numbers language-none"><code class="language-none">4.挂载数据卷（1）创建容器并挂载数据卷到容器内的HTML目录，把&#x2F;export&#x2F;data&#x2F;docker-data&#x2F;nginx-html&#x2F;数据卷挂载到容器内的&#x2F;user&#x2F;share&#x2F;nginx&#x2F;html目录中：docker run --name mn -v &#x2F;export&#x2F;data&#x2F;docker-data&#x2F;nginx-html&#x2F;:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html -p 80:80 -d nginx（2）进入html数据卷所在位置，并修改HTML内容,查看html数据卷的位置：docker volume inspect &#x2F;export&#x2F;data&#x2F;docker-data&#x2F;nginx-html&#x2F;（3）进入该目录cd &#x2F;export&#x2F;data&#x2F;docker-data&#x2F;nginx-html&#x2F;_data（4）修改文件vi index.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em><strong>*Docker应用部署*</strong></em></p><ol><li><p>搜索mysql镜像：</p><pre class="line-numbers language-none"><code class="language-none">docker search mysql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps105.jpg" alt="img" style="zoom:100%;"> <ol start="2"><li>拉取mysql镜像</li></ol><pre class="line-numbers language-none"><code class="language-none">docker pull mysql:5.6<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps106.jpg" alt="img" style="zoom:100%;">  <ol start="3"><li>创建容器，设置端口映射、目录映射</li></ol><pre class="line-numbers language-none"><code class="language-none">mkdir -p &#x2F;export&#x2F;data&#x2F;docker-data&#x2F;mysqlcd &#x2F;export&#x2F;data&#x2F;docker-data&#x2F;mysqldocker run -id \-p 3306:3306 \--name&#x3D;bigdata_mysql \-v $PWD&#x2F;conf:&#x2F;etc&#x2F;mysql&#x2F;conf.d \-v $PWD&#x2F;logs:&#x2F;logs \-v $PWD&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \-e MYSQL_ROOT_PASSWORD&#x3D;hadoop \mysql:5.7.29<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps107.jpg" alt="img" style="zoom:100%;"> <ol start="4"><li>进入容器，操作mysql</li></ol><pre class="line-numbers language-none"><code class="language-none">docker exec –it bigdata_mysql &#x2F;bin&#x2F;bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps108.jpg" alt="img" style="zoom:100%;"> <ol start="5"><li>使用外部机器连接容器中的mysql</li></ol><img src="/wiki/Docker%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/wps109.jpg" alt="img" style="zoom:100%;">  <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><p>docker应用部署</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、安装docker&quot;&gt;&lt;a href=&quot;#一、安装docker&quot; class=&quot;headerlink&quot; title=&quot;一、安装docker&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;安装docker&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）卸载（可选）&lt;/p&gt;
&lt;p&gt;如果之前</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="Docker" scheme="https://weichunxiu123.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Git安装与配置</title>
    <link href="https://weichunxiu123.github.io/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>https://weichunxiu123.github.io/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</id>
    <published>2023-06-07T09:39:32.518Z</published>
    <updated>2023-06-09T06:11:33.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Git安装"><a href="#一、Git安装" class="headerlink" title="一、Git安装"></a>一、<strong>Git安装</strong></h2><p>（1）Git下载</p><p>Git下载程序</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps21.jpg" alt="img" style="zoom:110%;"> <p>（2）可视化客户端</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps22.jpg" alt="img" style="zoom:100%;"><p>中文语言包</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps23.jpg" alt="img" style="zoom:110%;"> <p>（3）初始化仓库</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps24.jpg" alt="img" style="zoom:110%;">  <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps25.jpg" alt="img" style="zoom:100%;"> <p>（4）添加文件，提交文件至本地仓库</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps26.jpg" alt="img" style="zoom:100%;"> <p>（5）本地删除与恢复</p><p>文件选中删除，可用以下方式还原</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps27.jpg" alt="img" style="zoom:110%;">  <p>（6）创建分支</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps28.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps29.jpg" alt="img" style="zoom:100%;"> <p>（7）分支的查看切换</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps30.jpg" alt="img" style="zoom:110%;">  <p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps31.jpg" alt="img" style="zoom:110%;"><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps32.jpg" alt="img" style="zoom:110%;"> </p><p>（8）标签的创建</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps33.jpg" alt="img" style="zoom:110%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps34.jpg" alt="img" style="zoom:100%;"> <p>（9）切换与删除</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps35.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps36.jpg" alt="img" style="zoom:110%;"> <p>通过右键选中删除</p><p><em><strong>*远程仓库*</strong></em></p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps37.jpg" alt="img" style="zoom:100%;">  <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps38.jpg" alt="img" style="zoom:100%;"> <p>（1）码云账号注册</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps39.jpg" alt="img" style="zoom:100%;"> <p>填写邮箱发送验证码,然后可以注册账号,主页如下</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps40.jpg" alt="img" style="zoom:100%;"> <p>（2）创建远程仓库</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps41.jpg" alt="img" style="zoom:100%;"> <p>（3）把本地代码推送到远端</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps42.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps43.jpg" alt="img" style="zoom:90%;"> <p>生成公钥私钥</p><p>ssh-keygen -t rsa</p><p>一直回车</p><p>会默认用户目录 .ssh 目录生成一个默认的id_rsa文件 和id_rsa.pub</p><p>密钥配置</p><img src="/wiki/Git%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/wps44.jpg" alt="img" style="zoom:100%;">  <h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li><u><em><a href="%E8%AF%BE%E4%B8%8AGit%E9%85%8D%E7%BD%AE%E6%96%87%E6%A1%A3">1</a></em></u></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、Git安装&quot;&gt;&lt;a href=&quot;#一、Git安装&quot; class=&quot;headerlink&quot; title=&quot;一、Git安装&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;Git安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;（1）Git下载&lt;/p&gt;
&lt;p&gt;Git下载程序&lt;/p&gt;
&lt;img </summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="Git" scheme="https://weichunxiu123.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础配置</title>
    <link href="https://weichunxiu123.github.io/wiki/hive%E5%AE%89%E8%A3%85/"/>
    <id>https://weichunxiu123.github.io/wiki/hive%E5%AE%89%E8%A3%85/</id>
    <published>2023-06-07T08:56:02.680Z</published>
    <updated>2023-06-09T06:11:18.255Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Hive安装"><a href="#一、Hive安装" class="headerlink" title="一、Hive安装"></a>一、<strong>Hive安装</strong></h2><p><em><strong>*（1）*</strong></em><em><strong>*Mysql安装*</strong></em></p><p>1）卸载Centos7自带的mariadb</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps1.jpg" alt="img" style="zoom:115%;"><p>如果出现了mariadb-libs-5.5.64-1.el7.x86_64，输入rpm -e mariadb- libs-5.5.64-1.el7.x86_64 –nodeps,在输入rpm -qa|grep mariadb，即可</p><p>2）安装mysql</p><p>新建文件夹</p><pre class="line-numbers language-none"><code class="language-none">mkdir &#x2F;export&#x2F;server&#x2F;mysql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>上传mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar到上述文件夹下，解压</p><pre class="line-numbers language-none"><code class="language-none">tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps2.jpg" alt="img" style="zoom:130%;"><p>3）执行安装</p><pre class="line-numbers language-none"><code class="language-none">yum -y install libaio<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps3.jpg" alt="img" style="zoom:120%;"><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps4.jpg" alt="img" style="zoom:130%;"> <p>4）mysql初始化设置</p><p>初始化：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mysqld <span class="token parameter variable">--initialize</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>更改所属组：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">chown</span> mysql:mysql /var/lib/mysql <span class="token parameter variable">-R</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>启动mysql：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">systemctl start mysqld.service<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps5.jpg" alt="img" style="zoom:110%;"> <p>查看临时生成的root密码：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">cat</span>  /var/log/mysqld.log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps6.jpg" alt="img" style="zoom:130%;"> <p>5）修改root密码 授权远程访问 设置开机自启动</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps7.jpg" alt="img" style="zoom:120%;"> <p>修改root密码 设置为hadoop</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps8.jpg" alt="img" style="zoom:120%;"> <p>授权</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">use mysql<span class="token punctuation">;</span>GRANT ALL PRIVILEGES ON *.* TO <span class="token string">'root'</span>@<span class="token string">'%'</span> IDENTIFIED BY <span class="token string">'hadoop'</span> WITH GRANT OPTION<span class="token punctuation">;</span>FLUSH PRIVILEGES<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps9.jpg" alt="img" style="zoom:100%;"> <p>mysql的启动和关闭 状态查看 （这几个命令必须记住）</p><pre class="line-numbers language-none"><code class="language-none">systemctl stop mysqldsystemctl status mysqldsystemctl start mysqld设置开机自动启动：systemctl enable  mysqld <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看是否设置自动启动成功</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps10.jpg" alt="img" style="zoom:110%;"> <p><em><strong>*（2）*</strong></em><em><strong>*H*</strong></em><em><strong>*ive*</strong></em><em><strong>*的安装*</strong></em></p><p>1）上传安装包 解压</p><pre class="line-numbers language-none"><code class="language-none">tar zxvf apache-hive-3.1.2-bin.tar.gzln -s apache-hive-3.1.2-bin hive<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>2）解决Hive与Hadoop之间guava版本差异</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;rm -rf lib&#x2F;guava-19.0.jarcp &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;guava-27.0-jre.jar.&#x2F;lib&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps11.jpg" alt="img"> </p><p>3）修改配置文件</p><pre class="line-numbers language-none"><code class="language-none">hive-env.shcd &#x2F;export&#x2F;server&#x2F;hive&#x2F;confmv hive-env.sh.template hive-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">vim hive-env.shexport HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoopexport HIVE_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hive&#x2F;confexport HIVE_AUX_JARS_PATH&#x3D;&#x2F;export&#x2F;server&#x2F;hive&#x2F;lib<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps12.jpg" alt="img" style="zoom:100%;"> <img src="/wiki/hive%E5%AE%89%E8%A3%85/wps13.jpg" alt="img" style="zoom:100%;"> <p>hive-site.xml</p><p>vim hive-site.xml</p><pre class="line-numbers language-none"><code class="language-none">&lt;configuration&gt;&lt;!-- 存储元数据mysql相关配置 --&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;hive3?createDatabaseIfNotExist&#x3D;true&amp;useSSL&#x3D;false&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;&lt;value&gt;root&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;&lt;value&gt;hadoop&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;!-- H2S运行绑定host --&gt;&lt;property&gt;  &lt;name&gt;hive.server2.thrift.bind.host&lt;&#x2F;name&gt;  &lt;value&gt;node1&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;!-- 远程模式部署metastore metastore地址 --&gt;&lt;property&gt;  &lt;name&gt;hive.metastore.uris&lt;&#x2F;name&gt;  &lt;value&gt;thrift:&#x2F;&#x2F;node1:9083&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;!-- 关闭元数据存储授权  --&gt; &lt;property&gt;  &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;&#x2F;name&gt;  &lt;value&gt;false&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4）上传mysql jdbc驱动到hive安装包lib下</p><p>mysql-connector-java-5.1.32.jar</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps14.jpg" alt="img" style="zoom:110%;"> <p>5）初始化元数据</p><p>cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;</p><pre class="line-numbers language-none"><code class="language-none">bin&#x2F;schematool -initSchema -dbType mysql -verbos<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>初始化成功之后会在MySQL中创建74张表</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps15.jpg" alt="img" style="zoom:115%;"> <p>6）在hdfs创建hive存储目录（如存在则不用操作）</p><pre class="line-numbers language-none"><code class="language-none">hadoop fs -mkdir &#x2F;tmphadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehousehadoop fs -chmod g+w &#x2F;tmphadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>7）启动hive</p><p><em><strong>*（*</strong></em><em><strong>*3）*</strong></em><em><strong>*启动metastore服务*</strong></em> 前台启动  关闭ctrl+c</p><p>&#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive –service metastore</p><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps16.jpg" alt="img" style="zoom:110%;"> <p>前台启动开启debug日志</p><pre class="line-numbers language-none"><code class="language-none">&#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive --service metastore --hiveconf hive.root.logger&#x3D;DEBUG,console  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps17.jpg" alt="img" style="zoom:100%;"> <p>后台启动 进程挂起  关闭使用jps+ kill -9</p><pre class="line-numbers language-none"><code class="language-none">nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive --service metastore &amp;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps18.jpg" alt="img" style="zoom:100%;"> <p><em><strong>*（*</strong></em><em><strong>*4）*</strong></em><em><strong>*启动hiveserver2服务*</strong></em></p><pre class="line-numbers language-none"><code class="language-none">nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive --service hiveserver2 &amp;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><img src="/wiki/hive%E5%AE%89%E8%A3%85/wps19.jpg" alt="img" style="zoom:120%;"> <img src="/wiki/hive%E5%AE%89%E8%A3%85/wps20.jpg" alt="img" style="zoom:110%;"> <p>beeline客户端连接</p><p>拷贝node1安装包到beeline客户端机器上（node3）</p><pre class="line-numbers language-none"><code class="language-none">scp -r &#x2F;export&#x2F;server&#x2F;apache-hive-3.1.2-bin&#x2F; root@node3:&#x2F;export&#x2F;server&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em><strong>*（5）*</strong></em><em><strong>*hive注释信息中文乱码解决*</strong></em></p><p>以下sql语句均在mysql数据库中执行</p><pre class="line-numbers language-none"><code class="language-none">use hivenode2;show tables;alter table hivenode2.COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;alter table hivenode2.TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;alter table hivenode2.PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ;alter table hivenode2.PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;alter table hivenode2.INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、Hive安装&quot;&gt;&lt;a href=&quot;#一、Hive安装&quot; class=&quot;headerlink&quot; title=&quot;一、Hive安装&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;Hive安装&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;*（1）*&lt;/strong&gt;&lt;/e</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="搭建" scheme="https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
    <category term="Hive" scheme="https://weichunxiu123.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>第一个hexo</title>
    <link href="https://weichunxiu123.github.io/wiki/hello-world/"/>
    <id>https://weichunxiu123.github.io/wiki/hello-world/</id>
    <published>2023-06-07T00:16:18.933Z</published>
    <updated>2023-06-09T00:34:10.904Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    <category term="工具" scheme="https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="初识" scheme="https://weichunxiu123.github.io/tags/%E5%88%9D%E8%AF%86/"/>
    
    <category term="hexo" scheme="https://weichunxiu123.github.io/tags/hexo/"/>
    
  </entry>
  
</feed>
