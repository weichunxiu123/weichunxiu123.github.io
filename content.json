{"pages":[{"title":"Categories","date":"2023-06-07T07:39:09.527Z","path":"categories/index.html","text":""},{"title":"About","date":"2023-06-07T07:39:32.176Z","path":"about/index.html","text":""},{"title":"Tags","date":"2023-06-07T07:39:23.892Z","path":"tags/index.html","text":""}],"posts":[{"title":"kafka-eagle部署与使用","date":"2023-06-19T08:39:03.660Z","path":"wiki/kafka-eagle部署与使用/","text":"Kafka Eagle部署流程与运维监控Kafka Eagle是一个监控系统，监控Kafka集群以及偏移量、生产者、消费者等等。通过Kafka Eagle可以看到当前的消费者组，对于每个组，它们正在使用的主题以及该组在每个主题中的偏移量，消费积压等等。因此对于了解消息队列消费的速度以及消息队列消息写入的速度非常的有用。可以帮助我们调试Kafka的生产者和消费者，也可以对Kafka系统的整体运行情况有一个宏观的了解。 1.1.1具体部署流程（1）下载kafka eagle，下载地址：http://download.kafka-eagle.org/ （2）解压Kafka Eagle cd &#x2F;export&#x2F;software tar -zxvf kafka-eagle-bin-2.1.0.gz -C &#x2F;export&#x2F;server&#x2F; tar -zxvf kafka-eagle-web-2.0.2-bin.tar.gz -C &#x2F;export&#x2F;server&#x2F; mv kafka-eagle-web-2.0.2 kafka-eagle （3）修改&#x2F;etc&#x2F;profile 添加环境变量:JAVA_HOME 和 KE_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export KE_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka-eagle export PATH&#x3D;$PATH:$KE_HOME&#x2F;bin （4）配置 KafkaEagle cd &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;conf vi system-config.properties 1）修改以下配置： kafka.eagle.zk.cluster.alias&#x3D;cluster1 cluster1.zk.list&#x3D;node1:2181,node2:2181,node3:2181 cluster1.kafka.eagle.broker.size&#x3D;3 kafka.eagle.url&#x3D;jdbc:sqlite:&#x2F;export&#x2F;data&#x2F;db&#x2F;ke.db 2）启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录 mkdir &#x2F;export&#x2F;data&#x2F;db （5）启动Eagle &#x2F;export&#x2F;server&#x2F;kafka-eagle&#x2F;bin&#x2F;ke.sh start 1.1.2 kafka Eagle使用（1）Dashboard 展示kafka集群的Topic数量，消费者数量，kafka的Brokers数，以及所属的Zookeeper集群信息。Dashboard信息截图如下： （2）Topic 在Topic模块下，包含创建Topic和展示Topic信息详情。 1）Create 通过创建模块可以创建一个自定义分区的备份数的Topic，如下图所示： 2）List 该模块下列出kafka集群中所有的Topic，包含Topic的分区数，创建时间以及修改时间，如下图所示： ​ ![img](.&#x2F;kafka-eagle部署与使用&#x2F;37 上图中，每一个Topic名称下对应一个详情的超链接，通过该超链接可以查看该Topic的详情。如分区索引号、Leader、Replicas等，如下图所示： 3）Mock 直接通过kafka-eagle来发送信息 或者我们可以通过命令行来消费Topic中的信息； bin&#x2F;kafka-console-consumer.sh --topic tpc_12 --from-beginning --bootstrap-server 192.168.88.161:9092 4）KSQL 该模块通过SQL语句来查询Topic中的信息。 select * from tpc_12 where &#96;partition&#96; in (0) limit 10 （3）Consumers 该模块显示有消费记录的Topic信息，其中包括索引ID号、Active Topic Graph和Offsets Rate Graph （4）Kafka-eagle 监控功能 1）开启kafka监控功能，需要修改kafka的启动脚本，暴露JMX的端口。 查看监控页面 （4）kafka-eagle的可视化监控大屏 展示消费者和生产者当日及最近7天趋势、kafka集群读写速度、kafka集群历史总记录等。","tags":[{"name":"kafka","slug":"kafka","permalink":"https://weichunxiu123.github.io/tags/kafka/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"安装kafka以及kafka命令行操作","date":"2023-06-14T11:33:38.632Z","path":"wiki/安装kafka以及kafka命令行操作/","text":"kafka安装安装Kafka集群 （1）上传安装包 \\1. 修改配置文件 修改文件内容 #进入配置文件目录：cd &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1&#x2F;config #编辑配置文件vi server.properties #为依次增长的:0、1、2、3、4,集群中唯一 id -- 从0开始，每台不能重复，第一块要改的。broker.id&#x3D;0 #数据存储的目录，第二块要改的：log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs #指定 zk 集群地址，第三块要改的zookeeper.connect&#x3D;node1:2181， node2:2181 （2）分发kafla cd &#x2F;export&#x2F;server&#x2F; syncfile &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1 （3）配置环境变量 export KAFKA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1 export PATH&#x3D;$PATH:$KAFKA_HOME&#x2F;bin 注意:还需要分发环境变量！ syncfile &#x2F;etc&#x2F;profile （4）在node2上修改配置文件 vim &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;server.propertie 更改broker.id&#x3D;1 log.dirs&#x3D;&#x2F;export&#x2F;data&#x2F;kafka-logs （5）启动集群（各个节点） 启动： kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;server.properties 关闭： kafka-server-stop.sh stop （6）kafka命令行操作 1）创建topic 基本方式 .&#x2F;kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181 2）手动指定副本的存储位置 kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2 3）查看目前Kafka中的主题 kafka-topics.sh --list --bootstrap-server node1:9092 4）查看topic 列出当前系统中的所有 topic kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list 5）查看 topic 详细信息 kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 kafka-topics.sh --describe --topic tpc_1 --zookeeper node1:2181 6）增加分区数 kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181 7）动态配置topic 参数 添加参数 kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type&#x3D;gzip 8）删除参数 kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type 9）生产消息到Kafka并进行消费 kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1 10）消费信息（从头开始） kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning 11）指定要消费的分区,和要消费的起始 offset kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0 12）配置管理kafka-configs kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181 Kafka基准测试 1.三分区，两副本 kafka-topics.sh --create --topic tpc_3 --partitions 2 --replication-factor 1 --zookeeper node1:2181 kafka-producer-perf-test.sh --topic tpc_3 --num-records 1000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers&#x3D;node1:9092 acks&#x3D;1 \\2. 四分区，两副本 kafka-topics.sh --create --topic tpc_4 --partitions 2 --replication-factor 2 --zookeeper node1:2181 kafka-producer-perf-test.sh --topic tpc_4 --num-records 1000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers&#x3D;node1:9092 acks&#x3D;1 \\3. 七分区，十二副本 kafka-topics.sh --create --topic tpc_7 --partitions 12 --replication-factor 1 --zookeeper node1:2181 kafka-producer-perf-test.sh --topic tpc_7 --num-records 1000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers&#x3D;node1:9092 acks&#x3D;1 由此可知：在一定限度内：副本增多，吞吐量变大。 参考资料 [1]kafka安装配置文档.md","tags":[{"name":"kafka","slug":"kafka","permalink":"https://weichunxiu123.github.io/tags/kafka/"},{"name":"kafka命令行操作","slug":"kafka命令行操作","permalink":"https://weichunxiu123.github.io/tags/kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"zookeeper安装以及zookeeperJavaAPI操作命令","date":"2023-06-14T11:02:56.070Z","path":"wiki/安装zookeeper以及zookeeperJavaAPI操作命令/","text":"zookeeper安装安装前需要安装好jdk，检测集群时间是否同步，检测防火墙是否关闭，检测主机 ip映射有没有配置 （1）在node1上切换到 &#x2F;export&#x2F;server 目录下，上传zookeeper压缩包并解压，设置一个软连接。 切换到server目录下： cd &#x2F;export&#x2F;server 解压zookeeper压缩包： tar -zxvf zookeeper.tar.gz -C &#x2F;export&#x2F;server&#x2F; 设置软连接： ln -s &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; zookeeper \\1. 修改环境变量（三台都修改） vi &#x2F;etc&#x2F;profile export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin source &#x2F;etc&#x2F;profile \\2. 修改zookeeper配置文件 切换到conf&#x2F;目录下 cd &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 复制zoo_sample.cfg文件，文件名为zoo.cfg cp zoo_sample.cfg zoo.cfg 创建文件： mkdir -p &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F; vim zoo.cfg 填充以下内容： #Zookeeper的数据存放目录 dataDir &#x3D; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F; # 保留多少个快照 autopurge.snapRetainCount &#x3D; 3 # 日志多少小时清理一次 autopurge.purgeInterval &#x3D; 1 # 集群中服务器地址 server.1 &#x3D; node1:2888:3888 server.2 &#x3D; node2:2888:3888 server.3 &#x3D; node3:2888:3888 \\3. 添加myid配置 在node1主机的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1 echo 1 &gt; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid \\4. 安装包分发并修改myid的值 在node1主机上，将安装包分发到其他机器 第一台机器上面执行以下两个命令 cd &#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; root@node2:&#x2F;export&#x2F;server&#x2F; 分发node2 scp -r &#x2F;export&#x2F;server&#x2F;zookeeper-3.4.6&#x2F; root@node3:&#x2F;export&#x2F;server&#x2F; 分发node3 ln -s zookeeper-3.4.6&#x2F; zookeeper 建立软连接（node2 node3） echo 2 &gt; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid （node2上执行） echo 3 &gt; &#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid （node3上执行） \\5. 三台机器启动zookeeper服务 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start （2） 三台主机分别查看启动状态 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status \\6. 编写一个脚本批量启动node1，2，3的zookeeper （1）创建shell目录 mkdir &#x2F;export&#x2F;shell （2）再此目录下建立一个zkall.sh文件 vim zkall.sh 添加以下内容： #!&#x2F;bin&#x2F;bash case $1 in &quot;start&quot;)&#123; for i in node1 node2 node3 do echo ---------- zookeeper $i 启动 ------------ ssh $i &quot;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start&quot; done &#125;;; &quot;stop&quot;)&#123; for i in node1 node2 node3 do echo ---------- zookeeper $i 停止 ------------ ssh $i &quot;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh stop&quot; done &#125;;; &quot;status&quot;)&#123; for i in node1 node2 node3 do echo ---------- zookeeper $i 状态 ------------ ssh $i &quot;&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status&quot; done &#125;;; Esac （3）配置zk脚本环境变量、zookeeper的环境变量 vi &#x2F;etc&#x2F;profile #ZOOKEEPER_SHELL_HOME export ZKS_HOME&#x3D;&#x2F;export&#x2F;shell&#x2F; export PATH&#x3D;$PATH:$ZKS_HOME export ZK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$&#123;ZK_HOME&#125;&#x2F;bin:$PATH 让环境变量生效 source &#x2F;etc&#x2F;profile （4） 设置环境路径 vim .bashrc （5） 增加可执行权限 chmod +x zkall.sh （6）启动测试 zkall.sh start 查看状态 zkall.sh status （7）zookeeper服务器常用命令 启动 ZooKeeper 服务 .&#x2F;zkServer.sh start 查看 ZooKeeper 服务状态 .&#x2F;zkServer.sh status 停止 ZooKeeper 服务 .&#x2F;zkServer.sh stop 重启 ZooKeeper 服务 .&#x2F;zkServer.sh restart \\7. Zookeerper命令操作 （1）连接ZooKeeper服务端 .&#x2F;zkCli.sh -server node1:2181 \\8. ZooKeeper JavaAPI 操作 （1）建立连接 client &#x3D; CuratorFrameworkFactory.builder() .connectString(&quot;192.168.88.161:2181&quot;) .sessionTimeoutMs(60 * 1000) .connectionTimeoutMs(15 * 1000 .retryPolicy(retryPolicy) .namespace(&quot;bigdata&quot;) .build(); client.start(); （2）创建节点（带有数据） String path &#x3D; client.create().forPath(&quot;&#x2F;app2&quot;, &quot;hehe&quot;.getBytes()) System.out.println(path); （3）查询结点 byte[] data &#x3D; client.getData().forPath(&quot;&#x2F;app2&quot;) System.out.println(new String(data)); （4）修改数据 client.setData().forPath(&quot;&#x2F;app2&quot;, &quot;bigdata&quot;.getBytes()) （5）删除节点 client.delete().forPath(&quot;&#x2F;app1&quot;) （6）Watch事件监听 TreeCache treeCache &#x3D; new TreeCache(client,&quot;&#x2F;app2&quot;); treeCache.getListenable().addListener(new TreeCacheListener() &#123; @Override public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception &#123; System.out.println(&quot;节点变化了&quot;); System.out.println(event); &#125; &#125;); treeCache.start(); （7）分布式锁实现 public static void main(String[] args) &#123; Ticket12306_old ticket12306_old &#x3D; new Ticket12306_old(); Thread t1 &#x3D; new Thread(ticket12306_old,&quot;携程&quot;); Thread t2 &#x3D; new Thread(ticket12306_old,&quot;飞猪&quot;); t1.start(); t2.start(); &#125; 参考资料 [1]zookeeper安装文档.md [2]zookeeper.ppt","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://weichunxiu123.github.io/tags/zookeeper/"},{"name":"zookeeperJavaAPI操作命令","slug":"zookeeperJavaAPI操作命令","permalink":"https://weichunxiu123.github.io/tags/zookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"sqoop安装以及导入和导出","date":"2023-06-14T10:14:47.696Z","path":"wiki/sqoop安装以及导入和导出/","text":"sqoop安装（1）首先需要有java、mysql、hadoop和hive环境 （2）将以下下载到CentOS系统中，对其解压 （3）解压在&#x2F;export&#x2F;server&#x2F; tar xzf apache-flume-1.9.0-bin.tar.gz -C &#x2F;export&#x2F;server&#x2F; （4）添加软连接： ln -s apache-flume-1.9.0-bin flume （5）配置文件修改： vim &#x2F;etc&#x2F;profile vi sqoop-env.sh （5）加入mysql的jdbc驱动包 cp &#x2F;export&#x2F;server&#x2F;hive&#x2F;lib&#x2F;mysql-connector-java-5.1.32.jar $SQOOP_HOME&#x2F;lib&#x2F; （7）验证启动 bin&#x2F;sqoop list-databases --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F; --username root --password hadoop 本命令会列出所有mysql的数据库。 到这里，整个Sqoop安装工作完成。 sqoop导入（1）Sqoop测试表数据 在mysql中创建数据库userdb ，然后执行并参考资料中的sql脚本。 创建三张表: emp 雇员表、 emp_add 雇员地址表、 emp_conn 雇员联系表。 （2）全量导入mysql表数据到HDFS 下面的命令用于从MySQL数据库服务器中的emp表导入HDFS #example1-mysql-hdfs-start bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --delete-target-dir \\ --target-dir &#x2F;sqoop&#x2F;sqoopresult \\ --table emp --m 1 #example2-mysql-hdfs-terminated bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --target-dir &#x2F;sqoop&#x2F;sqoopresult2 \\ --fields-terminated-by &#39;\\t&#39; \\ --table emp --m 1 #example3-mysql-hdfs-split bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --target-dir &#x2F;sqoop&#x2F;sqoopresult3 \\ --fields-terminated-by &#39;\\t&#39; \\ --split-by id \\ --table emp --m 2 （3）全量导入mysql表数据到HIVE 先复制表结构到hive中再导入数据，在hive中新建数据库sqoop_test用于测试。 create database if not exists sqoop_test comment &quot;this is sqoop db&quot; with dbproperties(&#39;createdBy&#39;&#x3D;&#39;yzl&#39;); use sqoop_test; show tables; desc formatted emp_add_sp; 将关系型数据的表结构复制到hive中 #example4-1-mysql-hive-structure bin&#x2F;sqoop create-hive-table \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --table emp_add \\ --username root \\ --password hadoop \\ --hive-table sqoop_test.emp_add_sp 从关系数据库导入文件到hive中 #example4-2-mysql-hive-data bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --table emp_add \\ --hive-table sqoop_test.emp_add_sp \\ --hive-import \\ --m 1 （4）导入表数据子集(where过滤) #example6-mysql-hdfs-where bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --where &quot;city &#x3D;&#39;sec-bad&#39;&quot; \\ --target-dir &#x2F;sqoop&#x2F;wherequery \\ --table emp_add --m 1 （5）导入表数据子集(query查询 #example7-mysql-hdfs-query bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --target-dir &#x2F;sqoop&#x2F;wherequery2 \\ --query &#39;select id,name,deg from emp WHERE id&gt;1203 and $CONDITIONS&#39; \\ --split-by id \\ --fields-terminated-by &#39;\\001&#39; \\ --m 2 （6）增量导入 Append模式增量导入，执行以下指令先将我们之前的数据导入。 #example8-1-mysql-hdfs-append bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --target-dir &#x2F;sqoop&#x2F;appendresult \\ --table emp --m 1 使用hdfs dfs -cat查看生成的数据文件，发现数据已经导入到hdfs中 然后在mysql的emp表中插入2条数据: insert into &#96;userdb&#96;.&#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;deg&#96;, &#96;salary&#96;, &#96;dept&#96;) values (&#39;1206&#39;, &#39;allen&#39;, &#39;admin&#39;, &#39;30000&#39;, &#39;tp&#39;); insert into &#96;userdb&#96;.&#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;deg&#96;, &#96;salary&#96;, &#96;dept&#96;) values (&#39;1207&#39;, &#39;woon&#39;, &#39;admin&#39;, &#39;40000&#39;, &#39;tp&#39;); 执行如下的指令，实现增量的导入: #example8-2-mysql-hdfs-append bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --table emp --m 1 \\ --target-dir &#x2F;sqoop&#x2F;appendresult \\ --incremental append \\ --check-column id \\ --last-value 1205 Lastmodified模式增量导入 （1）首先创建一个customer表，指定一个时间戳字段： create table customertest(id int,name varchar(20),last_mod timestamp default current_timestamp on update current_timestamp); 此处的时间戳设置为在数据的产生和更新时都会发生改变. （2）插入如下记录: insert into customertest(id,name) values(1,&#39;neil&#39;); insert into customertest(id,name) values(2,&#39;jack&#39;); insert into customertest(id,name) values(3,&#39;martin&#39;); insert into customertest(id,name) values(4,&#39;tony&#39;); insert into customertest(id,name) values(5,&#39;eric&#39;); （3）此时执行sqoop指令将数据导入hdfs: #example9-1-mysql-hdfs-Lastmodified bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --target-dir &#x2F;sqoop&#x2F;lastmodifiedresult \\ --table customertest --m 1 （4）再次插入一条数据进入customertest表 insert into customertest(id,name) values(6,&#39;james&#39;) （5）使用incremental的方式进行增量的导入: #example9-2-mysql-hdfs-Lastmodified bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --table customertest \\ --target-dir &#x2F;sqoop&#x2F;lastmodifiedresult \\ --check-column last_mod \\ --incremental lastmodified \\ --last-value &quot;2019-05-28 18:42:06&quot; \\ --m 1 \\ --append Lastmodified模式:append、merge-key 使用lastmodified模式进行增量处理要指定增量数据是以append模式(附加) 和merge-key(合并)模式添加 。 下面演示使用merge-by的模式进行增量更新 ： （1）我们去更新 id为1的name字段。 update customertest set name &#x3D; &#39;Neil&#39; where id &#x3D; 1; 更新之后，这条数据的时间戳会更新为更新数据时的系统时间. （2）执行如下指令，把id字段作为merge-key: #example10-mysql-hdfs-merge-key bin&#x2F;sqoop import \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root \\ --password hadoop \\ --table customertest \\ --target-dir &#x2F;sqoop&#x2F;lastmodifiedresult \\ --check-column last_mod \\ --incremental lastmodified \\ --last-value &quot;2019-05-28 18:42:06&quot; \\ --m 1 \\ --merge-key id sqoop导出（1）准备HDFS数据 在HDFS文件系统中“&#x2F;emp&#x2F;”目录的下创建一个文件emp_data.txt：mkdir &#x2F;export&#x2F;data&#x2F;sqoop-data&#x2F;emp&#x2F; vim emp_data.txt 添加如下内容： 1201,gopal,manager,50000,TP 1202,manisha,preader,50000,TP 1203,kalil,php dev,30000,AC 1204,prasanth,php dev,30000,AC 1205,kranthi,admin,20000,TP 1206,satishp,grpdes,20000,GR 上传至hdfs： hadoop fs -mkdir &#x2F;sqoop&#x2F;emp_data hadoop fs -put emp_data.txt &#x2F;sqoop&#x2F;emp_data （2）手动创建mysql中的目标表 mysql&gt; use userdb; mysql&gt; create table employee ( id int not null primary key, name varchar(20), deg varchar(20),salary int,dept varchar(10)); （3）执行导出命令： bin&#x2F;sqoop export --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb --username root --password hadoop --table employee1 --columns id,name,deg,salary,dept --export-dir &#x2F;sqoop&#x2F;emp_data&#x2F; 更新导出（updateonly模式） （1）准备HDFS数据 在HDFS文件系统中&#x2F;sqoop&#x2F;updateonly_1&#x2F;目录的下创建一个文件updateonly_1.txt： 1201,gopal,manager,50000 1202,manisha,preader,50000 1203,kalil,php dev,30000 （2）手动创建mysql中的目标表 mysql&gt; USE userdb; mysql&gt; CREATE TABLE updateonly ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), deg VARCHAR(20),salary INT); （3）先执行全部导出操作 bin&#x2F;sqoop export --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb --username root --password hadoop --table allowinsert --export-dir &#x2F;sqoop&#x2F;allowinsert_1&#x2F; 查看此时的数据 （4）新增文件 创建文件allowinsert_2.txt。修改前三条数据并且新增了一条记录。上传至 &#x2F;sqoop&#x2F;allowinsert_2&#x2F;目录下： 1201,gopal,manager,1212 1202,manisha,preader,1313 1203,kalil,php dev,1414 1204,allen,java,1515 2.6 执行更新导出 bin&#x2F;sqoop export \\ --connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb \\ --username root --password hadoop \\ --table allowinsert \\ --export-dir &#x2F;sqoop&#x2F;allowinsert_2&#x2F; \\ --update-key id \\ --update-mode allowinsert 查看最终结果 参考资料 [1]sqoop配置文档.md","tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://weichunxiu123.github.io/tags/sqoop/"},{"name":"导入","slug":"导入","permalink":"https://weichunxiu123.github.io/tags/%E5%AF%BC%E5%85%A5/"},{"name":"导出","slug":"导出","permalink":"https://weichunxiu123.github.io/tags/%E5%AF%BC%E5%87%BA/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Fume测试以及综合案例","date":"2023-06-14T08:03:25.939Z","path":"wiki/flume测试以及综合案例/","text":"Flume测试以及综合案例（1）拦截器Host Interceptor拦截器将运行agent的hostname 或者 IP地址写入到事件的headers中。 1）在 &#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf 中添加example8-interceptor.conf，内容如下： # 定义agent名称为a1 # 设置3个组件的名称 a1.sources &#x3D; r1 a1.sinks &#x3D; k1 a1.channels &#x3D; c1 # 配置source类型为NetCat,监听地址为本机，端口为44444 a1.sources.r1.type &#x3D; netcat a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 # 配置拦截器为host a1.sources.r1.interceptors &#x3D; i1 a1.sources.r1.interceptors.i1.type &#x3D; host # 配置sink类型为Logger a1.sinks.k1.type &#x3D; logger # 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 100 # 将source和sink绑定到channel上 a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1 2）、在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume（复制链接进入相同路径下，输入nc node1 44444输入内容进行测试，结果显示IP地址，测试成功） bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console （2）Timestamp Interceptor拦截器 将当前时间写入到事件的headers中。 1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-inteptor.conf中： a1.sources.r1.interceptors &#x3D; i1 i2 a1.sources.r1.interceptors.i1.type &#x3D; host a1.sources.r1.interceptors.i2.type &#x3D; timestamp 2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume （与上一测试使用相同方法进行测试，结果显示timestanp，测试成功） bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console （2）Static Interceptor 运行用户对所有的事件添加固定的header 1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-interceptor.conf中： a1.sources.r1.interceptors &#x3D; i1 i2 i3 a1.sources.r1.interceptors.i1.type &#x3D; host a1.sources.r1.interceptors.i2.type &#x3D; timestamp a1.sources.r1.interceptors.i3.type &#x3D; static a1.sources.r1.interceptors.i3.key &#x3D; datacenter a1.sources.r1.interceptors.i3.value &#x3D; NEW_YORK 2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：（结果显示datacenter，则测试成功） bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console （4）UUID Interceptor 用于每个events header中生成一个UUID字符串，可以在sink中读取并使用 1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-interceptor.conf中： a1.sources.r1.interceptors &#x3D; i1 i2 i3 i4 a1.sources.r1.interceptors.i1.type &#x3D; host a1.sources.r1.interceptors.i2.type &#x3D; timestamp a1.sources.r1.interceptors.i3.type &#x3D; static a1.sources.r1.interceptors.i3.key &#x3D; datacenter a1.sources.r1.interceptors.i3.value &#x3D; NEW_YORK a1.sources.r1.interceptors.i4.type &#x3D; org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder 2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume：（显示字符串，则测试成功） bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console （5）Search and Replace Interceptor 用于将events中的正则匹配到的内容做相应的替换。 1）将以下内容添加到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf的example8-interceptor.conf中： a1.sources.r1.interceptors &#x3D; i1 i2 i3 i4 i5 a1.sources.r1.interceptors.i1.type &#x3D; host a1.sources.r1.interceptors.i2.type &#x3D; timestamp a1.sources.r1.interceptors.i3.type &#x3D; static a1.sources.r1.interceptors.i3.key &#x3D; datacenter a1.sources.r1.interceptors.i3.value &#x3D; NEW_YORK a1.sources.r1.interceptors.i4.type &#x3D; org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder a1.sources.r1.interceptors.i5.type &#x3D; search_replace a1.sources.r1.interceptors.i5.searchPattern &#x3D; \\\\d&#123;6&#125; a1.sources.r1.interceptors.i5.replaceString &#x3D; ******1234 2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example8-interceptor.conf -Dflume.root.logger&#x3D;INFO,console 3）复制node1链接，进入&#x2F;export&#x2F;server&#x2F;flume路径下，输入 nc node1 44444 （6）自定义拦截器 1）在ldea中添加自定义拦截器 2）将项目打包复制到node1的&#x2F;export&#x2F;server&#x2F;flume&#x2F;lib路径下 3）在node1、node2中添加上游服务器设置example9-1-taildir-f-avro-interceptor.conf a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.type &#x3D; TAILDIR a1.sources.r1.channels &#x3D; c1 a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.json a1.sources.r1.filegroups &#x3D; g1 a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;app&#x2F;event.* #提高吞吐量 a1.sources.r1.batchSize &#x3D; 1000 #动态的header-keys eg：filepath&#x3D;&#x2F;..&#x2F;..&#x2F;..&#x2F; a1.sources.r1.fileHeaderKey &#x3D; filepath #拦截器配置,添加自定义拦截器 a1.sources.r1.interceptors &#x3D; i1 a1.sources.r1.interceptors.i1.type &#x3D; ccjz.rgzn.flume.EventTimestampInterceptor$EventTimestampInterceptorBuilder a1.sources.r1.interceptors.i1.tsFiledName &#x3D; timeStamp a1.sources.r1.interceptors.i1.keyName &#x3D; timestamp a1.sources.r1.interceptors.i1.toEncryFieldName &#x3D; account a1.channels.c1.type &#x3D; file #本机数据汇集检查点、event存储目录 a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint a1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data a1.channels.c1.transactionCapacity &#x3D; 2000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; avro a1.sinks.k1.batch-size &#x3D; 1000 #下游目标主机、端口 a1.sinks.k1.hostname &#x3D; node3 a1.sinks.k1.port &#x3D; 44444 4）在node3中添加下游服务器配置example9-2-avro-f-hdfs-interceptor.conf a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 #下游数据汇集avro source a1.sources.r1.type &#x3D; avro a1.sources.r1.channels &#x3D; c1 a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 a1.sources.r1.threads &#x3D; 10 a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; file a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint a1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data a1.channels.c1.transactionCapacity &#x3D; 2000 #hdfs sink a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; hdfs a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata-interceptor&#x2F;%Y-%m-%d&#x2F;%H&#x2F; #eg：文件名 logdata_34438hxfd.log，在滚动时，logdata_34438hxfd.log.tmp a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k1.hdfs.fileSuffix &#x3D; .log #三个条件没有优先级，谁先达到就进行滚动 #按时间间隔滚动 a1.sinks.k1.hdfs.rollInterval &#x3D; 0 #按文件大小滚动 256MB a1.sinks.k1.hdfs.rollSize &#x3D; 268435456 #按event条数滚动 a1.sinks.k1.hdfs.rollCount &#x3D; 100000 a1.sinks.k1.hdfs.batchSize &#x3D; 1000 a1.sinks.k1.hdfs.codeC &#x3D; gzip a1.sinks.k1.hdfs.fileType &#x3D; CompressedStream 5）在node1和node2中创建日志目录来生成模拟日志数据 mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;app&#x2F; mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;app&#x2F; 6）在node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example9-2-avro-f-hdfs-interceptor.conf -Dflume.root.logger&#x3D;DEBUG,console 7）启动node1和node2上的flume agent bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example9-1-taildir-f-avro-interceptor.conf 8）在hdfs上查看是否采集到数据 （7）Channel选择器 Replicating Channel Selector中c2是一个可选的channel，写入c2失败的话会被忽略，c1没有标记为可选，如果写入c1失败会导致事务的失败 1）在 &#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf 中添加example10-channel-replicating.conf， a1.sources &#x3D; r1 a1.sinks &#x3D; k1 k2 a1.channels &#x3D; c1 c2 a1.sources.r1.type &#x3D; exec a1.sources.r1.channels &#x3D; c1 c2 a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;logdata&#x2F;access.log a1.sources.r1.batchSize &#x3D; 1000 a1.sources.r1.selector.type &#x3D; replicating a1.sources.r1.selector.optional &#x3D; c2 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.channels.c2.type &#x3D; memory a1.channels.c2.capacity &#x3D; 1000 a1.channels.c2.transactionCapacity &#x3D; 1000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; hdfs a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata_c1&#x2F;%Y-%m-%d&#x2F;%H&#x2F; a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k1.hdfs.fileSuffix &#x3D; .log a1.sinks.k1.hdfs.rollInterval &#x3D; 0 a1.sinks.k1.hdfs.rollSize &#x3D; 268435456 a1.sinks.k1.hdfs.rollCount &#x3D; 0 a1.sinks.k1.hdfs.batchSize &#x3D; 1000 a1.sinks.k1.hdfs.fileType &#x3D; DataStream a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true a1.sinks.k2.channel &#x3D; c2 a1.sinks.k2.type &#x3D; hdfs a1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata_c2&#x2F;%Y-%m-%d&#x2F;%H&#x2F; a1.sinks.k2.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k2.hdfs.fileSuffix &#x3D; .log a1.sinks.k2.hdfs.rollInterval &#x3D; 0 a1.sinks.k2.hdfs.rollSize &#x3D; 268435456 a1.sinks.k2.hdfs.rollCount &#x3D; 0 a1.sinks.k2.hdfs.batchSize &#x3D; 1000 a1.sinks.k2.hdfs.fileType &#x3D; DataStream a1.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true 2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example10-channel-replica ting.conf -Dflume.root.logger&#x3D;INFO,console （8）Multiplexing Channel Selector 可以根据event中的一个指定key的value来决定这条消息会写入哪个channel。 1）在 &#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf 中添加example11-channel-Multiplexing.conf，内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 c2 a1.sinks &#x3D; k1 k2 a1.sources.r1.type &#x3D; TAILDIR a1.sources.r1.channels &#x3D; c1 c2 a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.json a1.sources.r1.filegroups &#x3D; g1 g2 a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web.* a1.sources.r1.filegroups.g2 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx.* a1.sources.r1.headers.g1.logtype &#x3D; web a1.sources.r1.headers.g2.logtype &#x3D; wx a1.sources.r1.selector.type &#x3D; multiplexing a1.sources.r1.selector.header &#x3D; logtype a1.sources.r1.selector.mapping.web &#x3D; c1 a1.sources.r1.selector.mapping.wx &#x3D; c2 a1.sources.r1.selector.default &#x3D; c2 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.channels.c2.type &#x3D; memory a1.channels.c2.capacity &#x3D; 1000 a1.channels.c2.transactionCapacity &#x3D; 1000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; hdfs a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;%&#123;logtype&#125;&#x2F;%Y-%m-%d&#x2F;%H&#x2F; a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k1.hdfs.fileSuffix &#x3D; .log a1.sinks.k1.hdfs.rollInterval &#x3D; 0 a1.sinks.k1.hdfs.rollSize &#x3D; 268435456 a1.sinks.k1.hdfs.rollCount &#x3D; 0 a1.sinks.k1.hdfs.batchSize &#x3D; 1000 a1.sinks.k1.hdfs.fileType &#x3D; DataStream a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true a1.sinks.k2.channel &#x3D; c2 a1.sinks.k2.type &#x3D; hdfs a1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;%&#123;logtype&#125;&#x2F;%Y-%m-%d&#x2F;%H&#x2F; a1.sinks.k2.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k2.hdfs.fileSuffix &#x3D; .log a1.sinks.k2.hdfs.rollInterval &#x3D; 0 a1.sinks.k2.hdfs.rollSize &#x3D; 268435456 a1.sinks.k2.hdfs.rollCount &#x3D; 0 a1.sinks.k2.hdfs.batchSize &#x3D; 1000 a1.sinks.k2.hdfs.fileType &#x3D; DataStream a1.sinks.k2.hdfs.useLocalTimeStamp &#x3D; true 2）在&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example11-channel-Multiplexing.conf -Dflume.root.logger&#x3D;INFO,console 3）在hdfs上查看 （9）Sink处理器实例 Failover Sink Processor是一组中只有优先级高的那个sink在工作，另一个是等待中，如果高优先级的sink发送数据失败，则专用低优先级的sink去工作，并且，在配置时间penalty之后，还会尝试用高优先级的去发送数据 1）在node1上配置上游服务器example12-1-sink-failover.conf，内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 k2 a1.sources.r1.type &#x3D; exec a1.sources.r1.channels &#x3D; c1 a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;logdata&#x2F;access.log a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; avro a1.sinks.k1.hostname &#x3D; node2 a1.sinks.k1.port &#x3D; 44444 a1.sinks.k1.batch-size &#x3D; 1000 a1.sinks.k2.channel &#x3D; c1 a1.sinks.k2.type &#x3D; avro a1.sinks.k2.hostname &#x3D; node3 a1.sinks.k2.port &#x3D; 44444 a1.sinks.k2.batch-size &#x3D; 1000 a1.sinkgroups &#x3D; g1 a1.sinkgroups.g1.sinks &#x3D; k1 k2 a1.sinkgroups.g1.processor.type &#x3D; failover a1.sinkgroups.g1.processor.priority.k1 &#x3D; 200 a1.sinkgroups.g1.processor.priority.k2 &#x3D; 100 a1.sinkgroups.g1.processor.maxpenalty &#x3D; 5000 2）在node2、node3上配置下游服务器example12-2-sink-failover.conf，内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.type &#x3D; avro a1.sources.r1.channels &#x3D; c1 a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 a1.sources.r1.threads &#x3D; 10 a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.sinks.k1.type &#x3D; logger a1.sinks.k1.channel &#x3D; c1 3）在node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example12-2-sink-failover.conf -Dflume.root.logger&#x3D;DEBUG,console 4）启动node1和node2上的flume agent bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example12-1-sink-failover.Conf （10）Load balancing Sink Processor 允许channel中的数据在一组sink中的多个sink之间进行交替 1）在node1上配置上游服务器example13-1-sink-loadbalance.conf，内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 k2 a1.sources.r1.type &#x3D; exec a1.sources.r1.channels &#x3D; c1 a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;logdata&#x2F;access.log a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; avro a1.sinks.k1.hostname &#x3D; node2 a1.sinks.k1.port &#x3D; 44444 a1.sinks.k1.batch-size &#x3D; 1000 a1.sinks.k2.channel &#x3D; c1 a1.sinks.k2.type &#x3D; avro a1.sinks.k2.hostname &#x3D; node3 a1.sinks.k2.port &#x3D; 44444 a1.sinks.k2.batch-size &#x3D; 1000 a1.sinkgroups &#x3D; g1 a1.sinkgroups.g1.sinks &#x3D; k1 k2 a1.sinkgroups.g1.processor.type &#x3D; load_balance a1.sinkgroups.g1.processor.backoff &#x3D; true a1.sinkgroups.g1.processor.selector &#x3D; round_robin 2）在node2、node3上配置下游服务器example13-2-sink-loadbalance.conf内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.type &#x3D; avro a1.sources.r1.channels &#x3D; c1 a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 a1.sources.r1.threads &#x3D; 10 a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.sinks.k1.type &#x3D; logger a1.sinks.k1.channel &#x3D; c1 3）在node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example13-2-sink-loadbala nce.conf -Dflume.root.logger&#x3D;DEBUG,console 4）启动node1和node2上的flume agent bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example13-1-sink-loadbalance.conf （11）Flume综合实战案例1）准备一个MySQL服务器，创建一个库：realtimedw，字符集选择utf8，防止中文乱码： 2）将事先准备好的realtimedw.sql、t_md_areas.sql两个sql文件导入到realtimedw库中： 3）在node1中创建一个目录，用来存放配置文件： mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit 4）修改log_gen_app.jar、log_gen_wx.jar的配置文件中的other.properties,内容如下： #logger,kafka sink.type&#x3D;logger #roll console dayroll logger.type&#x3D;dayroll initdata.releasechannel&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;releasechannel.txt initdata.phoneinfo&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;phoneinfo.txt initdata.eventIds&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;eventIds.txt init.user.area&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit&#x2F;area.txt db.url&#x3D;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;realtimedw? useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf8&amp;useSSL&#x3D;false db.user&#x3D;root db.password&#x3D;hadoop # max concurrent accessor amount online.max.num&#x3D;1000 5）log_gen_app.jar的配置文件log4j.properties，内容如下： log4j.rootLogger&#x3D;INFO,trace log4j.appender.trace&#x3D;org.apache.log4j.ConsoleAppender log4j.appender.trace.Threshold&#x3D;DEBUG log4j.appender.trace.ImmediateFlush&#x3D;true log4j.appender.trace.Target&#x3D;System.out log4j.appender.trace.layout&#x3D;org.apache.log4j.PatternLayout log4j.appender.trace.layout.ConversionPattern&#x3D;[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n log4j.logger.console &#x3D; INFO,console log4j.additivity.console&#x3D;false log4j.appender.console&#x3D;org.apache.log4j.ConsoleAppender log4j.appender.console.Threshold&#x3D;DEBUG log4j.appender.console.ImmediateFlush&#x3D;true log4j.appender.console.Target&#x3D;System.out log4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern&#x3D;%m%n # log4j.logger.roll &#x3D; INFO,rollingFile # log4j.additivity.roll&#x3D;false # log4j.appender.rollingFile&#x3D;org.apache.log4j.RollingFileAppender # log4j.appender.rollingFile.Threshold&#x3D;DEBUG # log4j.appender.rollingFile.ImmediateFlush&#x3D;true # log4j.appender.rollingFile.Append&#x3D;true # log4j.appender.rollingFile.File&#x3D;&#x2F;loggen&#x2F;logdata&#x2F;wx&#x2F;event.log # log4j.appender.rollingFile.MaxFileSize&#x3D;120MB # log4j.appender.rollingFile.MaxBackupIndex&#x3D;50 # log4j.appender.rollingFile.layout&#x3D;org.apache.log4j.PatternLayout # log4j.appender.rollingFile.layout.ConversionPattern&#x3D;%m%n log4j.logger.dayroll &#x3D; INFO,DailyRolling log4j.additivity.dayroll&#x3D;false log4j.appender.DailyRolling&#x3D;org.apache.log4j.DailyRollingFileAppender log4j.appender.DailyRolling.File&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;gen_logdata&#x2F;event_log_app log4j.appender.DailyRolling.DatePattern&#x3D;yyyy-MM-dd&#39;.log&#39; log4j.appender.DailyRolling.layout&#x3D;org.apache.log4j.PatternLayout log4j.appender.DailyRolling.layout.ConversionPattern&#x3D;%m%n 6）修改log_gen_wx.jar的配置文件log4j.properties log4j.rootLogger&#x3D;INFO,trace log4j.appender.trace&#x3D;org.apache.log4j.ConsoleAppender log4j.appender.trace.Threshold&#x3D;DEBUG log4j.appender.trace.ImmediateFlush&#x3D;true log4j.appender.trace.Target&#x3D;System.out log4j.appender.trace.layout&#x3D;org.apache.log4j.PatternLayout log4j.appender.trace.layout.ConversionPattern&#x3D;[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n log4j.logger.console &#x3D; INFO,console log4j.additivity.console&#x3D;false log4j.appender.console&#x3D;org.apache.log4j.ConsoleAppender log4j.appender.console.Threshold&#x3D;DEBUG log4j.appender.console.ImmediateFlush&#x3D;true log4j.appender.console.Target&#x3D;System.out log4j.appender.console.layout&#x3D;org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern&#x3D;%m%n # log4j.logger.roll &#x3D; INFO,rollingFile # log4j.additivity.roll&#x3D;false # log4j.appender.rollingFile&#x3D;org.apache.log4j.RollingFileAppender # log4j.appender.rollingFile.Threshold&#x3D;DEBUG\\# log4j.appender.rollingFile.ImmediateFlush&#x3D;true # log4j.appender.rollingFile.Append&#x3D;true # log4j.appender.rollingFile.File&#x3D;&#x2F;loggen&#x2F;logdata&#x2F;wx&#x2F;event.log # log4j.appender.rollingFile.MaxFileSize&#x3D;120MB # log4j.appender.rollingFile.MaxBackupIndex&#x3D;50 # log4j.appender.rollingFile.layout&#x3D;org.apache.log4j.PatternLayout # log4j.appender.rollingFile.layout.ConversionPattern&#x3D;%m%n log4j.logger.dayroll &#x3D; INFO,DailyRolling log4j.additivity.dayroll&#x3D;false log4j.appender.DailyRolling&#x3D;org.apache.log4j.DailyRollingFileAppender log4j.appender.DailyRolling.File&#x3D;&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;gen_logdata&#x2F;event_log_wx log4j.appender.DailyRolling.DatePattern&#x3D;yyyy-MM-dd&#39;.log&#39; log4j.appender.DailyRolling.layout&#x3D;org.apache.log4j.PatternLayout log4j.appender.DailyRolling.layout.ConversionPattern&#x3D;%m%n 7）将以下文件上传到&#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;loginit路径下： 8）给shell文件授权，并启动日志生成器： chmod +x genapplog.sh chmod +x genwxlog.sh sh genapplog.sh sh genwxlog.sh 9）查看jps 10）进入&#x2F;export&#x2F;data&#x2F;flume-example&#x2F;gen_logdata下查看日志文件生成效果： 11）在node1、node2中配置上游服务器example14-1-Comprehensive-practical.conf，内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 k2 a1.sources.r1.type &#x3D; TAILDIR a1.sources.r1.channels &#x3D; c1 a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.json a1.sources.r1.filegroups &#x3D; g1 a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;gen_logdata&#x2F;event_.* a1.sources.r1.batchSize &#x3D; 1000 a1.sources.r1.interceptors &#x3D; i1 i2 i3 a1.sources.r1.interceptors.i1.type &#x3D; ccjz.rgzn.flume.EncryptSpecifiedFieldInterceptor$EncryptInterceptorBuilder a1.sources.r1.interceptors.i1.toEncryFieldName &#x3D; account a1.sources.r1.interceptors.i2.type &#x3D; ccjz.rgzn.flume.EventTimeStampExtractInterceptor$EventTimestampInterceptorBuilder a1.sources.r1.interceptors.i2.tsFiledName &#x3D; timeStamp a1.sources.r1.interceptors.i2.keyName &#x3D; timestamp \\#拥有openid的是wx小程序用户日志 a1.sources.r1.interceptors.i3.type &#x3D; ccjz.rgzn.flume.LogTypeInterceptor$LogTypeInterceptorBuilder a1.sources.r1.interceptors.i3.flag.fieldname &#x3D; openid a1.sources.r1.interceptors.i3.headerKey &#x3D; logtype a1.channels.c1.type &#x3D; file a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint a1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data a1.channels.c1.transactionCapacity &#x3D; 2000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; avro a1.sinks.k1.hostname &#x3D; node2 a1.sinks.k1.port &#x3D; 44444 a1.sinks.k1.batch-size &#x3D; 1000 a1.sinks.k2.channel &#x3D; c1 a1.sinks.k2.type &#x3D; avro a1.sinks.k2.hostname &#x3D; node3 a1.sinks.k2.port &#x3D; 44444 a1.sinks.k2.batch-size &#x3D; 1000 a1.sinkgroups &#x3D; g1 a1.sinkgroups.g1.sinks &#x3D; k1 k2 a1.sinkgroups.g1.processor.type &#x3D; failover a1.sinkgroups.g1.processor.priority.k1 &#x3D; 200 a1.sinkgroups.g1.processor.priority.k2 &#x3D; 100 a1.sinkgroups.g1.processor.maxpenalty &#x3D; 5000 12）在node2、node3中配置下游服务器example14-2-Comprehensive-practical.conf，内容如下： a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.type &#x3D; avro a1.sources.r1.channels &#x3D; c1 a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 a1.sources.r1.threads &#x3D; 10 a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; file a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata_2&#x2F;checkpoint a1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata_2&#x2F;data a1.channels.c1.transactionCapacity &#x3D; 2000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; hdfs a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;gen_logdata&#x2F;%&#123;logtype&#125;&#x2F;%Y-%m-%d&#x2F; a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k1.hdfs.fileSuffix &#x3D; .log a1.sinks.k1.hdfs.rollInterval &#x3D; 300 a1.sinks.k1.hdfs.rollSize &#x3D; 268435456 a1.sinks.k1.hdfs.rollCount &#x3D; 0 a1.sinks.k1.hdfs.batchSize &#x3D; 1000 a1.sinks.k1.hdfs.codeC &#x3D; gzip a1.sinks.k1.hdfs.fileType &#x3D; CompressedStream 13）将node1、node2、node3机器上之前的一些flumedata目录清除； 14）在node1、node2中创建日志数据目录，在此目录下创建log1、log2、log3文件夹。 mkdir &#x2F;export&#x2F;data&#x2F;flume-example&#x2F;weblog 15）在node2、node3中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example14-2-Comprehensive-practical.conf -Dflume.root.logger&#x3D;DEBUG,console 16）在&#x2F;export&#x2F;data&#x2F;flume-example&#x2F;weblog路径下， 输入for i in {i..10000}模拟往3类日志中写入日志： &gt;do &gt;echo $&#123;i&#125;-access,1389999,asdb,ccc,ddd,eee,fff &gt;&gt; log1&#x2F;access.log &gt;echo $&#123;i&#125;-nginx,1389999,asdb,ccc,ddd,eee,fff &gt;&gt; log2&#x2F;nginx.log &gt;echo $&#123;i&#125;-weblog,1389999,asdb,ccc,ddd,eee,fff &gt;&gt; log3&#x2F;weblog.log &gt;sleep 0.2 &gt;done 17）在node1中&#x2F;export&#x2F;server&#x2F;flume路径下启动flume： bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example14-1-Comprehensive-practical.conf -Dflume.root.logger&#x3D;DEBUG,console 18）在hdfs上查看结果，数据上传成功 参考资料 [1]Flume安装测试文档.md","tags":[{"name":"Flume","slug":"Flume","permalink":"https://weichunxiu123.github.io/tags/Flume/"},{"name":"测试","slug":"测试","permalink":"https://weichunxiu123.github.io/tags/%E6%B5%8B%E8%AF%95/"},{"name":"案例","slug":"案例","permalink":"https://weichunxiu123.github.io/tags/%E6%A1%88%E4%BE%8B/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Fume安装及测试","date":"2023-06-14T06:19:03.519Z","path":"wiki/flume安装及测试/","text":"一、Flume安装1、Flume安装 （1）上传flume的压缩包到&#x2F;export&#x2F;software下。 （2）解压到&#x2F;export&#x2F;server目录下。 tar -zxvf apache-flume-1.9.0-bin.tar.gz -C &#x2F;export&#x2F;server&#x2F; 同时设置软链接： ln -s &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin&#x2F; flume 如下图所示： （3）编辑&#x2F;etc&#x2F;profile，配置FLUME_HOME指向正确的路径 #FLUME_HOME export FLUME_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;flume export PATH&#x3D;$PATH:$FLUME_HOME&#x2F;bin source &#x2F;etc&#x2F;profile （4）通过scp命令发送给其余机器，并设置软链接。 scp -r &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin root@node1:&#x2F;export&#x2F;server&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin root@node3:&#x2F;export&#x2F;server&#x2F; ln -s &#x2F;export&#x2F;server&#x2F;apache-flume-1.9.0-bin &#x2F;export&#x2F;server&#x2F;flume （5）添加配置文件 在flume&#x2F;myconf目录下添加配置文件netcat-logger.conf，添加如下内容： # example1-netcat-logger.conf: 单节点Flume配置 \\# 定义agent名称为a1 \\# 设置3个组件的名称 a1.sources &#x3D; r1 a1.sinks &#x3D; k1 a1.channels &#x3D; c1 \\# 配置source类型为NetCat,监听地址为本机，端口为44444 a1.sources.r1.type &#x3D; netcat a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 \\#source和channel关联 a1.sources.r1.channels &#x3D; c1 \\# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 100 \\# 配置sink类型为Logger a1.sinks.k1.type &#x3D; logger \\# 将sink绑定到channel上 a1.sinks.k1.channel &#x3D; c1 （6）启动flume 在&#x2F;export&#x2F;server&#x2F;flume目录下： bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example1-netcat-logger.conf -Dflume.root.logger&#x3D;INFO,console 使用Netcat测试，复制node1标签，启动netcat连接到44444端口 可以看到agent控制台接收到信息。 二、Flume测试（1）exec_source测试 使用Flume从日志文件中将日志收集到日志中间，以便于查找和分析。启动测试流程为： 1）准备日志文件 mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data 2）写一个脚本模拟向日志文件中持续写入数据， mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;shell；vim access_data.sh for i in &#123;1..10000&#125;； do echo $&#123;i&#125; “bigdata log 5&#x2F;11” &gt;&gt; access.log; sleep 0.5; done 3）创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。 # example2-exec-source-logger.conf a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.type &#x3D; exec a1.sources.r1.channels &#x3D; c1 a1.sources.r1.command &#x3D; tail -F &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;shell &#x2F;access.log a1.sources.r1.batchSize &#x3D; 100 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 100 a1.sinks.k1.type &#x3D; logger a1.sinks.k1.channel &#x3D; c1 4）启动flume bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example2-exec-source-logger.conf -Dflume.root.logger&#x3D;INFO,console （2）spooldir_source测试 监听一个指定的文件夹，如果文件夹下有没有采集过的新文件，则会采集新文件中的数据。监听测试流程为： 1）创建spooldir监听目录。 mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;spooldir 2）在spooldir下新建文件。 mkdir flume511.txt 3）创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。 #example3-spooldir-source.conf a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.channels &#x3D; c1 a1.sources.r1.type &#x3D; spooldir a1.sources.r1.spoolDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;spooldir a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 100 a1.sinks.k1.type &#x3D; logger a1.sinks.k1.channel &#x3D; c1 4）启动flume bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example3-spooldir-source.conf -Dflume.root.logger&#x3D;INFO,console （3）taildir_source测试 监听指定目录下的文件，只要文件中有新写入的行，就会被tail到，它会记录每一个文件所tail到的位置，记录到一个指定的positionfile保存目录中，格式为json。保证数据不会漏采（丢失）。 测试流程如下： 1）新建监听目录 mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web-access.log mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web-access.log.1 mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx-access.log 2）生成测试数据 for i in &#123;1..1000&#125; do echo &quot;webweb111 $RANDOM&quot; &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F; &#x2F;web-access.log echo &quot;webweb222 $RANDOM&quot; &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F; log&#x2F;web-access.log.1 echo &quot;wxwxwx333 $RANDOM&quot; &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data &#x2F;wxlog&#x2F;wx-access.log sleep 0.1 done 3）创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。 #example4-taildir-source.conf a1.sources &#x3D; r1 a1.sinks &#x3D; k1 a1.channels &#x3D; c1 a1.sources.r1.type &#x3D; TAILDIR a1.sources.r1.channels &#x3D; c1 a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.json a1.sources.r1.filegroups &#x3D; g1 g2 a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web.* a1.sources.r1.filegroups.g2 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx.* a1.sources.r1.fileHeader &#x3D; true \\#动态的header-keys eg：filepath&#x3D;&#x2F;..&#x2F;..&#x2F;..&#x2F; a1.sources.r1.fileHeaderKey &#x3D; filepath \\#写死的header-keys（静态的） eg:a1 &#x3D; aa1 a1.sources.r1.headers.g1.a1 &#x3D; aa1 a1.sources.r1.headers.g1.b1 &#x3D; bb1 a1.sources.r1.headers.g2.a2 &#x3D; aa2 a1.sources.r1.headers.g2.b2 &#x3D; bb2 a1.sources.r1.maxBatchCount &#x3D; 1000 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 10000 a1.channels.c1.transactionCapacity &#x3D; 1000 a1.sinks.k1.type &#x3D; logger a1.sinks.k1.channel &#x3D; c1 4）启动flume bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example4-taildir-source.conf -Dflume.root.logger&#x3D;INFO,console （4）avro source测试 avro source 是通过监听一个网络端口来接收数据，被接收数据必须是使用avro序列化框架序列化后的数据。 工作机制：启动一个网络服务，监听一个端口，收集端口上收到的avro序列化数据流。 测试流程如下： 1）自定义创建flume自定义配置文件，放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf目录下。 #example5-avro-source.conf a1.sources &#x3D; r1 a1.sources.r1.type &#x3D; avro a1.sources.r1.channels &#x3D; c1 a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 4141 a1.channels &#x3D; c1 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 200 a1.channels.c1.transactionCapacity &#x3D; 100 a1.sinks &#x3D; k1 a1.sinks.k1.type &#x3D; logger a1.sinks.k1.channel &#x3D; c1 2）启动agent bin&#x2F;flume-ng agent -c conf -f myconf&#x2F;example5-avro-source.conf -n a1 -Dflume.root.logger&#x3D;INFO,console 注意：在node1的&#x2F;export&#x2F;server&#x2F;flume目录下执行。 3）新建avro-log.txt，用一个客户端去给启动好的source发送avro序列化数据 mkdir -p &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;avro-log.txt bin&#x2F;flume-ng avro-client --host node1 --port 4141 -f &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;avro-log.txt 注意：复制标签在新窗口执行！！！ 最后在agent控制台可以看到avro-log.txt内的内容。 （5）使用File Channel实现数据持久化 测试流程如下： 1）自定义flume配置文件，并放到&#x2F;export&#x2F;server&#x2F;flume&#x2F;myconf中 #example6-file-channel.conf \\# 定义agent名称为a1 \\# 设置3个组件的名称 a1.sources &#x3D; r1 a1.sinks &#x3D; k1 \\# 多个channel使用空格分隔 a1.channels &#x3D; c1 c2 \\# 配置source类型为NetCat,监听地址为本机，端口为44444 a1.sources.r1.type &#x3D; netcat a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 \\# 配置sink类型为Logger a1.sinks.k1.type &#x3D; logger \\# 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100 a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 1000 a1.channels.c1.transactionCapacity &#x3D; 100 \\# 配置FileChannel,checkpointDir为检查点文件存储目录，dataDirs为日志数据存储目录， a1.channels.c2.type &#x3D; file a1.channels.c2.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint_filechannel a1.channels.c2.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data_filechannel \\# 将source和sink绑定到channel上 \\# source同时绑定到c1和c2上 a1.sources.r1.channels &#x3D; c1 c2 a1.sinks.k1.channel &#x3D; c1 | c2 2）为了方便日志打印，可以将-Dflume.root.logger&#x3D;INFO,console添加在conf的环境配置中。 cp flume-env.sh.template flume-env.sh vi flume-env.sh # 添加JAVA_OPTS export JAVA_OPTS&#x3D;&quot;-Dflume.root.logger&#x3D;INFO,console&quot; 2）启动flume bin&#x2F;flume-ng agent -n a1 -c conf -f myconf&#x2F;example6-file-channel.conf -Dflume.root.logger&#x3D;INFO,console 3）通过Netcat发送数据，发送到c2的数据被消费 agent控制台接收数据成功 （6）利用avro source和avro sink实现agent级联 （1）基本介绍：可以将多个Flume agent 程序连接在一起，其中一个agent的sink将数据发送到另一个agent的source。从多个Web服务器收集日志，发送到一个或多个集中处理的agent，之后再发往日志存储中心。 （2）测试流程如下： \\1) 启动hdfs，并检查工作状态。node1上有DataNode,NodeManager,Namenode,ResourceManager。node2上有NodeManager,SecondaryNameNode,DataNode。node3上有NodeManager,DataNode。 \\2) 配置上游配置文件，保存到node1,node2上。 \\3) 配置下游配置文件，保存到node3上。 \\4) 启动下游node3上的flume agent。 bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example7-2-avro-f-hdfs.conf -Dflume.root.logger&#x3D;DEBUG，console \\5) 在node1和node2上准备两个日志目录来生成模拟日志数据。 mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F; mkdir &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F; \\6) 在node1和node2上利用shell脚本生成日志数据。 vim avro-hdfs.sh while true do echo webwebwebwebweb &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog &#x2F;web-access.log echo wxwxwxwxwxwxwx &gt;&gt; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F; wx-access.log sleep 0.01 Done 7）启动node1和node2上的flume agent nohup bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;example7-1-taildir-f-avro.conf -Dflume.root.logger&#x3D;INFO,console 1&gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp; 8）上游配置文件： #上游服务器配置 example7-1-taildir-f-avro.conf a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 a1.sources.r1.type &#x3D; TAILDIR a1.sources.r1.channels &#x3D; c1 a1.sources.r1.positionFile &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;taildir_position.json a1.sources.r1.filegroups &#x3D; g1 g2 a1.sources.r1.filegroups.g1 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;weblog&#x2F;web.* a1.sources.r1.filegroups.g2 &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;wxlog&#x2F;wx.* \\#提高吞吐量 a1.sources.r1.batchSize &#x3D; 1000 \\#动态的header-keys eg：filepath&#x3D;&#x2F;..&#x2F;..&#x2F;..&#x2F; a1.sources.r1.fileHeaderKey &#x3D; filepath \\#拦截器配置，添加header&#x3D;timestamp a1.sources.r1.interceptors &#x3D; i1 a1.sources.r1.interceptors.i1.type &#x3D; timestamp a1.sources.r1.interceptors.i1.headerName &#x3D; timestamp a1.channels.c1.type &#x3D; file \\#本机数据汇集检查点、event存储目录 a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint a1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data a1.channels.c1.transactionCapacity &#x3D; 2000 a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; avro a1.sinks.k1.batch-size &#x3D; 1000 \\#下游目标主机、端口 a1.sinks.k1.hostname &#x3D; node3 a1.sinks.k1.port &#x3D; 44444 9）下游配置文件： #下游服务器配置 example7-2-avro-f-hdfs.conf a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1 #下游数据汇集avro source a1.sources.r1.type &#x3D; avro a1.sources.r1.channels &#x3D; c1 a1.sources.r1.bind &#x3D; 0.0.0.0 a1.sources.r1.port &#x3D; 44444 a1.sources.r1.threads &#x3D; 10 a1.sources.r1.batchSize &#x3D; 1000 a1.channels.c1.type &#x3D; file a1.channels.c1.checkpointDir &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;checkpoint a1.channels.c1.dataDirs &#x3D; &#x2F;export&#x2F;data&#x2F;flume-example-data&#x2F;flumedata&#x2F;data a1.channels.c1.transactionCapacity &#x3D; 2000 #hdfs sink a1.sinks.k1.channel &#x3D; c1 a1.sinks.k1.type &#x3D; hdfs a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;node1:8020&#x2F;logdata&#x2F;%Y-%m-%d&#x2F;%H&#x2F; #eg：文件名 logdata_34438hxfd.log，在滚动时，logdata_34438hxfd.log.tmp a1.sinks.k1.hdfs.filePrefix &#x3D; logdata_ a1.sinks.k1.hdfs.fileSuffix &#x3D; .log #三个条件没有优先级，谁先达到就进行滚动 #按时间间隔滚动 a1.sinks.k1.hdfs.rollInterval &#x3D; 0 #按文件大小滚动 256MB a1.sinks.k1.hdfs.rollSize &#x3D; 268435456 #按event条数滚动 a1.sinks.k1.hdfs.rollCount &#x3D; 100000 a1.sinks.k1.hdfs.batchSize &#x3D; 1000 a1.sinks.k1.hdfs.codeC &#x3D; gzip a1.sinks.k1.hdfs.fileType &#x3D; CompressedStream 先在下游启动flume agent 。下游的agent的sink将数据发送到上游agent的source。 启动下游的flume agent 到hdfs上查看结果 参考资料 [1]Flume安装配置文档.md [2]https://skykip.github.io/2022/04/10/hello-world/","tags":[{"name":"Flume","slug":"Flume","permalink":"https://weichunxiu123.github.io/tags/Flume/"},{"name":"安装及测试","slug":"安装及测试","permalink":"https://weichunxiu123.github.io/tags/%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-pyspark基础编码环境","date":"2023-06-08T03:05:04.757Z","path":"wiki/Spark-pyspark基础编码环境/","text":"Spark（pyspark基础编码环境）（一）、pyspark环境配置安装。 PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。 前情提示： （1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。 （2）将文件夹内bin内的Hadoop.dll复制到C:\\Windows\\Systmctl32里面去。 （3）在系统环境变量中配置HADOOP_HOME，指向hadoop-3.3.0文件夹的路径。 （二）本机PySpark环境配置 在前面部署Spark的时候，已经在Linux系统上部署了acaconda的Python环境，详见Spark的Stand Alone模式部署章节。故本次在Windows上安装anaconda，并配置PySpark库。具体安装步骤如下： （1）在课程资料中选择anaconda应用程序双击安装。 （2）一直选择Next，进行安装。 注意：选择第一个，将anaconda添加至我的环境变量中！ （3）安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。 （4）配置国内源，加速网络下载。 1、在Anaconda Prompt(anaconda)中执行 conda config --set show_channel_urls yes 2、将如下内容替换到C:\\Users\\用户名.condarc文件中。 channels: \\- defaultsshow_channel_urls: truedefault_channels: \\- https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main \\- https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r \\- https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;msys2 custom_channels: conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud simpleitk: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud （5）创建虚拟环境 1、创建虚拟环境 pyspark, 基于Python 3.8 conda create -n pyspark python&#x3D;3.8 2、切换到虚拟环境内 conda activate pyspark 3、在虚拟环境内安装包 pip install pyhive pyspark jieba -i [https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;](https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple)simple 安装成功示例： （三）PyCharm中配置Python解释器 （1）配置本地解释器：创建Python项目，选择conda虚拟环境PySpark中的Python.exe解释器。 （2）配置远程SSH Linux解释器 1、远程SSH python pyspark环境 2、添加新的远程连接 3、设置虚拟的python环境路径 （四）WordCount应用实战 可以选择在本地的PySpark环境中执行spark代码，也可以选择在虚拟机环境PySpark中执行。选择本地的就是使用conda环境，应用其中的PySpark环境执行，来读取本地文件，完成单词计数的实例。选择远程虚拟机中的PySpark环境，需要SSH连接到服务器（这里需要安装Pycharm专业版），注意：无论是选择那种方案，都是在PyCharm软件中去执行，完成上述过程。 （1）WordCount代码本地执行 准备pyspark代码以及本地文件words.txt，在PyCharm中执行。 # coding:utf8 from pyspark import SparkConf, SparkContext\\# import os import os os.environ[&#39;PYSPARK_PYTHON&#39;]&#x3D;&#39;D:\\\\anaconda3\\\\envs\\\\pyspark\\\\python.exe&#39; os.environ [&#39;JAVA_HOME&#39;] &#x3D; &#39;D:\\\\Java\\\\jdk1.8.0_241&#39; \\#os.environ[&#39;PYSPARK_PYTHON&#39;]&#x3D;&#39;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python3.8&#39; if __name__ &#x3D;&#x3D; &#39;__main__&#39;: conf &#x3D; SparkConf().setAppName(&quot;WordCountHelloWorld&quot;).setMaster(&quot;local[*]&quot;) # 通过SparkConf对象构建SparkContext对象 sc &#x3D; SparkContext(conf&#x3D;conf) # 需求 : wordcount单词计数, 读取HDFS上的words.txt文件, 对其内部的单词统计出现 的数量 # 读取文件 #file_rdd &#x3D; sc.textFile(&quot;hdfs:&#x2F;&#x2F;node1:8020&#x2F;input&#x2F;words.txt&quot;) #file_rdd &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;tmp&#x2F;pycharm_project_621&#x2F;data&#x2F;words.txt&quot;) file_rdd &#x3D; sc.textFile(&quot;D:\\\\数据挖掘与分析实验报告合集\\\\pyspark\\\\data\\\\input\\\\words.txt&quot;) # 将单词进行切割, 得到一个存储全部单词的集合对象 words_rdd &#x3D; file_rdd.flatMap(lambda line: line.split(&quot; &quot;)) # 将单词转换为元组对象, key是单词, value是数字1 words_with_one_rdd &#x3D; words_rdd.map(lambda x: (x, 1)) # 将元组的value 按照key来分组, 对所有的value执行聚合操作(相加) result_rdd &#x3D; words_with_one_rdd.reduceByKey(lambda a, b: a + b) # 通过collect方法收集RDD的数据打印输出结果 print(result_rdd.collect()) 运行结果截图： （2）WordCount代码远程服务器上执行。 通过SSH连接到远程服务器上，详见上述操作。 完成与服务器连接后，会在服务器中的&#x2F;tmp文件夹下新建了pycharm_project_xxx文件夹用于放置本地的同步代码。 （3）读取HDFS上的文件 1、将读取文件路径改为hdfs上的&#x2F;input&#x2F;words.txt 2、在hdfs上新建&#x2F;input文件夹，使用命令 hadoop fs -mkdir &#x2F;input 3、上传words.txt到hdfs中，使用命令 hadoop fs -put words.txt &#x2F;input&#x2F; 4、在pycharm中执行spark代码 （五）spark-submit作业提交 （1）local本地模式 首先将helloword.py程序放到&#x2F;root&#x2F;目录下，使用命令完成提交作业。 bin&#x2F;spark-submit --master local[*] &#x2F;root&#x2F;helloworld.py （2）spark on yarn模式 使用命令完成提交作业。 bin&#x2F;spark-submit --master yarn &#x2F;root&#x2F;helloworld.py （3）使用历史服务器查看任务执行情况 node1:18080 参考资料 [1]spark环境部署.md [2]1-saprk基础入门.pdf [3]spark1.pptx","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"https://weichunxiu123.github.io/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-YARN模式部署","date":"2023-06-08T02:48:27.429Z","path":"wiki/spark-YARN模式部署/","text":"spark（YARN模式）（1）Client模式中driver运行在客户端，在客户端显示输出结果，但是在spark历史服务器不显示logs信息。 （2）Cluster模式中driver运行在YARN容器内部，和ApplicationMaster在同一个容器内，在客户端不显示输出结果，所以在spark历史服务器中显示logs的信息。 （3）client模式测试 bin&#x2F;spark-submit --master yarn --deploy-mode client --driver-memory 512m $&#123;SPARK_HOME&#125;&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 10 （4） cluster模式测试 bin&#x2F;spark-submit --master yarn --deploy-mode cluster --driver-memory 512m \\--conf &quot;spark.pyspark.driver.python&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3 &#x2F;bin&#x2F;python3&quot; \\--conf &quot;spark.pyspark.python&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3 &#x2F;bin&#x2F;python3&quot; $&#123;SPARK_HOME&#125;&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 10 参考资料 [1]spark环境部署 [2]spark1.pptx","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"https://weichunxiu123.github.io/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-HA环境部署","date":"2023-06-08T02:34:22.238Z","path":"wiki/spark-HA环境部署/","text":"spark（HA环境部署）（1）首先进入spark-env.sh中， vim &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh （2）在spark-env.sh配置文件中删除 export SPARK_MASTER_HOST&#x3D;node1 （目的是不让机器知道固定的master是谁，不然无法进行master切换） （3）在spark-env.sh配置文件中增加以下内容： SPARK_DAEMON_JAVA_OPTS&#x3D;&quot;-Dspark.deploy.recoveryMode&#x3D;ZOOKEEPER -Dspark.deploy.zookeeper.url&#x3D;node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir&#x3D;&#x2F;spark-ha&quot; # spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现 # 指定Zookeeper的连接地址 # 指定在Zookeeper中注册临时节点的路径 （4）将spark-env.sh配置文件分发给node2、node3。 scp -r &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; scp -r &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F; （5）启动StandAlone集群、zookeeper集群： 1）在node1上： sbin&#x2F;start-all.sh 2）在node2上： sbin&#x2F;start-master.sh （目的是：备用master，当kill掉node1的master后，程序依然能进行） （6）查看node1、node2的WEB UI （如果8080端口被占用了，可以顺延到8081、8082端口，其中node1上的master是alive的，node2上的是standby） node1:8080--&gt;8081 node2:8080--&gt;8081\\8082 （6）Master主备切换，在&#x2F;export&#x2F;server&#x2F;spark路径下提交一个任务到当前alive master上: bin&#x2F;spark-submit --master spark:&#x2F;&#x2F;node1:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000 （在提交成功后, 将alive master直接kill掉，系统不会中断，仍然能正常运行结果） （7）查看Master的WEB UI，只有node2是alive的，证明master切换成功 参考资料 [1]spark环境部署.md [2]1-spark基础环境配置.pdf","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"https://weichunxiu123.github.io/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-standalone环境部署","date":"2023-06-08T01:56:46.881Z","path":"wiki/Spark-standalone环境部署/","text":"spark（StandAlone环境部署）（一）、集群规划：选择三台机器分别为node1、node2、node3来组成集群环境。 其中node1上安装master和worker进程；node2上安装worker进程；node3上安装worker进程。 （二）、anaconda on linux安装过程： （1）前提：在linux服务器node1、node2、node3上都安装python(anaconda)。并安装pyspark虚拟环境。具体安装步骤如下。 1、在&#x2F;export&#x2F;server&#x2F;目录下上传anaconda的安装包Anaconda3-2021.05-Linux-x86_64.sh。 2、安装anaconda 使用命令： sh .&#x2F;Anaconda3-2021.05-Linux-x86_64.sh 3、安装完毕之后若没有出现base环境，进行如下配置。在&#x2F;root&#x2F;.condarc添加国内源 安装完毕后，关闭服务器重新启动，出现base环境即安装成功。 （2）在anaconda中，安装pyspark虚拟环境。 1、基于python3.8安装pyspark环境。 2、切换到pyspark中，并安装所需要的安装包。 注：在node1、node2、node3三台服务器上都完成配置！ （三）、StandAlone模式部署 （1）安装spark压缩文件。 1、进入到&#x2F;export&#x2F;server&#x2F;中上传并解压spark-3.2.0-bin-hadoop3.2.tgz。并设置软链接，命令为 ln-s&#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark。 （2）在&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf，配置文件。 1、首先在配置workers文件。 mv workers.template workers vim workers 2.配置spark-env.sh文件。mv spark-env.sh.template spark-env.sh； Vim spark-env.sh，添加如下内容。 ## 设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk \\## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop \\## 指定spark老大Master的IP和提交任务的通信端口 \\# 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;node1 \\# 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077 \\# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080 \\# worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 \\# worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g \\# worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078 \\# worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081 \\## 设置历史服务器 \\# 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;&quot;-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; -Dspark.history.fs.cleaner.enabled&#x3D;true&quot; 3、在HDFS上创建程序运行历史记录存放的文件夹。 hadoop fs -mkdir &#x2F;sparklog；hadoop fs -chmod 777 &#x2F;sparklog 4、配置spark-defaults.conf文件。 mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 添加如下内容。 5、配置log4j.properties 文件[可选配置]。 mv log4j.properties.template log4j.properties 修改配置，设置级别为WARN 只输出警告和错误日志。 （四）、将spark分发到node2和node3服务器上。注意同时要设置软链接。 scp -r spark-3.1.2-bin-hadoop3.2 node2:&#x2F;export&#x2F;server&#x2F; scp -r spark-3.1.2-bin-hadoop3.2 node3:&#x2F;export&#x2F;server&#x2F; ln -s &#x2F;export&#x2F;server&#x2F;spark-3.1.2-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark 注意：配置&#x2F;etc&#x2F;profile，JAVA_HOME；SPARK_HOME；PYSPARK_PYTHON都指向正确的目录。 （五）、启动历史服务器，启动Spark的Master和Worker进程 （1）启动历史服务器：sbin&#x2F;start-history-server.sh （2）启动全部的master和worker：sbin&#x2F;start-all.Sh （六）、查看Master的WEB UI 在浏览器中输入node1:8080 （七）、连接到StandAlone集群 （1）通过master来连接到StandAlone集群。 bin&#x2F;pyspark --master spark:&#x2F;&#x2F;node1:7077 （2）使用spark-shell连接StandAlone集群。 bin&#x2F;spark-shell –master spark:&#x2F;&#x2F;node1:7077 进行测试。 （3）使用spark-submit(PI)提交任务到集群上执行。 bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;node1:7077&#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;Python&#x2F;pi.py 10 查看历史服务器：在浏览器中输入node1：18080 参考资料 [1]spark(standalone部署文档) [2]1-spark基础入门.pdf","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"https://weichunxiu123.github.io/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spark-local模式配置","date":"2023-06-07T11:38:25.725Z","path":"wiki/Spark-local模式配置/","text":"spark（local环境部署）（1）安装Anaconda 上传安装包 sh .&#x2F;Anaconda3-2021.05-Linux-x86_64.sh 出现（base)即为安装成功 （2）创建虚拟环境 conda create -n pyspark python&#x3D;3.8 conda activate pyspark pip install pyhive pyspark jieba -i [https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn](https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple)&#x2F;simple （1）修改环境变量配置Spark由如下5个环境变量需要设置 SPARK_HOME: 表示Spark安装路径在哪里 PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器。 JAVA_HOME: 告知Spark Java在哪里 HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 HADOOP_HOME: 告知Spark Hadoop安装在哪里 这5个环境变量 都需要配置在: &#x2F;etc&#x2F;profile中！ （4）解压 解压下载的Spark安装包 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F; 设置软连接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark （2）测试 sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect() bin&#x2F;pyspark在这个环境可以运行spark代码，如图： 在所有机器安装Python(Anaconda)，并在所有机器配置环境变量。 （3）配置配置文件 进入到spark的配置文件目录中, cd $SPARK_HOME&#x2F;conf&#96; 配置workers文件vi workers # 改名, 去掉后面的.template后缀 mv workers.template workers # 编辑worker文件 vim workers # 将里面的localhost删除, 追加到workers文件内 node1 node2 node3 （4）配置spark-env.sh文件 # 1. 改名 mv spark-env.sh.template spark-env.sh # 2. 编辑spark-env.sh, 在底部追加如下内容 ## 设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST&#x3D;node1 # 告知sparkmaster的通讯端口 export SPARK_MASTER_PORT&#x3D;7077 # 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT&#x3D;8080 # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1 # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078 # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081 ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;&quot;-Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; -Dspark.history.fs.cleaner.enabled&#x3D;true&quot; 在HDFS上创建程序运行历史记录存放的文件夹: hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog （3）配置spark-defaults.conf文件 # 1. 改名 mv spark-defaults.conf.template spark-defaults.conf # 2. 修改内容, 追加如下内容 # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true （4)配置log4j.properties 文件 [可选配置] mv log4j.properties.template log4j.properties 注意：将Spark安装文件夹 分发到其它的服务器 1）scp -r spark-3.1.2-bin-hadoop3.2 node2:&#x2F;export&#x2F;server&#x2F; 2）scp -r spark-3.1.2-bin-hadoop3.2 node3:&#x2F;export&#x2F;server&#x2F; 3）在node2和node3上 给spark安装目录增加软链接 ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark （5）启动历史服务器 sbin&#x2F;start-history-server.sh （6）启动Spark的Master和Worker进程 sbin&#x2F;start-all.sh sbin&#x2F;start-master.sh sbin&#x2F;start-worker.sh sbin&#x2F;stop-all.sh sbin&#x2F;stop-master.sh sbin&#x2F;stop-worker.sh 查看Master的WEB UI 连接到StandAlone集群 参考资料 [1]spark部署.md [2]spark 1.pptx","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"https://weichunxiu123.github.io/tags/spark/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Docker安装与基本操作","date":"2023-06-07T10:25:11.923Z","path":"wiki/Docker安装与基本操作/","text":"一、安装docker（1）卸载（可选） 如果之前安装过旧版本的Docker，可以使用下面命令卸载： yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine \\ docker-ce （2）yum源配置 1.备份配置文件 &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.backup wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo [http:&#x2F;&#x2F;mirrors.aliyun](http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo).com&#x2F;repo&#x2F;Centos-7.repo wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo [http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;rep](http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo)o&#x2F;epel-7.repo yum clean all yum makecache yum install -y bash-completion vim lrzsz wget expect net-tools nc nmap treedos2unix htop iftop iotop unzip telnet sl psmisc nethogs glances bc ntpdate openldap-devel *安装docker* （1）受限需要虚拟机联网，安装yum工具 （2）配置网卡转发 1）docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能 #写入 2）重新加载内核参数 modprobe br_netfilter sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;docker.conf （3）利用yum进行docker安装 提前配置好yum仓库 1）阿里云自带仓库 curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;Centos-7.repo [http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;](http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo)Centos-7.repo 2）阿里云提供的docker专属repo仓库 curl-o&#x2F;etc&#x2F;yum.repos.d&#x2F;docker-ce.repohttp:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo 3）更新yum缓存 yum clean all &amp;&amp; yum makecache 4）查看源中可用版本 yum list docker-ce --showduplicates | sort -r 5）yum安装 yum install docker-ce -y docker -v 卸载 yum remove -y docker-ce-xxx （4）配置镜像加速器 用于加速镜像文件下载,选用阿里云镜像站 mkdir -p &#x2F;etc&#x2F;docker touch &#x2F;etc&#x2F;docker&#x2F;daemon.json 1）进入文件vim &#x2F;etc&#x2F;docker&#x2F;daemon.json编写以下内容： {“registry-mirrors” : [“","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"Docker","slug":"Docker","permalink":"https://weichunxiu123.github.io/tags/Docker/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Git安装与配置","date":"2023-06-07T09:39:32.518Z","path":"wiki/Git安装与配置/","text":"一、Git安装（1）Git下载 Git下载程序 （2）可视化客户端 中文语言包 （3）初始化仓库 （4）添加文件，提交文件至本地仓库 （5）本地删除与恢复 文件选中删除，可用以下方式还原 （6）创建分支 （7）分支的查看切换 （8）标签的创建 （9）切换与删除 通过右键选中删除 *远程仓库* （1）码云账号注册 填写邮箱发送验证码,然后可以注册账号,主页如下 （2）创建远程仓库 （3）把本地代码推送到远端 生成公钥私钥 ssh-keygen -t rsa 一直回车 会默认用户目录 .ssh 目录生成一个默认的id_rsa文件 和id_rsa.pub 密钥配置 参考资料 1","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"Git","slug":"Git","permalink":"https://weichunxiu123.github.io/tags/Git/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"Hive基础配置","date":"2023-06-07T08:56:02.680Z","path":"wiki/hive安装/","text":"一、Hive安装*（1）**Mysql安装* 1）卸载Centos7自带的mariadb 如果出现了mariadb-libs-5.5.64-1.el7.x86_64，输入rpm -e mariadb- libs-5.5.64-1.el7.x86_64 –nodeps,在输入rpm -qa|grep mariadb，即可 2）安装mysql 新建文件夹 mkdir &#x2F;export&#x2F;server&#x2F;mysql 上传mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar到上述文件夹下，解压 tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 3）执行安装 yum -y install libaio 4）mysql初始化设置 初始化： mysqld --initialize 更改所属组： chown mysql:mysql /var/lib/mysql -R 启动mysql： systemctl start mysqld.service 查看临时生成的root密码： cat /var/log/mysqld.log 5）修改root密码 授权远程访问 设置开机自启动 修改root密码 设置为hadoop 授权 use mysql; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'hadoop' WITH GRANT OPTION; FLUSH PRIVILEGES; mysql的启动和关闭 状态查看 （这几个命令必须记住） systemctl stop mysqld systemctl status mysqld systemctl start mysqld 设置开机自动启动：systemctl enable mysqld 查看是否设置自动启动成功 *（2）**H**ive**的安装* 1）上传安装包 解压 tar zxvf apache-hive-3.1.2-bin.tar.gz ln -s apache-hive-3.1.2-bin hive 2）解决Hive与Hadoop之间guava版本差异 cd &#x2F;export&#x2F;server&#x2F;hive&#x2F; rm -rf lib&#x2F;guava-19.0.jar cp &#x2F;export&#x2F;server&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;guava-27.0-jre.jar.&#x2F;lib&#x2F; 3）修改配置文件 hive-env.sh cd &#x2F;export&#x2F;server&#x2F;hive&#x2F;conf mv hive-env.sh.template hive-env.sh vim hive-env.sh export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop export HIVE_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hive&#x2F;conf export HIVE_AUX_JARS_PATH&#x3D;&#x2F;export&#x2F;server&#x2F;hive&#x2F;lib hive-site.xml vim hive-site.xml &lt;configuration&gt; &lt;!-- 存储元数据mysql相关配置 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt; &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;hive3?createDatabaseIfNotExist&#x3D;true&amp;useSSL&#x3D;false&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt; &lt;value&gt;hadoop&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- H2S运行绑定host --&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;&#x2F;name&gt; &lt;value&gt;node1&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 远程模式部署metastore metastore地址 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;&#x2F;name&gt; &lt;value&gt;thrift:&#x2F;&#x2F;node1:9083&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 关闭元数据存储授权 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;&#x2F;configuration&gt; 4）上传mysql jdbc驱动到hive安装包lib下 mysql-connector-java-5.1.32.jar 5）初始化元数据 cd &#x2F;export&#x2F;server&#x2F;hive&#x2F; bin&#x2F;schematool -initSchema -dbType mysql -verbos 初始化成功之后会在MySQL中创建74张表 6）在hdfs创建hive存储目录（如存在则不用操作） hadoop fs -mkdir &#x2F;tmp hadoop fs -mkdir -p &#x2F;user&#x2F;hive&#x2F;warehouse hadoop fs -chmod g+w &#x2F;tmp hadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse 7）启动hive *（**3）**启动metastore服务* 前台启动 关闭ctrl+c &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive –service metastore 前台启动开启debug日志 &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive --service metastore --hiveconf hive.root.logger&#x3D;DEBUG,console 后台启动 进程挂起 关闭使用jps+ kill -9 nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive --service metastore &amp; *（**4）**启动hiveserver2服务* nohup &#x2F;export&#x2F;server&#x2F;hive&#x2F;bin&#x2F;hive --service hiveserver2 &amp; beeline客户端连接 拷贝node1安装包到beeline客户端机器上（node3） scp -r &#x2F;export&#x2F;server&#x2F;apache-hive-3.1.2-bin&#x2F; root@node3:&#x2F;export&#x2F;server&#x2F; *（5）**hive注释信息中文乱码解决* 以下sql语句均在mysql数据库中执行 use hivenode2; show tables; alter table hivenode2.COLUMNS_V2 modify column COMMENT varchar(256) character set utf8; alter table hivenode2.TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; alter table hivenode2.PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 ; alter table hivenode2.PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8; alter table hivenode2.INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;","tags":[{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"Hive","slug":"Hive","permalink":"https://weichunxiu123.github.io/tags/Hive/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]},{"title":"第一个hexo","date":"2023-06-07T00:16:18.933Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","tags":[{"name":"初识","slug":"初识","permalink":"https://weichunxiu123.github.io/tags/%E5%88%9D%E8%AF%86/"},{"name":"hexo","slug":"hexo","permalink":"https://weichunxiu123.github.io/tags/hexo/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}]}],"categories":[{"name":"工具","slug":"工具","permalink":"https://weichunxiu123.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://weichunxiu123.github.io/tags/kafka/"},{"name":"kafka命令行操作","slug":"kafka命令行操作","permalink":"https://weichunxiu123.github.io/tags/kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://weichunxiu123.github.io/tags/zookeeper/"},{"name":"zookeeperJavaAPI操作命令","slug":"zookeeperJavaAPI操作命令","permalink":"https://weichunxiu123.github.io/tags/zookeeperJavaAPI%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"},{"name":"sqoop","slug":"sqoop","permalink":"https://weichunxiu123.github.io/tags/sqoop/"},{"name":"导入","slug":"导入","permalink":"https://weichunxiu123.github.io/tags/%E5%AF%BC%E5%85%A5/"},{"name":"导出","slug":"导出","permalink":"https://weichunxiu123.github.io/tags/%E5%AF%BC%E5%87%BA/"},{"name":"Flume","slug":"Flume","permalink":"https://weichunxiu123.github.io/tags/Flume/"},{"name":"测试","slug":"测试","permalink":"https://weichunxiu123.github.io/tags/%E6%B5%8B%E8%AF%95/"},{"name":"案例","slug":"案例","permalink":"https://weichunxiu123.github.io/tags/%E6%A1%88%E4%BE%8B/"},{"name":"安装及测试","slug":"安装及测试","permalink":"https://weichunxiu123.github.io/tags/%E5%AE%89%E8%A3%85%E5%8F%8A%E6%B5%8B%E8%AF%95/"},{"name":"搭建","slug":"搭建","permalink":"https://weichunxiu123.github.io/tags/%E6%90%AD%E5%BB%BA/"},{"name":"spark","slug":"spark","permalink":"https://weichunxiu123.github.io/tags/spark/"},{"name":"Docker","slug":"Docker","permalink":"https://weichunxiu123.github.io/tags/Docker/"},{"name":"Git","slug":"Git","permalink":"https://weichunxiu123.github.io/tags/Git/"},{"name":"Hive","slug":"Hive","permalink":"https://weichunxiu123.github.io/tags/Hive/"},{"name":"初识","slug":"初识","permalink":"https://weichunxiu123.github.io/tags/%E5%88%9D%E8%AF%86/"},{"name":"hexo","slug":"hexo","permalink":"https://weichunxiu123.github.io/tags/hexo/"}]}